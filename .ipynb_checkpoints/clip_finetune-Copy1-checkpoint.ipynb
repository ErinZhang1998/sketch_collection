{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import read_datasets as rd\n",
    "import constants as CONST\n",
    "from importlib import reload\n",
    "reload(rd)\n",
    "reload(CONST)\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finetune\n",
    "reload(finetune)\n",
    "from finetune import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nlpaug.augmenter.char as nac\n",
    "# import nlpaug.augmenter.word as naw\n",
    "# import nlpaug.augmenter.sentence as nas\n",
    "# import nlpaug.flow as nafc\n",
    "# from nlpaug.util import Action\n",
    "# import nltk\n",
    "\n",
    "# os.environ[\"MODEL_DIR\"] = '../model'\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merinz\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'clip' from '/home/xiaoyuz1/anaconda3/envs/clipdraw/lib/python3.8/site-packages/clip/__init__.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "https://github.com/openai/CLIP/issues/83 \n",
    "\n",
    "@Zasder3 have created a PyTorch lighting version to train the CLIP https://github.com/Zasder3/train-CLIP\n",
    "\n",
    "@mitchellnw researchers at UW, Google, Stanford, Amazon, Columbia, and Berkeley also create their training code https://github.com/mlfoundations/open_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'BATCH_SIZE' : 128,\n",
    "    'EVAL_BATCH_SIZE' : 32,\n",
    "    'START_EPOCH' : 0,\n",
    "    'EPOCH' : 300,\n",
    "    'EVAL_EVERY' : 50,\n",
    "    'LR' : 5e-6, \n",
    "    'BETAS' : (0.9,0.98),\n",
    "    'EPS' : 1e-6,\n",
    "    'WEIGHT_DECAY' : 0.1,\n",
    "    'TRAIN_CATEGORY' : ['angel'],\n",
    "    'TEST_CATEGORY' : ['face', 'angel'],\n",
    "    'IMAGE_PATH_TEMPLATE' : {\n",
    "            'face' : '/raid/xiaoyuz1/sketch_datasets/face_images_weight_5_all/{}.png',\n",
    "            'angel' : '/raid/xiaoyuz1/sketch_datasets/angel_images_weight_5_all/{}.png',\n",
    "        },\n",
    "    'TEMPLATE' : 0, \n",
    "    'line_diameter_scale' : [0.25,1.25],\n",
    "    'default_line_diameter': 10,\n",
    "    'rotate' : [-1/4*180, 1/4*180],\n",
    "    'trans' : [0.15, 0.15],\n",
    "    'scale' : [0.75, 1.25],\n",
    "    'open_clip' : False,\n",
    "    'adamw' : True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/xiaoyuz1/amazon_turk/wandb/run-20220418_010955-3u6iluad</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/erinz/clip-finetune/runs/3u6iluad\" target=\"_blank\">visionary-sun-30</a></strong> to <a href=\"https://wandb.ai/erinz/clip-finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_id = wandb.util.generate_id()\n",
    "run = wandb.init(project=\"clip-finetune\", id=run_id, config=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visionary-sun-30\n"
     ]
    }
   ],
   "source": [
    "print(wandb.run.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.update(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = get_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda:5\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "\n",
    "\n",
    "if args['open_clip']:\n",
    "    model, train_transform, preprocess = \\\n",
    "        open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32', device = device)\n",
    "else:\n",
    "    model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "    if device == \"cpu\":\n",
    "        model.float()\n",
    "    else :\n",
    "        clip.model.convert_weights(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### experimenting with text augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# text = \"small dotted eyes\"\n",
    "# import nlpaug.augmenter.word as naw\n",
    "\n",
    "# back_translation_aug = naw.BackTranslationAug(\n",
    "#     from_model_name='facebook/wmt19-en-de', \n",
    "#     to_model_name='facebook/wmt19-de-en'\n",
    "# )\n",
    "# back_translation_aug.augment(text)\n",
    "# aug = naw.WordEmbsAug(\n",
    "#     model_type='word2vec', model_path='/raid/xiaoyuz1/'+'GoogleNews-vectors-negative300.bin',\n",
    "#     action=\"substitute\")\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_preprocess = [\n",
    "#     naw.SpellingAug(),\n",
    "#     naw.SynonymAug(aug_src='wordnet'),\n",
    "#     naw.ContextualWordEmbsAug(model_path='distilbert-base-uncased', action=\"insert\"),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_1,template_2 = template[args['TEMPLATE']]\n",
    "img_path_template = args['IMAGE_PATH_TEMPLATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {'test' : {}, 'dev' : {}}\n",
    "\n",
    "df = dfp[(dfp['category'].isin(args['TRAIN_CATEGORY'])) & (dfp['split'] == 'train')]\n",
    "df = df.reset_index(drop=True)\n",
    "df['caption_1'] = df.apply(template_1, axis=1)\n",
    "df['caption_2'] = df.apply(template_2, axis=1)\n",
    "dfs['train'] = df\n",
    "\n",
    "for split in ['test','dev']:\n",
    "    for cat in args['TEST_CATEGORY']:\n",
    "        df = dfp[(dfp['category'] == cat) & (dfp['split'] == split)]\n",
    "        df = df.reset_index(drop=True)\n",
    "        df['caption_1'] = df.apply(template_1, axis=1)\n",
    "        df['caption_2'] = df.apply(template_2, axis=1)\n",
    "        dfs[split][cat] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocess = transform(args, n_px = 224, is_train = True, open_clip = args['open_clip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches:  34\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ImageTextDataset(\n",
    "    dfs['train'], \n",
    "    img_path_template, \n",
    "    train_preprocess,\n",
    "    [],\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = args['BATCH_SIZE'], shuffle=True) \n",
    "print(\"Number of train batches: \", len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import copy\n",
    "# def denormalize(tensor):\n",
    "#     mean = (0.48145466, 0.4578275, 0.40821073)\n",
    "#     std = (0.26862954, 0.26130258, 0.27577711)\n",
    "#     res = copy.deepcopy(tensor)\n",
    "#     for i, (m, s) in enumerate(zip(mean, std)):\n",
    "#         if len(res.shape) == 4:\n",
    "#             res[:,i,:,:].mul_(s).add_(s)\n",
    "#         else:\n",
    "#             res[i,:,:].mul_(s).add_(s)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mask_tensor = denormalize(train_dataset[0][0])\n",
    "# mask_PIL = T.ToPILImage()(mask_tensor).convert(\"L\")\n",
    "# mask_PIL = PIL.ImageOps.invert(mask_PIL)\n",
    "# mask_PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_dataset = ImageTextPairDataset(dfs['train'], img_path_template, preprocess)\n",
    "train_t_dataloader = DataLoader(train_t_dataset, batch_size = args['EVAL_BATCH_SIZE'], shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test batches:  16\n",
      "Number of test batches:  20\n"
     ]
    }
   ],
   "source": [
    "test_dataloaders = {}\n",
    "for cat in args['TEST_CATEGORY']:\n",
    "    test_dataset = ImageTextPairDataset(dfs['test'][cat], img_path_template, preprocess)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size = args['EVAL_BATCH_SIZE'], shuffle=False)\n",
    "    print(\"Number of test batches: \", len(test_dataloader))\n",
    "    \n",
    "    test_dataloaders[cat] = test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dev batches:  8\n",
      "Number of dev batches:  10\n"
     ]
    }
   ],
   "source": [
    "dev_dataloaders = {}\n",
    "for cat in args['TEST_CATEGORY']:\n",
    "    dev_dataset = ImageTextPairDataset(dfs['dev'][cat], img_path_template, preprocess)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size = args['EVAL_BATCH_SIZE'], shuffle=False)\n",
    "    print(\"Number of dev batches: \", len(dev_dataloader))\n",
    "    \n",
    "    dev_dataloaders[cat] = dev_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function, Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = lambda n, p: p.ndim < 2 or \"bn\" in n or \"ln\" in n or \"bias\" in n or 'logit_scale' in n\n",
    "include = lambda n, p: not exclude(n, p)\n",
    "\n",
    "named_parameters = list(model.named_parameters())\n",
    "gain_or_bias_params = [p for n, p in named_parameters if exclude(n, p) and p.requires_grad]\n",
    "rest_params = [p for n, p in named_parameters if include(n, p) and p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "if args['adamw']:\n",
    "    optimizer = optim.AdamW(\n",
    "            [\n",
    "                {\"params\": gain_or_bias_params, \"weight_decay\": 0.},\n",
    "                {\"params\": rest_params, \"weight_decay\": args['WEIGHT_DECAY']},\n",
    "            ],\n",
    "            lr=args['LR'],\n",
    "            betas=args['BETAS'],\n",
    "            eps=args['EPS'],\n",
    "        )\n",
    "else:\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr = args['LR'],\n",
    "        betas = args['BETAS'],\n",
    "        eps = args['EPS'],\n",
    "        weight_decay = args['WEIGHT_DECAY'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = collections.defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "losss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = '/raid/xiaoyuz1/clip_model_checkpoint'\n",
    "end_epoch = args['START_EPOCH']+args['EPOCH']\n",
    "torch_path_name = os.path.join(root_folder, \"{}.pt\".format(wandb.run.name))\n",
    "acc_path_name = os.path.join(root_folder, \"{}.pickle\".format(wandb.run.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid/xiaoyuz1/clip_model_checkpoint/visionary-sun-30.pt /raid/xiaoyuz1/clip_model_checkpoint/visionary-sun-30.pickle\n"
     ]
    }
   ],
   "source": [
    "print(torch_path_name, acc_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_acc = {\n",
    "    'face' : -1,\n",
    "    'angel' : -1,\n",
    "}\n",
    "save_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(args['START_EPOCH'], args['START_EPOCH']+args['EPOCH']):\n",
    "# for epoch in range(50,100):\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images, list_txt = batch\n",
    "        images = images.to(device)\n",
    "        if args['open_clip']:\n",
    "            texts = open_clip.tokenize(list_txt).to(device)\n",
    "            \n",
    "            image_features, text_features, logit_scale = model(images, texts)\n",
    "            logits_per_image = logit_scale * image_features @ text_features.T\n",
    "            logits_per_text = logit_scale * text_features @ image_features.T\n",
    "            \n",
    "        else:\n",
    "            texts = clip.tokenize(list_txt).to(device)\n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        ground_truth = torch.arange(min(args['BATCH_SIZE'], len(texts)), dtype=torch.long, device=device)\n",
    "\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        wandb.log({\"loss\":total_loss.item()}, step=step)\n",
    "        \n",
    "        \n",
    "        if args['open_clip']:\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            if device == \"cpu\":\n",
    "                optimizer.step()\n",
    "            else : \n",
    "                convert_models_to_fp32(model)\n",
    "                optimizer.step()\n",
    "                clip.model.convert_weights(model)\n",
    "\n",
    "        if step % args['EVAL_EVERY'] == 0:\n",
    "            wandb_dict = {}\n",
    "            \n",
    "            pred, gt = evaluate(model, train_t_dataloader, device, use_open_clip=args['open_clip'])\n",
    "            acc = accuracy_score(np.argmax(pred, axis=1).reshape(-1,), gt)\n",
    "            wandb_dict['train_acc'] = acc\n",
    "            \n",
    "            for cat in args['TEST_CATEGORY']:\n",
    "                pred, gt = evaluate(model, dev_dataloaders[cat], device, use_open_clip=args['open_clip'])\n",
    "                acc = accuracy_score(np.argmax(pred, axis=1).reshape(-1,), gt)\n",
    "                # print(\"Epoch={} Iteration={}: {} acc={:.3f}\".format(epoch, step, cat, acc))\n",
    "                if len(accs[cat]) == 0 or acc >= highest_acc[cat]:\n",
    "                    highest_acc[cat] = acc\n",
    "                    save_model = True\n",
    "                \n",
    "                accs[cat].append(acc)\n",
    "                wandb_dict['dev_{}_acc'.format(cat)] = acc\n",
    "            \n",
    "            wandb.log(wandb_dict, step=step)\n",
    "            \n",
    "            if save_model:\n",
    "                wandb_dict2 = {}\n",
    "                for cat in args['TEST_CATEGORY']:\n",
    "                    pred, gt = evaluate(model, test_dataloaders[cat], device, use_open_clip=args['open_clip'])\n",
    "                    acc = accuracy_score(np.argmax(pred, axis=1).reshape(-1,), gt)\n",
    "                    wandb_dict2['test_{}_acc'.format(cat)] = acc\n",
    "                wandb.log(wandb_dict2, step=step)\n",
    "                \n",
    "                torch.save(\n",
    "                {\n",
    "                    'epoch': epoch,\n",
    "                    'iteration' : step,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': total_loss,\n",
    "                    'args': args,\n",
    "                }, torch_path_name)\n",
    "                wandb.save(torch_path_name)\n",
    "            save_model = False\n",
    "        \n",
    "        step += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epoch, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_dict2 = {}\n",
    "for cat in args['TEST_CATEGORY']:\n",
    "    pred, gt = evaluate(model, test_dataloaders[cat], device, use_open_clip=args['open_clip'])\n",
    "    acc = accuracy_score(np.argmax(pred, axis=1).reshape(-1,), gt)\n",
    "    wandb_dict2['test_{}_acc'.format(cat)] = acc\n",
    "wandb.log(wandb_dict2, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final TRAIN: acc=0.778\n",
      "Final DEV: face acc=0.693\n",
      "Final DEV: angel acc=0.573\n",
      "Final TEST: face acc=0.671\n",
      "Final TEST: angel acc=0.583\n"
     ]
    }
   ],
   "source": [
    "final_accs_dup = {\n",
    "    'test' : {},\n",
    "    'dev' : {},\n",
    "}\n",
    "preds = {\n",
    "    'test' : {},\n",
    "    'dev' : {},\n",
    "}\n",
    "gts = {\n",
    "    'test' : {},\n",
    "    'dev' : {},\n",
    "}\n",
    "\n",
    "pred, gt = evaluate(model, train_t_dataloader, device, use_open_clip=args['open_clip'])\n",
    "acc = accuracy_score(np.argmax(pred, axis=1).reshape(-1,), gt)\n",
    "print(\"Final TRAIN: acc={:.3f}\".format(acc))\n",
    "\n",
    "preds['train'] = pred\n",
    "gts['train'] = gt\n",
    "final_accs_dup['train'] = acc\n",
    "\n",
    "for cat in args['TEST_CATEGORY']:\n",
    "    pred, gt = evaluate(model, dev_dataloaders[cat], device, use_open_clip=args['open_clip'])\n",
    "    acc = accuracy_score(np.argmax(pred, axis=1).reshape(-1,), gt)\n",
    "    print(\"Final DEV: {} acc={:.3f}\".format(cat, acc))\n",
    "\n",
    "    final_accs_dup['dev'][cat] = acc\n",
    "    preds['dev'][cat] = pred\n",
    "    gts['dev'][cat] = gt\n",
    "\n",
    "for cat in args['TEST_CATEGORY']:\n",
    "    pred, gt = evaluate(model, test_dataloaders[cat], device, use_open_clip=args['open_clip'])\n",
    "    acc = accuracy_score(np.argmax(pred, axis=1).reshape(-1,), gt)\n",
    "    print(\"Final TEST: {} acc={:.3f}\".format(cat, acc))\n",
    "\n",
    "    final_accs_dup['test'][cat] = acc\n",
    "    preds['test'][cat] = pred\n",
    "    gts['test'][cat] = gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid/xiaoyuz1/clip_model_checkpoint/face/pleasant-tree-10.pt\n"
     ]
    }
   ],
   "source": [
    "print(torch_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        'epoch': end_epoch,\n",
    "        'iteration' : step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        'args': args,\n",
    "    }, torch_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(acc_path_name, 'wb+') as f:\n",
    "    pickle.dump((accs, final_accs_dup, preds,gts), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/xiaoyuz1/amazon_turk/wandb/run-20220413_125019-hz9m7k9m/files/woven-lake-9.pt',\n",
       " '/home/xiaoyuz1/amazon_turk/wandb/run-20220413_125019-hz9m7k9m/files/woven-lake-9.pt']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.save(torch_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/xiaoyuz1/amazon_turk/wandb/run-20220413_174349-vmfkmbnb/files/pleasant-tree-10.pickle']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.save(acc_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipdraw",
   "language": "python",
   "name": "clipdraw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
