{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import read_datasets as rd\n",
    "import constants as CONST\n",
    "from importlib import reload\n",
    "reload(rd)\n",
    "reload(CONST)\n",
    "\n",
    "import base64\n",
    "\n",
    "import pickle\n",
    "import spacy\n",
    "import collections\n",
    "import itertools, collections\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finetune\n",
    "reload(finetune)\n",
    "from finetune import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoyuz1/anaconda3/envs/clipdraw/lib/python3.8/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/home/xiaoyuz1/anaconda3/envs/clipdraw/lib/python3.8/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/xiaoyuz1/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "from nlpaug.util import Action\n",
    "import nltk\n",
    "\n",
    "os.environ[\"MODEL_DIR\"] = '../model'\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merinz\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "https://github.com/openai/CLIP/issues/83 \n",
    "\n",
    "@Zasder3 have created a PyTorch lighting version to train the CLIP https://github.com/Zasder3/train-CLIP\n",
    "\n",
    "@mitchellnw researchers at UW, Google, Stanford, Amazon, Columbia, and Berkeley also create their training code https://github.com/mlfoundations/open_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'BATCH_SIZE' : 64,\n",
    "    'EVAL_BATCH_SIZE' : 32,\n",
    "    'START_EPOCH' : 0,\n",
    "    'EPOCH' : 100,\n",
    "    'EVAL_EVERY' : 50,\n",
    "    'LR' : 5e-7, #optim\n",
    "    'BETAS' : (0.9,0.98),#optim\n",
    "    'EPS' : 1e-6,#optim\n",
    "    'WEIGHT_DECAY' : 0.01,#optim\n",
    "    'TRAIN_CATEGORY' : ['face'],#data\n",
    "    'TEST_CATEGORY' : ['face', 'angel'],\n",
    "    'IMAGE_PATH_TEMPLATE' : {\n",
    "            'face' : '/raid/xiaoyuz1/sketch_datasets/face_images_weight_5_all/{}.png',\n",
    "            'angel' : '/raid/xiaoyuz1/sketch_datasets/angel_images_weight_5_all/{}.png',\n",
    "        },\n",
    "    'TEMPLATE' : 0, \n",
    "    'line_diameter_scale' : [0.25,1.25],\n",
    "    'default_line_diameter': 10,\n",
    "    'rotate' : [-1/4*180, 1/4*180],\n",
    "    'trans' : [0.2, 0.2],\n",
    "    'scale' : [0.75,1.25],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/xiaoyuz1/amazon_turk/wandb/run-20220413_114817-3pzrvvod</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/erinz/clip-finetune/runs/3pzrvvod\" target=\"_blank\">woven-pine-7</a></strong> to <a href=\"https://wandb.ai/erinz/clip-finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"clip-finetune\", config=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'woven-pine-7'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.run.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.update(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_1</th>\n",
       "      <th>image_2</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>part</th>\n",
       "      <th>category</th>\n",
       "      <th>time</th>\n",
       "      <th>folder</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>tokenized_1</th>\n",
       "      <th>lower_1</th>\n",
       "      <th>no_punc_1</th>\n",
       "      <th>no_punc_str_1</th>\n",
       "      <th>tokenized_2</th>\n",
       "      <th>lower_2</th>\n",
       "      <th>no_punc_2</th>\n",
       "      <th>no_punc_str_2</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>722</td>\n",
       "      <td>260</td>\n",
       "      <td>AKQAI78JTXXC9</td>\n",
       "      <td>0</td>\n",
       "      <td>face</td>\n",
       "      <td>353.0</td>\n",
       "      <td>/raid/xiaoyuz1/amazon_turk/2022_03_21_release</td>\n",
       "      <td>linear</td>\n",
       "      <td>open oval</td>\n",
       "      <td>['linear']</td>\n",
       "      <td>['linear']</td>\n",
       "      <td>[linear]</td>\n",
       "      <td>linear</td>\n",
       "      <td>['open', 'oval']</td>\n",
       "      <td>['open', 'oval']</td>\n",
       "      <td>[open, oval]</td>\n",
       "      <td>open oval</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>556</td>\n",
       "      <td>144</td>\n",
       "      <td>AKQAI78JTXXC9</td>\n",
       "      <td>1</td>\n",
       "      <td>face</td>\n",
       "      <td>353.0</td>\n",
       "      <td>/raid/xiaoyuz1/amazon_turk/2022_03_21_release</td>\n",
       "      <td>small curved</td>\n",
       "      <td>large curved</td>\n",
       "      <td>['small', 'curved']</td>\n",
       "      <td>['small', 'curved']</td>\n",
       "      <td>[small, curved]</td>\n",
       "      <td>small curved</td>\n",
       "      <td>['large', 'curved']</td>\n",
       "      <td>['large', 'curved']</td>\n",
       "      <td>[large, curved]</td>\n",
       "      <td>large curved</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>419</td>\n",
       "      <td>452</td>\n",
       "      <td>AKQAI78JTXXC9</td>\n",
       "      <td>2</td>\n",
       "      <td>face</td>\n",
       "      <td>353.0</td>\n",
       "      <td>/raid/xiaoyuz1/amazon_turk/2022_03_21_release</td>\n",
       "      <td>wide u-shaped</td>\n",
       "      <td>small u-shaped</td>\n",
       "      <td>['wide', 'u', '-']</td>\n",
       "      <td>['wide', 'u', '-']</td>\n",
       "      <td>[wide, u]</td>\n",
       "      <td>wide u</td>\n",
       "      <td>['small', 'u', '-']</td>\n",
       "      <td>['small', 'u', '-']</td>\n",
       "      <td>[small, u]</td>\n",
       "      <td>small u</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>AKQAI78JTXXC9</td>\n",
       "      <td>4</td>\n",
       "      <td>face</td>\n",
       "      <td>353.0</td>\n",
       "      <td>/raid/xiaoyuz1/amazon_turk/2022_03_21_release</td>\n",
       "      <td>long</td>\n",
       "      <td>shorty spikey</td>\n",
       "      <td>['long']</td>\n",
       "      <td>['long']</td>\n",
       "      <td>[long]</td>\n",
       "      <td>long</td>\n",
       "      <td>['shorty', 'spikey']</td>\n",
       "      <td>['shorty', 'spikey']</td>\n",
       "      <td>[shorty, spikey]</td>\n",
       "      <td>shorty spikey</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71</td>\n",
       "      <td>416</td>\n",
       "      <td>AKQAI78JTXXC9</td>\n",
       "      <td>6</td>\n",
       "      <td>face</td>\n",
       "      <td>353.0</td>\n",
       "      <td>/raid/xiaoyuz1/amazon_turk/2022_03_21_release</td>\n",
       "      <td>circular</td>\n",
       "      <td>wide oval</td>\n",
       "      <td>['circular']</td>\n",
       "      <td>['circular']</td>\n",
       "      <td>[circular]</td>\n",
       "      <td>circular</td>\n",
       "      <td>['wide', 'oval']</td>\n",
       "      <td>['wide', 'oval']</td>\n",
       "      <td>[wide, oval]</td>\n",
       "      <td>wide oval</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_1  image_2      worker_id  part category   time  \\\n",
       "0      722      260  AKQAI78JTXXC9     0     face  353.0   \n",
       "1      556      144  AKQAI78JTXXC9     1     face  353.0   \n",
       "2      419      452  AKQAI78JTXXC9     2     face  353.0   \n",
       "3       11      100  AKQAI78JTXXC9     4     face  353.0   \n",
       "4       71      416  AKQAI78JTXXC9     6     face  353.0   \n",
       "\n",
       "                                          folder         text_1  \\\n",
       "0  /raid/xiaoyuz1/amazon_turk/2022_03_21_release         linear   \n",
       "1  /raid/xiaoyuz1/amazon_turk/2022_03_21_release   small curved   \n",
       "2  /raid/xiaoyuz1/amazon_turk/2022_03_21_release  wide u-shaped   \n",
       "3  /raid/xiaoyuz1/amazon_turk/2022_03_21_release           long   \n",
       "4  /raid/xiaoyuz1/amazon_turk/2022_03_21_release       circular   \n",
       "\n",
       "           text_2          tokenized_1              lower_1        no_punc_1  \\\n",
       "0       open oval           ['linear']           ['linear']         [linear]   \n",
       "1    large curved  ['small', 'curved']  ['small', 'curved']  [small, curved]   \n",
       "2  small u-shaped   ['wide', 'u', '-']   ['wide', 'u', '-']        [wide, u]   \n",
       "3   shorty spikey             ['long']             ['long']           [long]   \n",
       "4       wide oval         ['circular']         ['circular']       [circular]   \n",
       "\n",
       "  no_punc_str_1           tokenized_2               lower_2         no_punc_2  \\\n",
       "0        linear      ['open', 'oval']      ['open', 'oval']      [open, oval]   \n",
       "1  small curved   ['large', 'curved']   ['large', 'curved']   [large, curved]   \n",
       "2        wide u   ['small', 'u', '-']   ['small', 'u', '-']        [small, u]   \n",
       "3          long  ['shorty', 'spikey']  ['shorty', 'spikey']  [shorty, spikey]   \n",
       "4      circular      ['wide', 'oval']      ['wide', 'oval']      [wide, oval]   \n",
       "\n",
       "   no_punc_str_2  split  \n",
       "0      open oval  train  \n",
       "1   large curved  train  \n",
       "2        small u  train  \n",
       "3  shorty spikey  train  \n",
       "4      wide oval  train  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp = pd.read_csv('/raid/xiaoyuz1/amazon_turk/df_all_pair_withsplit.csv')\n",
    "dfp['no_punc_1'] = dfp.no_punc_1.apply(lambda x: [str(y).strip()[1:-1] for y in x[1:-1].split(',')])\n",
    "dfp['no_punc_2'] = dfp.no_punc_2.apply(lambda x: [str(y).strip()[1:-1] for y in x[1:-1].split(',')])\n",
    "dfp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "if device == \"cpu\":\n",
    "    model.float()\n",
    "else :\n",
    "    clip.model.convert_weights(model) \n",
    "    # Actually this line is unnecessary since clip by default already on float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=PIL.Image.BICUBIC)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x7ff35ff7ab80>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Load fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "# checkpoint = torch.load()\n",
    "\n",
    "# # Use these 3 lines if you use default model setting(not training setting) of the clip. \n",
    "# # For example, if you set context_length to 100 since your string is very long during training, \n",
    "# # then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
    "# checkpoint['model_state_dict'][\"input_resolution\"] = model.input_resolution #default is 224\n",
    "# checkpoint['model_state_dict'][\"context_length\"] = model.context_length # default is 77\n",
    "# checkpoint['model_state_dict'][\"vocab_size\"] = model.vocab_size \n",
    "\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### experimenting with text augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# text = \"small dotted eyes\"\n",
    "# import nlpaug.augmenter.word as naw\n",
    "\n",
    "# back_translation_aug = naw.BackTranslationAug(\n",
    "#     from_model_name='facebook/wmt19-en-de', \n",
    "#     to_model_name='facebook/wmt19-de-en'\n",
    "# )\n",
    "# back_translation_aug.augment(text)\n",
    "# aug = naw.WordEmbsAug(\n",
    "#     model_type='word2vec', model_path='/raid/xiaoyuz1/'+'GoogleNews-vectors-negative300.bin',\n",
    "#     action=\"substitute\")\n",
    "# augmented_text = aug.augment(text)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "def _convert_to_rgb(image):\n",
    "    return image.convert('RGB')\n",
    "\n",
    "def transform(n_px = 224, is_train = True):\n",
    "    normalize = T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    if is_train:\n",
    "        return T.Compose([\n",
    "            T.RandomResizedCrop(n_px, scale=(0.9, 1.0), interpolation=Image.BICUBIC),\n",
    "            T.RandomAffine(degrees=args['rotate'], translate=args['trans'], scale=args['scale']),\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            #T.RandomVerticalFlip(p=0.5),\n",
    "            _convert_to_rgb,\n",
    "            T.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    else:\n",
    "        return T.Compose([\n",
    "            T.Resize(n_px, interpolation=Image.BICUBIC),\n",
    "            T.CenterCrop(n_px),\n",
    "            _convert_to_rgb,\n",
    "            T.ToTensor(),\n",
    "            normalize,\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'naw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3ff64e210787>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m text_preprocess = [\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnaw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpellingAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnaw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSynonymAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_src\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnaw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mContextualWordEmbsAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'distilbert-base-uncased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"insert\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'naw' is not defined"
     ]
    }
   ],
   "source": [
    "text_preprocess = [\n",
    "    naw.SpellingAug(),\n",
    "    naw.SynonymAug(aug_src='wordnet'),\n",
    "    naw.ContextualWordEmbsAug(model_path='distilbert-base-uncased', action=\"insert\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_1,template_2 = template[args['TEMPLATE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {'test' : {}, 'dev' : {}}\n",
    "\n",
    "df = dfp[(dfp['category'].isin(args['TRAIN_CATEGORY'])) & (dfp['split'] == 'train')]\n",
    "df = df.reset_index(drop=True)\n",
    "df['caption_1'] = df.apply(template_1, axis=1)\n",
    "df['caption_2'] = df.apply(template_2, axis=1)\n",
    "dfs['train'] = df\n",
    "\n",
    "for split in ['test','dev']:\n",
    "    for cat in args['TEST_CATEGORY']:\n",
    "        df = dfp[(dfp['category'] == cat) & (dfp['split'] == split)]\n",
    "        df = df.reset_index(drop=True)\n",
    "        df['caption_1'] = df.apply(template_1, axis=1)\n",
    "        df['caption_2'] = df.apply(template_2, axis=1)\n",
    "        dfs[split][cat] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches:  55\n"
     ]
    }
   ],
   "source": [
    "img_path_template = args['IMAGE_PATH_TEMPLATE']\n",
    "\n",
    "train_dataset = ImageTextDataset(\n",
    "    dfs['train'], \n",
    "    img_path_template, \n",
    "    transform(n_px = 224, is_train = True),\n",
    "    [],\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = args['BATCH_SIZE'], shuffle=True) \n",
    "print(\"Number of train batches: \", len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import copy\n",
    "# def denormalize(tensor):\n",
    "#     mean = (0.48145466, 0.4578275, 0.40821073)\n",
    "#     std = (0.26862954, 0.26130258, 0.27577711)\n",
    "#     res = copy.deepcopy(tensor)\n",
    "#     for i, (m, s) in enumerate(zip(mean, std)):\n",
    "#         if len(res.shape) == 4:\n",
    "#             res[:,i,:,:].mul_(s).add_(s)\n",
    "#         else:\n",
    "#             res[i,:,:].mul_(s).add_(s)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mask_tensor = denormalize(train_dataset[0][0])\n",
    "# mask_PIL = T.ToPILImage()(mask_tensor).convert(\"L\")\n",
    "# mask_PIL = PIL.ImageOps.invert(mask_PIL)\n",
    "# mask_PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_dataset = ImageTextPairDataset(dfs['train'], img_path_template, preprocess)\n",
    "train_t_dataloader = DataLoader(train_t_dataset, batch_size = args['EVAL_BATCH_SIZE'], shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test batches:  16\n",
      "Number of test batches:  20\n"
     ]
    }
   ],
   "source": [
    "test_dataloaders = {}\n",
    "for cat in args['TEST_CATEGORY']:\n",
    "    test_dataset = ImageTextPairDataset(dfs['test'][cat], img_path_template, preprocess)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size = args['EVAL_BATCH_SIZE'], shuffle=False)\n",
    "    print(\"Number of test batches: \", len(test_dataloader))\n",
    "    \n",
    "    test_dataloaders[cat] = test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dev batches:  8\n",
      "Number of dev batches:  10\n"
     ]
    }
   ],
   "source": [
    "dev_dataloaders = {}\n",
    "for cat in args['TEST_CATEGORY']:\n",
    "    dev_dataset = ImageTextPairDataset(dfs['dev'][cat], img_path_template, preprocess)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size = args['EVAL_BATCH_SIZE'], shuffle=False)\n",
    "    print(\"Number of dev batches: \", len(dev_dataloader))\n",
    "    \n",
    "    dev_dataloaders[cat] = dev_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function, Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr = args['LR'],\n",
    "    betas = args['BETAS'],\n",
    "    eps = args['EPS'],\n",
    "    weight_decay = args['WEIGHT_DECAY'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = collections.defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "losss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for epoch in range(args['START_EPOCH'], args['START_EPOCH']+args['EPOCH']):\n",
    "for epoch in range(100,200):\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images, list_txt = batch\n",
    "        images = images.to(device)\n",
    "        texts = clip.tokenize(list_txt).to(device)\n",
    "\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        ground_truth = torch.arange(min(args['BATCH_SIZE'], len(texts)), dtype=torch.long, device=device)\n",
    "\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        wandb.log({\"loss\":total_loss.item()}, step=step)\n",
    "        \n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else : \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "        \n",
    "        if step % args['EVAL_EVERY'] == 0:\n",
    "            wandb_dict = {}\n",
    "            \n",
    "            pred, gt = evaluate(model, train_t_dataloader, device)\n",
    "            acc = accuracy_score(np.argmax(pred, axis=1).reshape(-1,), gt)\n",
    "            wandb_dict['train_acc'] = acc\n",
    "            \n",
    "            for cat in args['TEST_CATEGORY']:\n",
    "                pred, gt = evaluate(model, dev_dataloaders[cat], device)\n",
    "                acc = accuracy_score(np.argmax(pred, axis=1).reshape(-1,), gt)\n",
    "                # print(\"Epoch={} Iteration={}: {} acc={:.3f}\".format(epoch, step, cat, acc))\n",
    "                accs[cat].append(acc)\n",
    "                wandb_dict['dev_{}_acc'.format(cat)] = acc\n",
    "            \n",
    "            wandb.log(wandb_dict, step=step)\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final TRAIN: acc=0.773\n",
      "Final DEV: face acc=0.673\n",
      "Final DEV: angel acc=0.578\n",
      "Final TEST: face acc=0.672\n",
      "Final TEST: angel acc=0.591\n"
     ]
    }
   ],
   "source": [
    "final_accs_dup = {\n",
    "    'test' : {},\n",
    "    'dev' : {},\n",
    "}\n",
    "preds = {\n",
    "    'test' : {},\n",
    "    'dev' : {},\n",
    "}\n",
    "gts = {\n",
    "    'test' : {},\n",
    "    'dev' : {},\n",
    "}\n",
    "\n",
    "pred, gt = evaluate(model, train_t_dataloader, device)\n",
    "acc = accuracy_score(np.argmax(pred, axis=1).reshape(-1,), gt)\n",
    "print(\"Final TRAIN: acc={:.3f}\".format(acc))\n",
    "\n",
    "preds['train'] = pred\n",
    "gts['train'] = gt\n",
    "final_accs_dup['train'] = acc\n",
    "\n",
    "for cat in args['TEST_CATEGORY']:\n",
    "    pred, gt = evaluate(model, dev_dataloaders[cat], device)\n",
    "    acc = accuracy_score(np.argmax(pred, axis=1).reshape(-1,), gt)\n",
    "    print(\"Final DEV: {} acc={:.3f}\".format(cat, acc))\n",
    "\n",
    "    final_accs_dup['dev'][cat] = acc\n",
    "    preds['dev'][cat] = pred\n",
    "    gts['dev'][cat] = gt\n",
    "\n",
    "for cat in args['TEST_CATEGORY']:\n",
    "    pred, gt = evaluate(model, test_dataloaders[cat], device)\n",
    "    acc = accuracy_score(np.argmax(pred, axis=1).reshape(-1,), gt)\n",
    "    print(\"Final TEST: {} acc={:.3f}\".format(cat, acc))\n",
    "\n",
    "    final_accs_dup['test'][cat] = acc\n",
    "    preds['test'][cat] = pred\n",
    "    gts['test'][cat] = gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = '/raid/xiaoyuz1/clip_model_checkpoint'\n",
    "end_epoch = args['START_EPOCH']+args['EPOCH']\n",
    "torch_path_name = os.path.join(root_folder, \"all/{}.pt\".format(wandb.run.name))\n",
    "acc_path_name = os.path.join(root_folder, \"all/{}.pickle\".format(wandb.run.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid/xiaoyuz1/clip_model_checkpoint/all/giddy-field-6.pt\n"
     ]
    }
   ],
   "source": [
    "print(torch_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/xiaoyuz1/amazon_turk/wandb/run-20220413_082652-lc75n5s5/files/giddy-field-6.pt']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.save(torch_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        'epoch': end_epoch,\n",
    "        'iteration' : step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        'args': args,\n",
    "    }, torch_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(acc_path_name, 'wb+') as f:\n",
    "    pickle.dump((accs, final_accs_dup, preds,gts), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/xiaoyuz1/amazon_turk/wandb/run-20220413_082652-lc75n5s5/files/giddy-field-6.pickle']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.save(acc_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipdraw",
   "language": "python",
   "name": "clipdraw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
