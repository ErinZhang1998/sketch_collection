{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import read_datasets as rd\n",
    "import constants as CONST\n",
    "from importlib import reload\n",
    "reload(rd)\n",
    "reload(CONST)\n",
    "import base64\n",
    "import pickle\n",
    "import collections\n",
    "import itertools, collections\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import cv2 \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.rnn as rnn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo = pd.read_csv('/raid/xiaoyuz1/amazon_turk/df_all.csv')\n",
    "dfo['no_punc_1'] = dfo.no_punc_1.apply(lambda x: [str(y).strip()[1:-1] for y in x[1:-1].split(',')])\n",
    "\n",
    "dfp = pd.read_csv('/raid/xiaoyuz1/amazon_turk/df_all_pair.csv')\n",
    "dfp['no_punc_1'] = dfp.no_punc_1.apply(lambda x: [str(y).strip()[1:-1] for y in x[1:-1].split(',')])\n",
    "dfp['no_punc_2'] = dfp.no_punc_2.apply(lambda x: [str(y).strip()[1:-1] for y in x[1:-1].split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, OneHotCategorical\n",
    "\n",
    "out_dim = 1\n",
    "n_components = 4\n",
    "param_dim = 2 * out_dim * n_components\n",
    "params = torch.rand((5, param_dim))\n",
    "mean, sd = torch.split(params, params.shape[1] // 2, dim=1)\n",
    "mean = torch.stack(mean.split(mean.shape[1] // n_components, 1))\n",
    "sd = torch.stack(sd.split(sd.shape[1] // n_components, 1))\n",
    "\n",
    "print(mean.shape, sd.shape, (F.elu(sd)+1+1e-7).transpose(0, 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, 1)\n",
    "y = torch.randn(5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = Normal(mean.transpose(0, 1), (F.elu(sd)+1+1e-7).transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = y.unsqueeze(1).expand_as(normal.loc)\n",
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglik = normal.log_prob(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaced torch.exp(sd) with ELU plus to improve numerical stability\n",
    "# added epsilon to avoid zero scale\n",
    "# due to non associativity of floating point add, 1 and 1e-7 need to be added seperately\n",
    "loglik.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## use pretrained word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "device = \"cuda\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False) #Must set jit=False for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_folder = '/raid/xiaoyuz1/clip_model_checkpoint'\n",
    "torch_path_name = os.path.join(root_folder, \"pleasant-tree-10.pt\")\n",
    "checkpoint = torch.load(torch_path_name)\n",
    "print(checkpoint.keys())\n",
    "args = checkpoint['args']\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## testing out sketch-rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, '/home/xiaoyuz1/amazon_turk/Pytorch-Sketch-RNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sketch_rnn\n",
    "reload(sketch_rnn)\n",
    "from sketch_rnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sketch_dataset = np.load(hp.data_location, encoding='latin1', allow_pickle=True)\n",
    "# sketch_data = sketch_dataset['train']\n",
    "# print(sketch_data.shape)\n",
    "# sketch_data, kept_idx = purify(sketch_data)\n",
    "# scale_factor = calculate_normalizing_scale_factor(sketch_data)\n",
    "# sketch_data = normalize(sketch_data)\n",
    "# Nmax = max_size(sketch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch, lengths = make_batch(hp.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mask, dx, dy, p = model.make_target(batch, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "z, mu, sigma = model.encoder(batch, hp.batch_size)\n",
    "sos = torch.stack([torch.Tensor([0,0,1,0,0])]*hp.batch_size).cuda().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_init = torch.cat([sos, batch], 0)\n",
    "z_stack = torch.stack([z] * (Nmax + 1)) \n",
    "inputs = torch.cat([batch_init, z_stack], 2) \n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pi,mu_x,mu_y,sigma_x,sigma_y,rho_xy,q,_,_ = model.decoder(inputs, z) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.pi = pi\n",
    "model.mu_x = mu_x\n",
    "model.mu_y = mu_y\n",
    "model.sigma_x = sigma_x\n",
    "model.sigma_y = sigma_y\n",
    "model.rho_xy = rho_xy\n",
    "model.q = q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdf = model.bivariate_normal_pdf(dx, dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pi.shape, mu_x.shape, mu_y.shape, sigma_x.shape, sigma_y.shape, rho_xy.shape, q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class HParams():\n",
    "    def __init__(self):\n",
    "        self.word_embed_dim = 128\n",
    "        self.lstm_output_dim = 512\n",
    "        self.lstm_layers = 2 \n",
    "        self.lstm_drop_prob = 0.4\n",
    "        self.num_primitives = 6\n",
    "        self.num_transformation_params = 6\n",
    "        self.vocab_size = None\n",
    "        self.M = 2\n",
    "        self.weight_decay = 0.0\n",
    "        self.start_epoch = 0\n",
    "        self.num_epochs = 50\n",
    "        self.wandb_enabled = False \n",
    "        self.wandb_project_name = \"doodler-draw\"\n",
    "        self.wandb_project_entity=\"erinz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hp = HParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hp.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import LongTensor\n",
    "from torch.nn import Embedding, LSTM\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# We want to run LSTM on a batch following 3 character sequences\n",
    "seqs = ['long_str',  # len = 8\n",
    "        'tiny',      # len = 4\n",
    "        'medium']    # len = 6\n",
    "\n",
    "vocab = ['<pad>'] + sorted(set([char for seq in seqs for char in seq]))\n",
    "vectorized_seqs = [[vocab.index(tok) for tok in seq]for seq in seqs]\n",
    "##--------------------##\n",
    "embed = Embedding(len(vocab), 4) # embedding_dim = 4\n",
    "lstm = LSTM(input_size=4, hidden_size=5, batch_first=True) # input_dim = 4, hidden_dim = 5\n",
    "seq_lengths = LongTensor(list(map(len, vectorized_seqs)))\n",
    "seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
    "\n",
    "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "    seq_tensor[idx, :seqlen] = LongTensor(seq)\n",
    "\n",
    "seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "seq_tensor = seq_tensor[perm_idx]\n",
    "\n",
    "embedded_seq_tensor = embed(seq_tensor)\n",
    "packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\n",
    "packed_output, (ht, ct) = lstm(packed_input)\n",
    "output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ht[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fitting primitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = h = 256\n",
    "TEMPLATE_DICT = {\n",
    "    'arc' : lambda n : rd.generate_arc(n1=n, radius=10, x0=0, y0=0, template_size=w),\n",
    "    'circle' : lambda n : rd.generate_circle(n1=n, radius=100, x0=0, y0=0, template_size=w),\n",
    "    'square' : lambda n : rd.generate_square(n1=n, template_size=w),\n",
    "    'semicircle' : lambda n : rd.generate_semicircle(n1=n, radius=100, x0=0, y0=0, template_size=w),\n",
    "    'zigzag1' : lambda n : rd.generate_zigzag1(n1=n, template_size=w),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_spline_num_sampled_points = n = 200\n",
    "use_projective = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "templates = {}\n",
    "template_dict_processed = {}\n",
    "for i,(k,v) in enumerate(TEMPLATE_DICT.items()):\n",
    "    arr = v(b_spline_num_sampled_points)\n",
    "    template_dict_processed[k] = arr\n",
    "    templates[i] = (k,arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import primitive_selector as ps\n",
    "reload(ps)\n",
    "all_data = ps.prepare_data(dfo, templates, num_sampled_points = 200, use_projective = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/raid/xiaoyuz1/primitive_selector_training_data/july_13_all.pkl\", \"wb+\") as f:\n",
    "    pickle.dump(all_data_expanded, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [\"train\", \"dev\", \"test\"]:\n",
    "    data_split = [x for x in all_data_expanded if x[\"split\"] == t]\n",
    "    data_split_dict = dict(zip(range(len(data_split)), data_split))\n",
    "    with open(f\"/raid/xiaoyuz1/primitive_selector_training_data/july_13_{t}.pkl\", \"wb+\") as f:\n",
    "        pickle.dump(data_split, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pickle.load(open(\"/raid/xiaoyuz1/primitive_selector_training_data/july_13_all.pkl\", \"rb\"))\n",
    "# df = pd.DataFrame.from_dict(all_data, orient='index')\n",
    "df = pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(rd)\n",
    "all_data_expanded = []\n",
    "for info in all_data:\n",
    "    if info['category'] == 'face':\n",
    "        drawing_raw = CONST.face_json['train_data'][info['image_idx']]\n",
    "        part_indices = list(CONST.face_parts_idx_dict_doodler.keys())\n",
    "    else:\n",
    "        drawing_raw = CONST.angel_json['train_data'][info['image_idx']]\n",
    "        part_indices = list(CONST.angel_parts_idx_dict_doodler.keys())\n",
    "\n",
    "    parts = rd.process_quickdraw_to_part_convex_hull(\n",
    "        drawing_raw,\n",
    "        part_indices,\n",
    "        b_spline_num_sampled_points=num_sampled_points,\n",
    "    )\n",
    "    data = parts[info['part']]\n",
    "    template = templates[info['primitive_type']][1]\n",
    "    transform_mat = np.asarray(info['M']).astype(float).reshape(3,3)\n",
    "    theta, scale_mat, shear_mat = decompose_affine(transform_mat)\n",
    "    \n",
    "    info[\"theta\"] = theta\n",
    "    info[\"sx\"] = scale_mat[0][0]\n",
    "    info[\"sy\"] = scale_mat[1][1]\n",
    "    info[\"hx\"] = shear_mat[0][1]\n",
    "    info[\"tx\"] = transform_mat[0][2]\n",
    "    info[\"ty\"] = transform_mat[1][2]\n",
    "    \n",
    "    T_rotate = rd.get_rotation_matrix(theta)\n",
    "    T_scale = rd.get_scale_matrix(info[\"sx\"], info[\"sy\"])\n",
    "    T_shear = rd.get_shear_matrix(info[\"hx\"], 0)\n",
    "    T_translate = T_translate = rd.get_translation_matrix(info[\"tx\"], info[\"ty\"])\n",
    "    T = T_translate @ T_rotate @ T_scale @ T_shear\n",
    "    #print(T.data)\n",
    "    if not check_close(info[\"M\"].reshape(-1,), T.reshape(-1,)):\n",
    "        print(info)\n",
    "    all_data_expanded.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prepared_data(info, num_sampled_points = 200, use_projective=False):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12,24)) \n",
    "    if info['category'] == 'face':\n",
    "        drawing_raw = CONST.face_json['train_data'][info['image_idx']]\n",
    "        part_indices = list(CONST.face_parts_idx_dict_doodler.keys())\n",
    "    else:\n",
    "        drawing_raw = CONST.angel_json['train_data'][info['image_idx']]\n",
    "        part_indices = list(CONST.angel_parts_idx_dict_doodler.keys())\n",
    "\n",
    "    parts = rd.process_quickdraw_to_part_convex_hull(\n",
    "        drawing_raw,\n",
    "        part_indices,\n",
    "        b_spline_num_sampled_points=num_sampled_points,\n",
    "    )\n",
    "    data = parts[info['part']]\n",
    "    template = templates[info['primitive_type']][1]\n",
    "    transform_mat = np.asarray(info['M']).astype(float).reshape(3,3)\n",
    "    axs[0].scatter(template[:,0], template[:,1], s=1, c='b')\n",
    "    \n",
    "    if use_projective:\n",
    "        result = cv2.perspectiveTransform(template, transform_mat).reshape(-1,2)\n",
    "    else:\n",
    "        result = cv2.transform(np.array([template]).astype(\n",
    "            np.float32), transform_mat)[0][:,:-1]\n",
    "    \n",
    "    \n",
    "    axs[0].scatter(result[:,0], result[:,1], s=1, c='r')\n",
    "    \n",
    "    theta, scale_mat, shear_mat = decompose_affine(transform_mat)\n",
    "    T_rotate = rd.get_rotation_matrix(theta)\n",
    "    T_scale = rd.get_scale_matrix(scale_mat[0][0], scale_mat[1][1])\n",
    "    T_shear = rd.get_shear_matrix(shear_mat[0][1], shear_mat[1][0])\n",
    "    tx,ty = transform_mat[0][2],transform_mat[1][2]\n",
    "    T_translate = rd.get_translation_matrix(tx,ty)\n",
    "    \n",
    "    result1 = cv2.transform(np.array([template]).astype(\n",
    "            np.float32), T_rotate)[0][:,:-1]\n",
    "    axs[0].scatter(result1[:,0], result1[:,1], s=1, c='m')\n",
    "    \n",
    "    result2 = cv2.transform(np.array([template]).astype(\n",
    "            np.float32), T_rotate @ T_scale @ T_shear)[0][:,:-1]\n",
    "    axs[0].scatter(result2[:,0], result2[:,1], s=1, alpha=0.5, c='pink')\n",
    "\n",
    "    result3 = cv2.transform(np.array([template]).astype(\n",
    "            np.float32), T_translate @ T_rotate @ T_scale @ T_shear)[0][:,:-1]\n",
    "    axs[0].scatter(result3[:,0], result3[:,1], s=1, c='lime', alpha=0.5)\n",
    "    \n",
    "    axs[1].scatter(data[:,0], data[:,1], alpha=0.5, s=1, c='lime')\n",
    "\n",
    "    axs[0].axis(xmin=-w,xmax=w)\n",
    "    axs[0].axis(ymin=h,ymax=-h)\n",
    "    axs[1].axis(xmin=-w,xmax=w)\n",
    "    axs[1].axis(ymin=h,ymax=-h)\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_prepared_data(df.iloc[892])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### plot GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "M_all = np.asarray(df.M.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "X = M_all[:,0].reshape(-1, 1)\n",
    "print(X.min(), X.max())\n",
    "\n",
    "N = np.arange(1, 11)\n",
    "models = [None for i in range(len(N))]\n",
    "\n",
    "for i in range(len(N)):\n",
    "    models[i] = GaussianMixture(N[i]).fit(X)\n",
    "\n",
    "# compute the AIC and the BIC\n",
    "AIC = [m.aic(X) for m in models]\n",
    "BIC = [m.bic(X) for m in models]\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "#  We'll use three panels:\n",
    "#   1) data + best-fit mixture\n",
    "#   2) AIC and BIC vs number of components\n",
    "#   3) probability that a point came from each component\n",
    "\n",
    "fig = plt.figure(figsize=(15, 20))\n",
    "fig.subplots_adjust(left=0.12, right=0.97,\n",
    "                    bottom=0.21, top=0.9, wspace=0.5)\n",
    "\n",
    "\n",
    "# plot 1: data + best-fit mixture\n",
    "ax = fig.add_subplot(311)\n",
    "M_best = models[np.argmin(AIC)]\n",
    "\n",
    "x = np.linspace(X.min(), X.max(), 1000)\n",
    "logprob = M_best.score_samples(x.reshape(-1, 1))\n",
    "responsibilities = M_best.predict_proba(x.reshape(-1, 1))\n",
    "pdf = np.exp(logprob)\n",
    "pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "\n",
    "ax.hist(X, 30, density=True, histtype='stepfilled', alpha=0.4)\n",
    "ax.plot(x, pdf, '-k')\n",
    "ax.plot(x, pdf_individual, '--k')\n",
    "ax.text(0.04, 0.96, \"Best-fit Mixture\",\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$p(x)$')\n",
    "\n",
    "\n",
    "# plot 2: AIC and BIC\n",
    "ax = fig.add_subplot(312)\n",
    "ax.plot(N, AIC, '-k', label='AIC')\n",
    "ax.plot(N, BIC, '--k', label='BIC')\n",
    "ax.set_xlabel('n. components')\n",
    "ax.set_ylabel('information criterion')\n",
    "ax.legend(loc=2)\n",
    "\n",
    "\n",
    "# plot 3: posterior probabilities for each component\n",
    "ax = fig.add_subplot(313)\n",
    "\n",
    "p = responsibilities\n",
    "p = p[:, (1, 0, 2)]  # rearrange order so the plot looks better\n",
    "p = p.cumsum(1).T\n",
    "\n",
    "ax.fill_between(x, 0, p[0], color='gray', alpha=0.3)\n",
    "ax.fill_between(x, p[0], p[1], color='gray', alpha=0.5)\n",
    "ax.fill_between(x, p[1], 1, color='gray', alpha=0.7)\n",
    "ax.set_xlim(-6, 6)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel(r'$p({\\rm class}|x)$')\n",
    "\n",
    "ax.text(-5, 0.3, 'class 1', rotation='vertical')\n",
    "ax.text(0, 0.5, 'class 2', rotation='vertical')\n",
    "ax.text(3, 0.3, 'class 3', rotation='vertical')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(M_best.weights_)\n",
    "print(M_best.means_)\n",
    "print(M_best.covariances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prepared_data(df.iloc[110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawing_arr = CONST.face_json['train_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(rd)\n",
    "from tqdm import tqdm\n",
    "\n",
    "M_arr = collections.defaultdict(lambda : np.zeros(9))\n",
    "M_name = collections.defaultdict(lambda : \" \")\n",
    "M_mse = collections.defaultdict(float)\n",
    "\n",
    "\n",
    "for idx in tqdm(range(len(drawing_arr))):\n",
    "# for idx in range(len(drawing_arr)):\n",
    "#     print(idx)\n",
    "    \n",
    "    strokes_spline_fitted = rd.process_quickdraw_to_part_convex_hull(\n",
    "        drawing_arr[idx],\n",
    "        list(CONST.face_parts_idx_dict.keys()),\n",
    "        b_spline_num_sampled_points=n,\n",
    "    )\n",
    "\n",
    "    for _, (part_type, sketch_data) in enumerate(strokes_spline_fitted.items()):\n",
    "        \n",
    "        mat, prim_name, prim_mse = rd.get_transform_smallest_mse(\n",
    "            sketch_data, template_dict_processed, use_projective=use_projective,\n",
    "        )\n",
    "        M_arr[(idx, part_type)] = mat\n",
    "        M_name[(idx, part_type)] = prim_name\n",
    "        M_mse[(idx, part_type)] = prim_mse\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/raid/xiaoyuz1/SPG_Face_Part_256_projective_{}_mse.pkl'.format(use_projective), \"wb+\") as f:\n",
    "    pickle.dump((dict(M_arr), dict(M_name), dict(M_mse)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'/raid/xiaoyuz1/SPG_Face_Part_256_projective_{}_mse.pkl'.format(use_projective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_dict = dict(filter(lambda x: x[1] == 'line', M_name.items()))\n",
    "print(len(line_dict), line_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_arr, M_name = None,None\n",
    "M_mse = None\n",
    "fname = '/raid/xiaoyuz1/SPG_Face_Part_256_projective_False_mse.pkl'\n",
    "with open(fname, \"rb\") as f:\n",
    "    M_arr, M_name, M_mse = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_l = list(filter(lambda x: not math.isinf(x), list(M_mse.values())))\n",
    "np.mean(mse_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_list = list(map(lambda x: int(x[0]), M_name.keys()))\n",
    "stroke_list = list(map(lambda x: int(x[1]), M_name.keys()))\n",
    "arr_list = list(M_arr.values())\n",
    "name_list = list(M_name.values())\n",
    "mse_list = list(M_mse.values())\n",
    "\n",
    "data = {'image': image_list, 'stroke' : stroke_list, 'M' : arr_list, 'primitive' : name_list, 'mse' : mse_list}\n",
    "\n",
    "for c_idx,c in enumerate(np.asarray(arr_list).T):\n",
    "    if c_idx > 5:\n",
    "        continue\n",
    "    data['M{}'.format(str(c_idx))] = c\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = df[df.mse.apply(lambda x: not math.isinf(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_all = np.asarray(dfa.M.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "stroke_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rd.show_these_sketches(\n",
    "    CONST.face_json,\n",
    "    [idx], \n",
    "    [str(idx)], \n",
    "    [[]], \n",
    "    num_pngs_per_row = 6,\n",
    "    row_figsize = 4,\n",
    "    column_figsize = 4,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strokes_spline_fitted = rd.process_quickdraw_to_part_convex_hull(\n",
    "    drawing_arr[idx],\n",
    "    list(CONST.face_parts_idx_dict.keys()),\n",
    "    b_spline_num_sampled_points=n,\n",
    ")\n",
    "\n",
    "# strokes_spline_fitted = rd.process_quickdraw_to_stroke_no_normalize(\n",
    "#     drawing_arr[idx], \n",
    "#     b_spline_num_sampled_points=200,\n",
    "# )\n",
    "# print(len(strokes_spline_fitted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "\n",
    "parts_indices = list(CONST.face_parts_idx_dict.keys())\n",
    "\n",
    "drawing_raw = np.asarray(drawing_arr[idx])\n",
    "drawing_raw[:,0] = np.cumsum(drawing_raw[:,0], 0) + 25\n",
    "drawing_raw[:,1] = np.cumsum(drawing_raw[:,1], 0) + 25\n",
    "\n",
    "parts = []\n",
    "part_idx_part = {}\n",
    "\n",
    "for k in parts_indices:\n",
    "    strokes = drawing_raw[drawing_raw[:,-1] == k]\n",
    "    if len(strokes) < 1:\n",
    "        continue\n",
    "    parts.append(strokes)\n",
    "    part_idx_part[k] = strokes[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_arr_idx, M_name_idx, M_mse_idx = {},{},{}\n",
    "for stroke_index, (part_type, data) in enumerate(strokes_spline_fitted.items()):\n",
    "        \n",
    "    mat, prim_name, prim_mse = rd.get_transform_smallest_mse(\n",
    "        data, template_dict_processed, use_projective=use_projective,\n",
    "    )\n",
    "    M_arr_idx[(idx, part_type)] = mat\n",
    "    M_name_idx[(idx, part_type)] = prim_name\n",
    "    M_mse_idx[(idx, part_type)] = prim_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "for k, M in M_arr_idx.items():\n",
    "#     if k[1] != 3:\n",
    "#         continue\n",
    "    data = strokes_spline_fitted[k[1]]\n",
    "    \n",
    "    template = template_dict_processed[M_name_idx[(idx, k[1])]]\n",
    "    Mm = M.reshape(3,3)\n",
    "    \n",
    "    result, mse = rd.get_transformed_template(template, data, Mm,projective=use_projective)\n",
    "    ax1.scatter(result[:,0], result[:,1], label=\"transformed template\", s=1, c='r')\n",
    "    ax2.scatter(data[:,0], data[:,1], label=\"data\", alpha=0.5, s=1, c='g')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.xlim(0,w)\n",
    "# plt.ylim(h,0)\n",
    "\n",
    "\n",
    "ax1.axis(xmin=0,xmax=w)\n",
    "ax1.axis(ymin=h,ymax=0)\n",
    "\n",
    "ax2.axis(xmin=0,xmax=w)\n",
    "ax2.axis(ymin=h,ymax=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_arr_idx, M_name_idx, M_mse_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "for index, entry in dfa[dfa.image == idx].iterrows():\n",
    "    data = strokes_spline_fitted[entry.stroke]\n",
    "    \n",
    "    template_func = TEMPLATE_DICT[entry.primitive]\n",
    "    template = template_func(n)\n",
    "    \n",
    "    print(entry.M.reshape(3,3))\n",
    "    \n",
    "    result, mse = rd.get_transformed_template(template, data, entry.M.reshape(3,3),projective=use_projective)\n",
    "    ax1.scatter(result[:,0], result[:,1], label=\"transformed template\", s=1, c='r')\n",
    "    ax2.scatter(data[:,0], data[:,1], label=\"data\", alpha=0.5, s=1, c='g')\n",
    "    orig_data = part_idx_part[int(entry.stroke)]\n",
    "    ax2.plot(orig_data[:,0], orig_data[:,1], alpha=0.5, c='b')\n",
    "    \n",
    "#     ax1.plot(result[:,0], result[:,1], label=\"transformed template\", c='r')\n",
    "#     ax2.plot(data[:,0], data[:,1], label=\"data\", alpha=0.5,c='g')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.xlim(0,w)\n",
    "# plt.ylim(h,0)\n",
    "\n",
    "ax1.axis(xmin=0,xmax=w)\n",
    "ax1.axis(ymin=h,ymax=0)\n",
    "\n",
    "ax2.axis(xmin=0,xmax=w)\n",
    "ax2.axis(ymin=h,ymax=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "scrolled": true
   },
   "source": [
    "## old icp code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "from numpy.random import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def del_miss(indeces, dist, max_dist, th_rate = 0.8):\n",
    "    th_dist = max_dist * th_rate\n",
    "    return np.array([indeces[0][np.where(dist.T[0] < th_dist)]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def is_converge(Tr, scale):\n",
    "    delta_angle = 0.0001\n",
    "    delta_scale = scale * 0.0001\n",
    "    \n",
    "    min_cos = 1 - delta_angle\n",
    "    max_cos = 1 + delta_angle\n",
    "    min_sin = -delta_angle\n",
    "    max_sin = delta_angle\n",
    "    min_move = -delta_scale\n",
    "    max_move = delta_scale\n",
    "    \n",
    "    return min_cos < Tr[0, 0] and Tr[0, 0] < max_cos and \\\n",
    "           min_cos < Tr[1, 1] and Tr[1, 1] < max_cos and \\\n",
    "           min_sin < -Tr[1, 0] and -Tr[1, 0] < max_sin and \\\n",
    "           min_sin < Tr[0, 1] and Tr[0, 1] < max_sin and \\\n",
    "           min_move < Tr[0, 2] and Tr[0, 2] < max_move and \\\n",
    "           min_move < Tr[1, 2] and Tr[1, 2] < max_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def icp(d1, d2, max_iterate = 100):\n",
    "    src = np.array([d1.T], copy=True).astype(np.float32)\n",
    "    dst = np.array([d2.T], copy=True).astype(np.float32)\n",
    "    \n",
    "    knn = cv2.KNearest()\n",
    "    responses = np.array(range(len(d2[0]))).astype(np.float32)\n",
    "    knn.train(src[0], responses)\n",
    "        \n",
    "    Tr = np.array([[np.cos(0), -np.sin(0), 0],\n",
    "                   [np.sin(0), np.cos(0),  0],\n",
    "                   [0,         0,          1]])\n",
    "\n",
    "    dst = cv2.transform(dst, Tr[0:2])\n",
    "    max_dist = sys.maxint\n",
    "    \n",
    "    scale_x = np.max(d1[0]) - np.min(d1[0])\n",
    "    scale_y = np.max(d1[1]) - np.min(d1[1])\n",
    "    scale = max(scale_x, scale_y)\n",
    "       \n",
    "    for i in range(max_iterate):\n",
    "        ret, results, neighbours, dist = knn.find_nearest(dst[0], 1)\n",
    "        \n",
    "        indeces = results.astype(np.int32).T     \n",
    "        indeces = del_miss(indeces, dist, max_dist)  \n",
    "        \n",
    "        T = cv2.estimateRigidTransform(dst[0, indeces], src[0, indeces], True)\n",
    "\n",
    "        max_dist = np.max(dist)\n",
    "        dst = cv2.transform(dst, T)\n",
    "        Tr = np.dot(np.vstack((T,[0,0,1])), Tr)\n",
    "        \n",
    "        if (is_converge(T, scale)):\n",
    "            break\n",
    "        \n",
    "    return Tr[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    point_count = 100\n",
    "    th = np.pi / 8\n",
    "    move = np.array([[0.30], [0.5]])\n",
    "    rnd_scale = 0.03\n",
    "    x1 = np.linspace(0, 1.1, point_count)\n",
    "    y1 = np.sin(x1 * np.pi)\n",
    "    d1 = np.array([x1, y1])\n",
    "\n",
    "    rot = np.array([[np.cos(th), -np.sin(th)], [np.sin(th), np.cos(th)]])\n",
    "    rand = np.random.rand(2, point_count)*rnd_scale\n",
    "    d2 = np.dot(rot, d1) + move\n",
    "    d2 = np.add(d2, rand)\n",
    "\n",
    "    plt.plot(d1[0], d1[1])\n",
    "    plt.plot(d2[0], d2[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    ret = icp(d1, d2)\n",
    "    \n",
    "    plt.plot(d1[0], d1[1])\n",
    "    dst = np.array([d2.T], copy=True).astype(np.float32)\n",
    "    dst = cv2.transform(dst, ret)\n",
    "    plt.plot(dst[0].T[0], dst[0].T[1])\n",
    "    plt.show()\n",
    "    \n",
    "    print ret[0][0] * ret[0][0] + ret[0][1] * ret[0][1]\n",
    "    print np.arccos(ret[0][0]) / 2 / np.pi * 360\n",
    "    print np.arcsin(ret[0][1]) / 2 / np.pi * 360\n",
    "\n",
    "    print ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def icp(a, b,\n",
    "        max_time = 1\n",
    "    ):\n",
    "    import cv2\n",
    "    import numpy\n",
    "    import copy\n",
    "    import pylab\n",
    "    import time\n",
    "    import sys\n",
    "    import sklearn.neighbors\n",
    "    import scipy.optimize\n",
    "\n",
    "\n",
    "\n",
    "    def res(p,src,dst):\n",
    "        T = numpy.matrix([[numpy.cos(p[2]),-numpy.sin(p[2]),p[0]],\n",
    "        [numpy.sin(p[2]), numpy.cos(p[2]),p[1]],\n",
    "        [0 ,0 ,1 ]])\n",
    "        n = numpy.size(src,0)\n",
    "        xt = numpy.ones([n,3])\n",
    "        xt[:,:-1] = src\n",
    "        xt = (xt*T.T).A\n",
    "        d = numpy.zeros(numpy.shape(src))\n",
    "        d[:,0] = xt[:,0]-dst[:,0]\n",
    "        d[:,1] = xt[:,1]-dst[:,1]\n",
    "        r = numpy.sum(numpy.square(d[:,0])+numpy.square(d[:,1]))\n",
    "        return r\n",
    "\n",
    "    def jac(p,src,dst):\n",
    "        T = numpy.matrix([[numpy.cos(p[2]),-numpy.sin(p[2]),p[0]],\n",
    "        [numpy.sin(p[2]), numpy.cos(p[2]),p[1]],\n",
    "        [0 ,0 ,1 ]])\n",
    "        n = numpy.size(src,0)\n",
    "        xt = numpy.ones([n,3])\n",
    "        xt[:,:-1] = src\n",
    "        xt = (xt*T.T).A\n",
    "        d = numpy.zeros(numpy.shape(src))\n",
    "        d[:,0] = xt[:,0]-dst[:,0]\n",
    "        d[:,1] = xt[:,1]-dst[:,1]\n",
    "        dUdth_R = numpy.matrix([[-numpy.sin(p[2]),-numpy.cos(p[2])],\n",
    "                            [ numpy.cos(p[2]),-numpy.sin(p[2])]])\n",
    "        dUdth = (src*dUdth_R.T).A\n",
    "        g = numpy.array([  numpy.sum(2*d[:,0]),\n",
    "                        numpy.sum(2*d[:,1]),\n",
    "                        numpy.sum(2*(d[:,0]*dUdth[:,0]+d[:,1]*dUdth[:,1])) ])\n",
    "        return g\n",
    "    \n",
    "    def hess(p,src,dst):\n",
    "        n = numpy.size(src,0)\n",
    "        T = numpy.matrix([[numpy.cos(p[2]),-numpy.sin(p[2]),p[0]],\n",
    "        [numpy.sin(p[2]), numpy.cos(p[2]),p[1]],\n",
    "        [0 ,0 ,1 ]])\n",
    "        n = numpy.size(src,0)\n",
    "        xt = numpy.ones([n,3])\n",
    "        xt[:,:-1] = src\n",
    "        xt = (xt*T.T).A\n",
    "        d = numpy.zeros(numpy.shape(src))\n",
    "        d[:,0] = xt[:,0]-dst[:,0]\n",
    "        d[:,1] = xt[:,1]-dst[:,1]\n",
    "        dUdth_R = numpy.matrix([[-numpy.sin(p[2]),-numpy.cos(p[2])],[numpy.cos(p[2]),-numpy.sin(p[2])]])\n",
    "        dUdth = (src*dUdth_R.T).A\n",
    "        H = numpy.zeros([3,3])\n",
    "        H[0,0] = n*2\n",
    "        H[0,2] = numpy.sum(2*dUdth[:,0])\n",
    "        H[1,1] = n*2\n",
    "        H[1,2] = numpy.sum(2*dUdth[:,1])\n",
    "        H[2,0] = H[0,2]\n",
    "        H[2,1] = H[1,2]\n",
    "        d2Ud2th_R = numpy.matrix([[-numpy.cos(p[2]), numpy.sin(p[2])],[-numpy.sin(p[2]),-numpy.cos(p[2])]])\n",
    "        d2Ud2th = (src*d2Ud2th_R.T).A\n",
    "        H[2,2] = numpy.sum(2*(numpy.square(dUdth[:,0])+numpy.square(dUdth[:,1]) + d[:,0]*d2Ud2th[:,0]+d[:,0]*d2Ud2th[:,0]))\n",
    "        return H\n",
    "    \n",
    "    \n",
    "    t0 = time.time()\n",
    "    init_pose = (0,0,0)\n",
    "    src = numpy.array([a.T], copy=True).astype(numpy.float32)\n",
    "    dst = numpy.array([b.T], copy=True).astype(numpy.float32)\n",
    "    Tr = numpy.array([[numpy.cos(init_pose[2]),-numpy.sin(init_pose[2]),init_pose[0]],\n",
    "                   [numpy.sin(init_pose[2]), numpy.cos(init_pose[2]),init_pose[1]],\n",
    "                   [0,                    0,                   1          ]])\n",
    "    print(\"src\",numpy.shape(src))\n",
    "    print(\"Tr[0:2]\",numpy.shape(Tr[0:2]))\n",
    "    src = cv2.transform(src, Tr[0:2])\n",
    "    p_opt = numpy.array(init_pose)\n",
    "    T_opt = numpy.array([])\n",
    "    error_max = sys.maxsize\n",
    "    first = False\n",
    "    while not(first and time.time() - t0 > max_time):\n",
    "        distances, indices = sklearn.neighbors.NearestNeighbors(n_neighbors=1, algorithm='auto',p = 3).fit(dst[0]).kneighbors(src[0])\n",
    "        p = scipy.optimize.minimize(res,[0,0,0],args=(src[0],dst[0, indices.T][0]),method='Newton-CG',jac=jac,hess=hess).x\n",
    "        T  = numpy.array([[numpy.cos(p[2]),-numpy.sin(p[2]),p[0]],[numpy.sin(p[2]), numpy.cos(p[2]),p[1]]])\n",
    "        p_opt[:2]  = (p_opt[:2]*numpy.matrix(T[:2,:2]).T).A       \n",
    "        p_opt[0] += p[0]\n",
    "        p_opt[1] += p[1]\n",
    "        p_opt[2] += p[2]\n",
    "        src = cv2.transform(src, T)\n",
    "        Tr = (numpy.matrix(numpy.vstack((T,[0,0,1])))*numpy.matrix(Tr)).A\n",
    "        error = res([0,0,0],src[0],dst[0, indices.T][0])\n",
    "\n",
    "        if error < error_max:\n",
    "            error_max = error\n",
    "            first = True\n",
    "            T_opt = Tr\n",
    "\n",
    "    p_opt[2] = p_opt[2] % (2*numpy.pi)\n",
    "    return T_opt, error_max\n",
    "\n",
    "\n",
    "def main():\n",
    "    import cv2\n",
    "    import numpy\n",
    "    import random\n",
    "    import matplotlib.pyplot\n",
    "    n1 = 100\n",
    "    n2 = 75\n",
    "    bruit = 1/10\n",
    "    center = [random.random()*(2-1)*3,random.random()*(2-1)*3]\n",
    "    radius = random.random()\n",
    "    deformation = 2\n",
    "\n",
    "    template = numpy.array([\n",
    "        [numpy.cos(i*2*numpy.pi/n1)*radius*deformation for i in range(n1)], \n",
    "        [numpy.sin(i*2*numpy.pi/n1)*radius for i in range(n1)]\n",
    "    ])\n",
    "\n",
    "    data = numpy.array([\n",
    "        [numpy.cos(i*2*numpy.pi/n2)*radius*(1+random.random()*bruit)+center[0] for i in range(n2)], \n",
    "        [numpy.sin(i*2*numpy.pi/n2)*radius*deformation*(1+random.random()*bruit)+center[1] for i in range(n2)]\n",
    "    ])\n",
    "\n",
    "    T,error = icp(data,template)\n",
    "    dx = T[0,2]\n",
    "    dy = T[1,2]\n",
    "    rotation = numpy.arcsin(T[0,1]) * 360 / 2 / numpy.pi\n",
    "\n",
    "    print(\"T\",T)\n",
    "    print(\"error\",error)\n",
    "    print(\"rotation°\",rotation)\n",
    "    print(\"dx\",dx)\n",
    "    print(\"dy\",dy)\n",
    "\n",
    "    result = cv2.transform(numpy.array([data.T], copy=True).astype(numpy.float32), T).T\n",
    "    matplotlib.pyplot.plot(template[0], template[1], label=\"template\")\n",
    "    matplotlib.pyplot.plot(data[0], data[1], label=\"data\")\n",
    "    matplotlib.pyplot.plot(result[0], result[1], label=\"result: \"+str(rotation)+\"° - \"+str([dx,dy]))\n",
    "    matplotlib.pyplot.legend(loc=\"upper left\")\n",
    "    matplotlib.pyplot.axis('square')\n",
    "    matplotlib.pyplot.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipdraw",
   "language": "python",
   "name": "clipdraw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
