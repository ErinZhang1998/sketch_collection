{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import read_datasets as rd\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "import base64\n",
    "import pickle\n",
    "import collections\n",
    "import itertools, collections\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import cv2 \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'read_datasets' from '/home/xiaoyuz1/amazon_turk/read_datasets.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.rnn as rnn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo = pd.read_csv('/raid/xiaoyuz1/amazon_turk/df_all.csv')\n",
    "dfo['no_punc_1'] = dfo.no_punc_1.apply(lambda x: [str(y).strip()[1:-1] for y in x[1:-1].split(',')])\n",
    "\n",
    "dfp = pd.read_csv('/raid/xiaoyuz1/amazon_turk/df_all_pair.csv')\n",
    "dfp['no_punc_1'] = dfp.no_punc_1.apply(lambda x: [str(y).strip()[1:-1] for y in x[1:-1].split(',')])\n",
    "dfp['no_punc_2'] = dfp.no_punc_2.apply(lambda x: [str(y).strip()[1:-1] for y in x[1:-1].split(',')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = torch.rand((3, 128, 196))\n",
    "W_b = torch.rand((512,128))\n",
    "Q = torch.rand((3, 10, 512))\n",
    "torch.matmul(Q, torch.matmul(W_b, V)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1.-dropout)*torch.ones(hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys    \n",
    "path_to_module = '/home/xiaoyuz1/DoodlerGAN'\n",
    "sys.path.append(path_to_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import part_selector as part_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pClfs = part_s.Classifier(64, 16, n_part=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (blocks): ModuleList(\n",
       "    (0): ClassifierBlock(\n",
       "      (conv_res): Conv2d(10, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (net): Sequential(\n",
       "        (0): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (downsample): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (1): ClassifierBlock(\n",
       "      (conv_res): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (net): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (downsample): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (2): ClassifierBlock(\n",
       "      (conv_res): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (net): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (downsample): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (3): ClassifierBlock(\n",
       "      (conv_res): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (net): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (downsample): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (4): ClassifierBlock(\n",
       "      (conv_res): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (net): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (downsample): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (5): ClassifierBlock(\n",
       "      (conv_res): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (net): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (flatten): Flatten()\n",
       "  (to_logit): Linear(in_features=2048, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pClfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "res18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use pretrained word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "device = \"cuda:3\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False) #Must set jit=False for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_folder = '/raid/xiaoyuz1/clip_model_checkpoint'\n",
    "torch_path_name = os.path.join(root_folder, \"pleasant-tree-10.pt\")\n",
    "checkpoint = torch.load(torch_path_name)\n",
    "print(checkpoint.keys())\n",
    "args = checkpoint['args']\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test out primitive selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\",\"test\",\"dev\"]:\n",
    "    test_file = f\"/raid/xiaoyuz1/primitive_selector_training_data/july_15_{split}.pkl\"\n",
    "\n",
    "    data_raw = pickle.load(open(test_file, \"rb\"))\n",
    "    data_extended = []\n",
    "    for info in data_raw:\n",
    "        t1 = info[\"raw\"]\n",
    "        info[\"raw\"] = \" \".join(t1.split(\" \")[:-1])\n",
    "        t2 = info[\"processed\"]\n",
    "        info[\"processed\"] = \" \".join(t2.split(\" \")[:-1])\n",
    "        a,b,c,d,e,f,_,_,_ = info[\"M\"]\n",
    "        info.update({\n",
    "            \"p0\" : a,\n",
    "            \"p1\" : b,\n",
    "            \"p2\" : c,\n",
    "            \"p3\" : d,\n",
    "            \"p4\" : e,\n",
    "            \"p5\" : f,\n",
    "        })\n",
    "        data_extended.append(info)\n",
    "    with open(f\"/raid/xiaoyuz1/primitive_selector_training_data/july_18_{split}.pkl\", 'wb+') as f:\n",
    "        pickle.dump(data_extended, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import primitive_selector as ps\n",
    "reload(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['']\n",
    "args = ps.get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = ps.HParams()\n",
    "args_dict = vars(args)\n",
    "for k,v in args_dict.items():\n",
    "    if k == \"parameter_names\":\n",
    "        for j,name in enumerate(v):\n",
    "            hp.parameter_names[j] = name\n",
    "        continue\n",
    "    if hasattr(hp, k):\n",
    "        setattr(hp, k,v)\n",
    "vocab = ps.preprocess_dataset_language(\n",
    "    \"/raid/xiaoyuz1/primitive_selector_training_data/july_15_train.pkl\", old = True,\n",
    ")\n",
    "hp.vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_DICT = {\n",
    "    'arc' : lambda n : rd.generate_arc(n1=n, radius=1, x0=0, y0=0, template_size=1),\n",
    "    'circle' : lambda n : rd.generate_circle(n1=n, radius=1, x0=0, y0=0, template_size=1),\n",
    "    'square' : lambda n : rd.generate_square(n1=n, template_size=2),\n",
    "    'semicircle' : lambda n : rd.generate_semicircle(n1=n, radius=1, x0=0, y0=0, template_size=1),\n",
    "    'zigzag1' : lambda n : rd.generate_zigzag(n1=n, num_fold=1, side_length=1,template_size=1),\n",
    "}\n",
    "templates = {}\n",
    "for i,(k,v) in enumerate(TEMPLATE_DICT.items()):\n",
    "    arr = v(args.num_sampled_points)\n",
    "    templates[i] = (k,arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('/raid/xiaoyuz1/doodler_model_checkpoint/legendary-sun-11/55001.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.M = 10\n",
    "\n",
    "trainer = ps.Trainer(hp, args, vocab, templates)\n",
    "trainer.model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint['iteration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.eval()\n",
    "torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (description_ts, primitive_types, affine_paramss, dataset_indices) in \\\n",
    "    enumerate(trainer.test_dataset_loader):\n",
    "    if batch_idx == 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ii, descr in enumerate(description_ts):\n",
    "    desc_str = \" \".join([trainer.train_dataset.vocab_reverse[j.item()] for j in descr])\n",
    "    print(batch_idx, ii, dataset_indices[ii].item(), desc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_ts_packed = rnn.pack_sequence(description_ts)                     \n",
    "description_ts_packed, primitive_types, affine_paramss = \\\n",
    "    description_ts_packed.to(trainer.device), primitive_types.to(trainer.device), affine_paramss.to(trainer.device)\n",
    "\n",
    "prim_pred, normal_dists, pi_dists = trainer.model(description_ts_packed)\n",
    "prim_types = torch.argmax(prim_pred, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_samples = trainer.sample_parameters(normal_dists, pi_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, _ = trainer.calculate_metric(description_ts, dataset_indices, prim_types, param_samples, \\\n",
    "                                  plot_indices=None, train=False, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 29\n",
    "train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_tensor = description_ts[idx]\n",
    "data_idx = dataset_indices[idx].item()\n",
    "prim_type = prim_types[idx].item()\n",
    "pred_params = [sample[idx].item() for sample in param_samples]\n",
    "if train:\n",
    "    info = trainer.train_dataset.data_raw[data_idx]\n",
    "else:\n",
    "    info = trainer.test_dataset.data_raw[data_idx]\n",
    "\n",
    "pred_template_name, pred_template = trainer.templates[int(prim_type)]\n",
    "gt_template_name,_ = trainer.templates[int(info[\"primitive_type\"])]\n",
    "if train:\n",
    "    data, gt_template, gt_fitted_template, gt_img = trainer.train_sequences[data_idx]\n",
    "else:\n",
    "    data, gt_template, gt_fitted_template, gt_img = trainer.test_sequences[data_idx]\n",
    "gt_params = [info[trainer.hp.parameter_names[pi]] for pi in range(len(trainer.hp.parameter_names))]\n",
    "\n",
    "pred_info = {}\n",
    "for pi in range(len(trainer.hp.parameter_names)):\n",
    "    pred_info[trainer.hp.parameter_names[pi]] = pred_params[pi]\n",
    "pred_M = ps.GET_AFFINE_FUNCS[\"get_affine_transformation\"](pred_info)\n",
    "\n",
    "if trainer.args.use_projective:\n",
    "    pred_template_pred_param = cv2.perspectiveTransform(pred_template, pred_M).reshape(-1,2)\n",
    "else:\n",
    "    pred_template_pred_param = cv2.transform(np.array([pred_template]).astype(np.float32), pred_M)[0][:,:-1]\n",
    "\n",
    "# gt_img = np.asarray(rd.render_img([gt_fitted_template], line_diameter=3))\n",
    "pred_img = np.asarray(rd.render_img([pred_template_pred_param], line_diameter=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "face_sketches = ps.CONST.face_json['train_data']\n",
    "angel_sketches = ps.CONST.angel_json['train_data']\n",
    "face_points = rd.process_all_to_part_convex_hull(face_sketches, list(ps.CONST.face_parts_idx_dict_doodler.keys()), trainer.args.num_sampled_points)\n",
    "angel_points = rd.process_all_to_part_convex_hull(angel_sketches, list(ps.CONST.angel_parts_idx_dict_doodler.keys()), trainer.args.num_sampled_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "x,y = data, gt_fitted_template\n",
    "x_nn = NearestNeighbors(n_neighbors=1, leaf_size=1, algorithm='kd_tree', metric='l2').fit(x)\n",
    "min_y_to_x = x_nn.kneighbors(y)[0]\n",
    "y_nn = NearestNeighbors(n_neighbors=1, leaf_size=1, algorithm='kd_tree', metric='l2').fit(y)\n",
    "min_x_to_y = y_nn.kneighbors(x)[0]\n",
    "dist_y_to_x = np.mean(min_y_to_x)\n",
    "dist_x_to_y = np.mean(min_x_to_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_y_to_x + dist_x_to_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.set_printoptions(edgeitems=3)\n",
    "for param_idx,(normal_dist, pi_dist) in enumerate(zip(normal_dists, pi_dists)):\n",
    "    mu = normal_dist.loc[idx].reshape(-1,).detach().cpu().numpy()\n",
    "    sigma = normal_dist.scale[idx].reshape(-1,).detach().cpu().numpy()\n",
    "    pi = pi_dist.probs[idx].reshape(-1,).detach().cpu().numpy()\n",
    "    print(\"\\n\\t\", trainer.hp.parameter_names[param_idx], \"\\n\\t\", mu, \"\\n\\t\",sigma,\"\\n\\t\", pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-29 - 73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_tensor = description_ts[idx]\n",
    "data_idx = dataset_indices[idx].item()\n",
    "prim_type = prim_types[idx].item()\n",
    "pred_params = [sample[idx].item() for sample in param_samples]\n",
    "if train:\n",
    "    info = trainer.train_dataset.data_raw[data_idx]\n",
    "else:\n",
    "    info = trainer.test_dataset.data_raw[data_idx]\n",
    "\n",
    "pred_template_name, pred_template = trainer.templates[int(prim_type)]\n",
    "gt_template_name,_ = trainer.templates[int(info[\"primitive_type\"])]\n",
    "if train:\n",
    "    data, gt_template, gt_fitted_template, gt_img = trainer.train_sequences[data_idx]\n",
    "else:\n",
    "    data, gt_template, gt_fitted_template, gt_img = trainer.test_sequences[data_idx]\n",
    "gt_params = [info[trainer.hp.parameter_names[pi]] for pi in range(len(trainer.hp.parameter_names))]\n",
    "\n",
    "pred_info = {}\n",
    "for pi in range(len(trainer.hp.parameter_names)):\n",
    "    pred_info[trainer.hp.parameter_names[pi]] = pred_params[pi]\n",
    "pred_M = ps.GET_AFFINE_FUNCS[\"get_affine_transformation\"](pred_info)\n",
    "\n",
    "if trainer.args.use_projective:\n",
    "    pred_template_pred_param = cv2.perspectiveTransform(pred_template, pred_M).reshape(-1,2)\n",
    "else:\n",
    "    pred_template_pred_param = cv2.transform(np.array([pred_template]).astype(np.float32), pred_M)[0][:,:-1]\n",
    "\n",
    "# gt_img = np.asarray(rd.render_img([gt_fitted_template], line_diameter=3))\n",
    "pred_img = np.asarray(rd.render_img([pred_template_pred_param], line_diameter=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f\"{gt_template_name},{pred_template_name}\\n\" \n",
    "desc = info[\"processed\"]\n",
    "desc2 = \" \".join([trainer.train_dataset.vocab_reverse[j.item()] for j in description_tensor])\n",
    "title += f\"{desc},{desc2}\\n\"\n",
    "\n",
    "for pi,(p1,p2) in enumerate(zip(gt_params, pred_params)):\n",
    "    if trainer.hp.parameter_names[pi] == \"theta\":\n",
    "        p1 = np.degrees(p1)\n",
    "        p2 = np.degrees(p2)\n",
    "    title += f\"{trainer.hp.parameter_names[pi]}: {p1:.3f} {p2:.3f}\"\n",
    "    if pi % 2 == 0:\n",
    "        title += \"\\n\"\n",
    "    else:\n",
    "        title += \" | \"\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.fromarray(gt_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.fromarray(pred_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod(a, n):\n",
    "    return (a % n + n) % n\n",
    "\n",
    "def diff_angle(targetA, sourceA):\n",
    "    a = targetA - sourceA\n",
    "    a = mod(a+np.pi, 2 * np.pi) - np.pi\n",
    "    return a\n",
    "\n",
    "np.degrees(diff_angle(pred_info[\"theta\"], info[\"theta\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fitting primitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = h = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_spline_num_sampled_points = num_sampled_points = n = 200\n",
    "use_projective = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "templates = {}\n",
    "template_dict_processed = {}\n",
    "for i,(k,v) in enumerate(TEMPLATE_DICT.items()):\n",
    "    arr = v(b_spline_num_sampled_points)\n",
    "    template_dict_processed[k] = arr\n",
    "    templates[i] = (k,arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.plot_primitives(template_dict_processed,w=2, h=2,num_pngs_per_row = 1, row_figsize = 10, column_figsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import primitive_selector as ps\n",
    "reload(ps)\n",
    "all_data = ps.prepare_data(dfo, templates, num_sampled_points = 200, use_projective = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/raid/xiaoyuz1/primitive_selector_training_data/july_15_all.pkl\", \"wb+\") as f:\n",
    "    pickle.dump(all_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [\"train\", \"dev\", \"test\"]:\n",
    "    data_split = [x for x in all_data if x[\"split\"] == t]\n",
    "    data_split_dict = dict(zip(range(len(data_split)), data_split))\n",
    "    with open(f\"/raid/xiaoyuz1/primitive_selector_training_data/july_15_{t}.pkl\", \"wb+\") as f:\n",
    "        pickle.dump(data_split, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pickle.load(open(\"/raid/xiaoyuz1/primitive_selector_training_data/july_15_all.pkl\", \"rb\"))\n",
    "df = pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data,template,result = all_sequences[100]\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12,24)) \n",
    "axs[0].scatter(data[:,0], data[:,1], s=1, c='g')\n",
    "axs[0].scatter(result[:,0], result[:,1], s=1, c='r')\n",
    "axs[0].scatter(template[:,0], template[:,1], s=1, c='b')\n",
    "\n",
    "axs[0].axis(xmin=-w,xmax=w)\n",
    "axs[0].axis(ymin=h,ymax=-h)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def plot_prepared_data(info, num_sampled_points = 200, use_projective=False):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12,24)) \n",
    "    if info['category'] == 'face':\n",
    "        drawing_raw = CONST.face_json['train_data'][info['image_idx']]\n",
    "        part_indices = list(CONST.face_parts_idx_dict_doodler.keys())\n",
    "    else:\n",
    "        drawing_raw = CONST.angel_json['train_data'][info['image_idx']]\n",
    "        part_indices = list(CONST.angel_parts_idx_dict_doodler.keys())\n",
    "\n",
    "    parts = rd.process_quickdraw_to_part_convex_hull(\n",
    "        drawing_raw,\n",
    "        part_indices,\n",
    "        b_spline_num_sampled_points=num_sampled_points,\n",
    "    )\n",
    "    data = parts[info['part']]\n",
    "    template = templates[info['primitive_type']][1]\n",
    "    transform_mat = np.asarray(info['M']).astype(float).reshape(3,3)\n",
    "#     axs[0].scatter(template[:,0], template[:,1], s=1, c='b')\n",
    "    \n",
    "    if use_projective:\n",
    "        result = cv2.perspectiveTransform(template, transform_mat).reshape(-1,2)\n",
    "    else:\n",
    "        result = cv2.transform(np.array([template]).astype(\n",
    "            np.float32), transform_mat)[0][:,:-1]\n",
    "    \n",
    "    \n",
    "    axs[0].scatter(result[:,0], result[:,1], s=1, c='r')\n",
    "    \n",
    "#     theta, scale_mat, shear_mat = rd.decompose_affine(transform_mat)\n",
    "#     T_rotate = rd.get_rotation_matrix(theta)\n",
    "#     T_scale = rd.get_scale_matrix(scale_mat[0][0], scale_mat[1][1])\n",
    "#     T_shear = rd.get_shear_matrix(shear_mat[0][1], shear_mat[1][0])\n",
    "#     tx,ty = transform_mat[0][2],transform_mat[1][2]\n",
    "#     T_translate = rd.get_translation_matrix(tx,ty)\n",
    "    \n",
    "#     result1 = cv2.transform(np.array([template]).astype(\n",
    "#             np.float32), T_rotate)[0][:,:-1]\n",
    "#     axs[0].scatter(result1[:,0], result1[:,1], s=1, c='m')\n",
    "    \n",
    "#     result2 = cv2.transform(np.array([template]).astype(\n",
    "#             np.float32), T_rotate @ T_scale @ T_shear)[0][:,:-1]\n",
    "#     axs[0].scatter(result2[:,0], result2[:,1], s=1, alpha=0.5, c='pink')\n",
    "\n",
    "#     result3 = cv2.transform(np.array([template]).astype(\n",
    "#             np.float32), T_translate @ T_rotate @ T_scale @ T_shear)[0][:,:-1]\n",
    "#     axs[0].scatter(result3[:,0], result3[:,1], s=1, c='lime', alpha=0.5)\n",
    "    \n",
    "    axs[0].scatter(data[:,0], data[:,1], alpha=0.5, s=1, c='lime')\n",
    "\n",
    "    axs[0].axis(xmin=-w,xmax=w)\n",
    "    axs[0].axis(ymin=h,ymax=-h)\n",
    "    axs[1].axis(xmin=-w,xmax=w)\n",
    "    axs[1].axis(ymin=h,ymax=-h)\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_prepared_data(df.iloc[123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### plot GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "M_all = np.asarray(df.M.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "X = np.asarray(df.sx.to_list()).reshape(-1, 1)\n",
    "# M_all[:,0].reshape(-1, 1)\n",
    "print(X.min(), X.max())\n",
    "\n",
    "N = np.arange(1, 11)\n",
    "models = [None for i in range(len(N))]\n",
    "\n",
    "for i in range(len(N)):\n",
    "    models[i] = GaussianMixture(N[i]).fit(X)\n",
    "\n",
    "# compute the AIC and the BIC\n",
    "AIC = [m.aic(X) for m in models]\n",
    "BIC = [m.bic(X) for m in models]\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "#  We'll use three panels:\n",
    "#   1) data + best-fit mixture\n",
    "#   2) AIC and BIC vs number of components\n",
    "#   3) probability that a point came from each component\n",
    "\n",
    "fig = plt.figure(figsize=(15, 20))\n",
    "fig.subplots_adjust(left=0.12, right=0.97,\n",
    "                    bottom=0.21, top=0.9, wspace=0.5)\n",
    "\n",
    "\n",
    "# plot 1: data + best-fit mixture\n",
    "ax = fig.add_subplot(311)\n",
    "M_best = models[np.argmin(AIC)]\n",
    "\n",
    "x = np.linspace(X.min(), X.max(), 1000)\n",
    "logprob = M_best.score_samples(x.reshape(-1, 1))\n",
    "responsibilities = M_best.predict_proba(x.reshape(-1, 1))\n",
    "pdf = np.exp(logprob)\n",
    "pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "\n",
    "ax.hist(X, 30, density=True, histtype='stepfilled', alpha=0.4)\n",
    "ax.plot(x, pdf, '-k')\n",
    "ax.plot(x, pdf_individual, '--k')\n",
    "ax.text(0.04, 0.96, \"Best-fit Mixture\",\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$p(x)$')\n",
    "\n",
    "\n",
    "# plot 2: AIC and BIC\n",
    "ax = fig.add_subplot(312)\n",
    "ax.plot(N, AIC, '-k', label='AIC')\n",
    "ax.plot(N, BIC, '--k', label='BIC')\n",
    "ax.set_xlabel('n. components')\n",
    "ax.set_ylabel('information criterion')\n",
    "ax.legend(loc=2)\n",
    "\n",
    "\n",
    "# plot 3: posterior probabilities for each component\n",
    "ax = fig.add_subplot(313)\n",
    "\n",
    "p = responsibilities\n",
    "p = p[:, (1, 0, 2)]  # rearrange order so the plot looks better\n",
    "p = p.cumsum(1).T\n",
    "\n",
    "ax.fill_between(x, 0, p[0], color='gray', alpha=0.3)\n",
    "ax.fill_between(x, p[0], p[1], color='gray', alpha=0.5)\n",
    "ax.fill_between(x, p[1], 1, color='gray', alpha=0.7)\n",
    "ax.set_xlim(-6, 6)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel(r'$p({\\rm class}|x)$')\n",
    "\n",
    "ax.text(-5, 0.3, 'class 1', rotation='vertical')\n",
    "ax.text(0, 0.5, 'class 2', rotation='vertical')\n",
    "ax.text(3, 0.3, 'class 3', rotation='vertical')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(M_best.weights_)\n",
    "print(M_best.means_)\n",
    "print(M_best.covariances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prepared_data(df.iloc[110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawing_arr = CONST.face_json['train_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(rd)\n",
    "from tqdm import tqdm\n",
    "\n",
    "M_arr = collections.defaultdict(lambda : np.zeros(9))\n",
    "M_name = collections.defaultdict(lambda : \" \")\n",
    "M_mse = collections.defaultdict(float)\n",
    "\n",
    "\n",
    "for idx in tqdm(range(len(drawing_arr))):\n",
    "# for idx in range(len(drawing_arr)):\n",
    "#     print(idx)\n",
    "    \n",
    "    strokes_spline_fitted = rd.process_quickdraw_to_part_convex_hull(\n",
    "        drawing_arr[idx],\n",
    "        list(CONST.face_parts_idx_dict.keys()),\n",
    "        b_spline_num_sampled_points=n,\n",
    "    )\n",
    "\n",
    "    for _, (part_type, sketch_data) in enumerate(strokes_spline_fitted.items()):\n",
    "        \n",
    "        mat, prim_name, prim_mse = rd.get_transform_smallest_mse(\n",
    "            sketch_data, template_dict_processed, use_projective=use_projective,\n",
    "        )\n",
    "        M_arr[(idx, part_type)] = mat\n",
    "        M_name[(idx, part_type)] = prim_name\n",
    "        M_mse[(idx, part_type)] = prim_mse\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/raid/xiaoyuz1/SPG_Face_Part_256_projective_{}_mse.pkl'.format(use_projective), \"wb+\") as f:\n",
    "    pickle.dump((dict(M_arr), dict(M_name), dict(M_mse)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'/raid/xiaoyuz1/SPG_Face_Part_256_projective_{}_mse.pkl'.format(use_projective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_dict = dict(filter(lambda x: x[1] == 'line', M_name.items()))\n",
    "print(len(line_dict), line_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_arr, M_name = None,None\n",
    "M_mse = None\n",
    "fname = '/raid/xiaoyuz1/SPG_Face_Part_256_projective_False_mse.pkl'\n",
    "with open(fname, \"rb\") as f:\n",
    "    M_arr, M_name, M_mse = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_l = list(filter(lambda x: not math.isinf(x), list(M_mse.values())))\n",
    "np.mean(mse_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_list = list(map(lambda x: int(x[0]), M_name.keys()))\n",
    "stroke_list = list(map(lambda x: int(x[1]), M_name.keys()))\n",
    "arr_list = list(M_arr.values())\n",
    "name_list = list(M_name.values())\n",
    "mse_list = list(M_mse.values())\n",
    "\n",
    "data = {'image': image_list, 'stroke' : stroke_list, 'M' : arr_list, 'primitive' : name_list, 'mse' : mse_list}\n",
    "\n",
    "for c_idx,c in enumerate(np.asarray(arr_list).T):\n",
    "    if c_idx > 5:\n",
    "        continue\n",
    "    data['M{}'.format(str(c_idx))] = c\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = df[df.mse.apply(lambda x: not math.isinf(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_all = np.asarray(dfa.M.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "stroke_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rd.show_these_sketches(\n",
    "    CONST.face_json,\n",
    "    [idx], \n",
    "    [str(idx)], \n",
    "    [[]], \n",
    "    num_pngs_per_row = 6,\n",
    "    row_figsize = 4,\n",
    "    column_figsize = 4,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strokes_spline_fitted = rd.process_quickdraw_to_part_convex_hull(\n",
    "    drawing_arr[idx],\n",
    "    list(CONST.face_parts_idx_dict.keys()),\n",
    "    b_spline_num_sampled_points=n,\n",
    ")\n",
    "\n",
    "# strokes_spline_fitted = rd.process_quickdraw_to_stroke_no_normalize(\n",
    "#     drawing_arr[idx], \n",
    "#     b_spline_num_sampled_points=200,\n",
    "# )\n",
    "# print(len(strokes_spline_fitted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "\n",
    "parts_indices = list(CONST.face_parts_idx_dict.keys())\n",
    "\n",
    "drawing_raw = np.asarray(drawing_arr[idx])\n",
    "drawing_raw[:,0] = np.cumsum(drawing_raw[:,0], 0) + 25\n",
    "drawing_raw[:,1] = np.cumsum(drawing_raw[:,1], 0) + 25\n",
    "\n",
    "parts = []\n",
    "part_idx_part = {}\n",
    "\n",
    "for k in parts_indices:\n",
    "    strokes = drawing_raw[drawing_raw[:,-1] == k]\n",
    "    if len(strokes) < 1:\n",
    "        continue\n",
    "    parts.append(strokes)\n",
    "    part_idx_part[k] = strokes[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_arr_idx, M_name_idx, M_mse_idx = {},{},{}\n",
    "for stroke_index, (part_type, data) in enumerate(strokes_spline_fitted.items()):\n",
    "        \n",
    "    mat, prim_name, prim_mse = rd.get_transform_smallest_mse(\n",
    "        data, template_dict_processed, use_projective=use_projective,\n",
    "    )\n",
    "    M_arr_idx[(idx, part_type)] = mat\n",
    "    M_name_idx[(idx, part_type)] = prim_name\n",
    "    M_mse_idx[(idx, part_type)] = prim_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "for k, M in M_arr_idx.items():\n",
    "#     if k[1] != 3:\n",
    "#         continue\n",
    "    data = strokes_spline_fitted[k[1]]\n",
    "    \n",
    "    template = template_dict_processed[M_name_idx[(idx, k[1])]]\n",
    "    Mm = M.reshape(3,3)\n",
    "    \n",
    "    result, mse = rd.get_transformed_template(template, data, Mm,projective=use_projective)\n",
    "    ax1.scatter(result[:,0], result[:,1], label=\"transformed template\", s=1, c='r')\n",
    "    ax2.scatter(data[:,0], data[:,1], label=\"data\", alpha=0.5, s=1, c='g')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.xlim(0,w)\n",
    "# plt.ylim(h,0)\n",
    "\n",
    "\n",
    "ax1.axis(xmin=0,xmax=w)\n",
    "ax1.axis(ymin=h,ymax=0)\n",
    "\n",
    "ax2.axis(xmin=0,xmax=w)\n",
    "ax2.axis(ymin=h,ymax=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_arr_idx, M_name_idx, M_mse_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "for index, entry in dfa[dfa.image == idx].iterrows():\n",
    "    data = strokes_spline_fitted[entry.stroke]\n",
    "    \n",
    "    template_func = TEMPLATE_DICT[entry.primitive]\n",
    "    template = template_func(n)\n",
    "    \n",
    "    print(entry.M.reshape(3,3))\n",
    "    \n",
    "    result, mse = rd.get_transformed_template(template, data, entry.M.reshape(3,3),projective=use_projective)\n",
    "    ax1.scatter(result[:,0], result[:,1], label=\"transformed template\", s=1, c='r')\n",
    "    ax2.scatter(data[:,0], data[:,1], label=\"data\", alpha=0.5, s=1, c='g')\n",
    "    orig_data = part_idx_part[int(entry.stroke)]\n",
    "    ax2.plot(orig_data[:,0], orig_data[:,1], alpha=0.5, c='b')\n",
    "    \n",
    "#     ax1.plot(result[:,0], result[:,1], label=\"transformed template\", c='r')\n",
    "#     ax2.plot(data[:,0], data[:,1], label=\"data\", alpha=0.5,c='g')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.xlim(0,w)\n",
    "# plt.ylim(h,0)\n",
    "\n",
    "ax1.axis(xmin=0,xmax=w)\n",
    "ax1.axis(ymin=h,ymax=0)\n",
    "\n",
    "ax2.axis(xmin=0,xmax=w)\n",
    "ax2.axis(ymin=h,ymax=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "scrolled": true
   },
   "source": [
    "## old icp code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "from numpy.random import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def del_miss(indeces, dist, max_dist, th_rate = 0.8):\n",
    "    th_dist = max_dist * th_rate\n",
    "    return np.array([indeces[0][np.where(dist.T[0] < th_dist)]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def is_converge(Tr, scale):\n",
    "    delta_angle = 0.0001\n",
    "    delta_scale = scale * 0.0001\n",
    "    \n",
    "    min_cos = 1 - delta_angle\n",
    "    max_cos = 1 + delta_angle\n",
    "    min_sin = -delta_angle\n",
    "    max_sin = delta_angle\n",
    "    min_move = -delta_scale\n",
    "    max_move = delta_scale\n",
    "    \n",
    "    return min_cos < Tr[0, 0] and Tr[0, 0] < max_cos and \\\n",
    "           min_cos < Tr[1, 1] and Tr[1, 1] < max_cos and \\\n",
    "           min_sin < -Tr[1, 0] and -Tr[1, 0] < max_sin and \\\n",
    "           min_sin < Tr[0, 1] and Tr[0, 1] < max_sin and \\\n",
    "           min_move < Tr[0, 2] and Tr[0, 2] < max_move and \\\n",
    "           min_move < Tr[1, 2] and Tr[1, 2] < max_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def icp(d1, d2, max_iterate = 100):\n",
    "    src = np.array([d1.T], copy=True).astype(np.float32)\n",
    "    dst = np.array([d2.T], copy=True).astype(np.float32)\n",
    "    \n",
    "    knn = cv2.KNearest()\n",
    "    responses = np.array(range(len(d2[0]))).astype(np.float32)\n",
    "    knn.train(src[0], responses)\n",
    "        \n",
    "    Tr = np.array([[np.cos(0), -np.sin(0), 0],\n",
    "                   [np.sin(0), np.cos(0),  0],\n",
    "                   [0,         0,          1]])\n",
    "\n",
    "    dst = cv2.transform(dst, Tr[0:2])\n",
    "    max_dist = sys.maxint\n",
    "    \n",
    "    scale_x = np.max(d1[0]) - np.min(d1[0])\n",
    "    scale_y = np.max(d1[1]) - np.min(d1[1])\n",
    "    scale = max(scale_x, scale_y)\n",
    "       \n",
    "    for i in range(max_iterate):\n",
    "        ret, results, neighbours, dist = knn.find_nearest(dst[0], 1)\n",
    "        \n",
    "        indeces = results.astype(np.int32).T     \n",
    "        indeces = del_miss(indeces, dist, max_dist)  \n",
    "        \n",
    "        T = cv2.estimateRigidTransform(dst[0, indeces], src[0, indeces], True)\n",
    "\n",
    "        max_dist = np.max(dist)\n",
    "        dst = cv2.transform(dst, T)\n",
    "        Tr = np.dot(np.vstack((T,[0,0,1])), Tr)\n",
    "        \n",
    "        if (is_converge(T, scale)):\n",
    "            break\n",
    "        \n",
    "    return Tr[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    point_count = 100\n",
    "    th = np.pi / 8\n",
    "    move = np.array([[0.30], [0.5]])\n",
    "    rnd_scale = 0.03\n",
    "    x1 = np.linspace(0, 1.1, point_count)\n",
    "    y1 = np.sin(x1 * np.pi)\n",
    "    d1 = np.array([x1, y1])\n",
    "\n",
    "    rot = np.array([[np.cos(th), -np.sin(th)], [np.sin(th), np.cos(th)]])\n",
    "    rand = np.random.rand(2, point_count)*rnd_scale\n",
    "    d2 = np.dot(rot, d1) + move\n",
    "    d2 = np.add(d2, rand)\n",
    "\n",
    "    plt.plot(d1[0], d1[1])\n",
    "    plt.plot(d2[0], d2[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    ret = icp(d1, d2)\n",
    "    \n",
    "    plt.plot(d1[0], d1[1])\n",
    "    dst = np.array([d2.T], copy=True).astype(np.float32)\n",
    "    dst = cv2.transform(dst, ret)\n",
    "    plt.plot(dst[0].T[0], dst[0].T[1])\n",
    "    plt.show()\n",
    "    \n",
    "    print ret[0][0] * ret[0][0] + ret[0][1] * ret[0][1]\n",
    "    print np.arccos(ret[0][0]) / 2 / np.pi * 360\n",
    "    print np.arcsin(ret[0][1]) / 2 / np.pi * 360\n",
    "\n",
    "    print ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def icp(a, b,\n",
    "        max_time = 1\n",
    "    ):\n",
    "    import cv2\n",
    "    import numpy\n",
    "    import copy\n",
    "    import pylab\n",
    "    import time\n",
    "    import sys\n",
    "    import sklearn.neighbors\n",
    "    import scipy.optimize\n",
    "\n",
    "\n",
    "\n",
    "    def res(p,src,dst):\n",
    "        T = numpy.matrix([[numpy.cos(p[2]),-numpy.sin(p[2]),p[0]],\n",
    "        [numpy.sin(p[2]), numpy.cos(p[2]),p[1]],\n",
    "        [0 ,0 ,1 ]])\n",
    "        n = numpy.size(src,0)\n",
    "        xt = numpy.ones([n,3])\n",
    "        xt[:,:-1] = src\n",
    "        xt = (xt*T.T).A\n",
    "        d = numpy.zeros(numpy.shape(src))\n",
    "        d[:,0] = xt[:,0]-dst[:,0]\n",
    "        d[:,1] = xt[:,1]-dst[:,1]\n",
    "        r = numpy.sum(numpy.square(d[:,0])+numpy.square(d[:,1]))\n",
    "        return r\n",
    "\n",
    "    def jac(p,src,dst):\n",
    "        T = numpy.matrix([[numpy.cos(p[2]),-numpy.sin(p[2]),p[0]],\n",
    "        [numpy.sin(p[2]), numpy.cos(p[2]),p[1]],\n",
    "        [0 ,0 ,1 ]])\n",
    "        n = numpy.size(src,0)\n",
    "        xt = numpy.ones([n,3])\n",
    "        xt[:,:-1] = src\n",
    "        xt = (xt*T.T).A\n",
    "        d = numpy.zeros(numpy.shape(src))\n",
    "        d[:,0] = xt[:,0]-dst[:,0]\n",
    "        d[:,1] = xt[:,1]-dst[:,1]\n",
    "        dUdth_R = numpy.matrix([[-numpy.sin(p[2]),-numpy.cos(p[2])],\n",
    "                            [ numpy.cos(p[2]),-numpy.sin(p[2])]])\n",
    "        dUdth = (src*dUdth_R.T).A\n",
    "        g = numpy.array([  numpy.sum(2*d[:,0]),\n",
    "                        numpy.sum(2*d[:,1]),\n",
    "                        numpy.sum(2*(d[:,0]*dUdth[:,0]+d[:,1]*dUdth[:,1])) ])\n",
    "        return g\n",
    "    \n",
    "    def hess(p,src,dst):\n",
    "        n = numpy.size(src,0)\n",
    "        T = numpy.matrix([[numpy.cos(p[2]),-numpy.sin(p[2]),p[0]],\n",
    "        [numpy.sin(p[2]), numpy.cos(p[2]),p[1]],\n",
    "        [0 ,0 ,1 ]])\n",
    "        n = numpy.size(src,0)\n",
    "        xt = numpy.ones([n,3])\n",
    "        xt[:,:-1] = src\n",
    "        xt = (xt*T.T).A\n",
    "        d = numpy.zeros(numpy.shape(src))\n",
    "        d[:,0] = xt[:,0]-dst[:,0]\n",
    "        d[:,1] = xt[:,1]-dst[:,1]\n",
    "        dUdth_R = numpy.matrix([[-numpy.sin(p[2]),-numpy.cos(p[2])],[numpy.cos(p[2]),-numpy.sin(p[2])]])\n",
    "        dUdth = (src*dUdth_R.T).A\n",
    "        H = numpy.zeros([3,3])\n",
    "        H[0,0] = n*2\n",
    "        H[0,2] = numpy.sum(2*dUdth[:,0])\n",
    "        H[1,1] = n*2\n",
    "        H[1,2] = numpy.sum(2*dUdth[:,1])\n",
    "        H[2,0] = H[0,2]\n",
    "        H[2,1] = H[1,2]\n",
    "        d2Ud2th_R = numpy.matrix([[-numpy.cos(p[2]), numpy.sin(p[2])],[-numpy.sin(p[2]),-numpy.cos(p[2])]])\n",
    "        d2Ud2th = (src*d2Ud2th_R.T).A\n",
    "        H[2,2] = numpy.sum(2*(numpy.square(dUdth[:,0])+numpy.square(dUdth[:,1]) + d[:,0]*d2Ud2th[:,0]+d[:,0]*d2Ud2th[:,0]))\n",
    "        return H\n",
    "    \n",
    "    \n",
    "    t0 = time.time()\n",
    "    init_pose = (0,0,0)\n",
    "    src = numpy.array([a.T], copy=True).astype(numpy.float32)\n",
    "    dst = numpy.array([b.T], copy=True).astype(numpy.float32)\n",
    "    Tr = numpy.array([[numpy.cos(init_pose[2]),-numpy.sin(init_pose[2]),init_pose[0]],\n",
    "                   [numpy.sin(init_pose[2]), numpy.cos(init_pose[2]),init_pose[1]],\n",
    "                   [0,                    0,                   1          ]])\n",
    "    print(\"src\",numpy.shape(src))\n",
    "    print(\"Tr[0:2]\",numpy.shape(Tr[0:2]))\n",
    "    src = cv2.transform(src, Tr[0:2])\n",
    "    p_opt = numpy.array(init_pose)\n",
    "    T_opt = numpy.array([])\n",
    "    error_max = sys.maxsize\n",
    "    first = False\n",
    "    while not(first and time.time() - t0 > max_time):\n",
    "        distances, indices = sklearn.neighbors.NearestNeighbors(n_neighbors=1, algorithm='auto',p = 3).fit(dst[0]).kneighbors(src[0])\n",
    "        p = scipy.optimize.minimize(res,[0,0,0],args=(src[0],dst[0, indices.T][0]),method='Newton-CG',jac=jac,hess=hess).x\n",
    "        T  = numpy.array([[numpy.cos(p[2]),-numpy.sin(p[2]),p[0]],[numpy.sin(p[2]), numpy.cos(p[2]),p[1]]])\n",
    "        p_opt[:2]  = (p_opt[:2]*numpy.matrix(T[:2,:2]).T).A       \n",
    "        p_opt[0] += p[0]\n",
    "        p_opt[1] += p[1]\n",
    "        p_opt[2] += p[2]\n",
    "        src = cv2.transform(src, T)\n",
    "        Tr = (numpy.matrix(numpy.vstack((T,[0,0,1])))*numpy.matrix(Tr)).A\n",
    "        error = res([0,0,0],src[0],dst[0, indices.T][0])\n",
    "\n",
    "        if error < error_max:\n",
    "            error_max = error\n",
    "            first = True\n",
    "            T_opt = Tr\n",
    "\n",
    "    p_opt[2] = p_opt[2] % (2*numpy.pi)\n",
    "    return T_opt, error_max\n",
    "\n",
    "\n",
    "def main():\n",
    "    import cv2\n",
    "    import numpy\n",
    "    import random\n",
    "    import matplotlib.pyplot\n",
    "    n1 = 100\n",
    "    n2 = 75\n",
    "    bruit = 1/10\n",
    "    center = [random.random()*(2-1)*3,random.random()*(2-1)*3]\n",
    "    radius = random.random()\n",
    "    deformation = 2\n",
    "\n",
    "    template = numpy.array([\n",
    "        [numpy.cos(i*2*numpy.pi/n1)*radius*deformation for i in range(n1)], \n",
    "        [numpy.sin(i*2*numpy.pi/n1)*radius for i in range(n1)]\n",
    "    ])\n",
    "\n",
    "    data = numpy.array([\n",
    "        [numpy.cos(i*2*numpy.pi/n2)*radius*(1+random.random()*bruit)+center[0] for i in range(n2)], \n",
    "        [numpy.sin(i*2*numpy.pi/n2)*radius*deformation*(1+random.random()*bruit)+center[1] for i in range(n2)]\n",
    "    ])\n",
    "\n",
    "    T,error = icp(data,template)\n",
    "    dx = T[0,2]\n",
    "    dy = T[1,2]\n",
    "    rotation = numpy.arcsin(T[0,1]) * 360 / 2 / numpy.pi\n",
    "\n",
    "    print(\"T\",T)\n",
    "    print(\"error\",error)\n",
    "    print(\"rotation°\",rotation)\n",
    "    print(\"dx\",dx)\n",
    "    print(\"dy\",dy)\n",
    "\n",
    "    result = cv2.transform(numpy.array([data.T], copy=True).astype(numpy.float32), T).T\n",
    "    matplotlib.pyplot.plot(template[0], template[1], label=\"template\")\n",
    "    matplotlib.pyplot.plot(data[0], data[1], label=\"data\")\n",
    "    matplotlib.pyplot.plot(result[0], result[1], label=\"result: \"+str(rotation)+\"° - \"+str([dx,dy]))\n",
    "    matplotlib.pyplot.legend(loc=\"upper left\")\n",
    "    matplotlib.pyplot.axis('square')\n",
    "    matplotlib.pyplot.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipdraw",
   "language": "python",
   "name": "clipdraw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
