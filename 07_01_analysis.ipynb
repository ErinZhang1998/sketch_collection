{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import read_datasets as rd\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "import base64\n",
    "import pickle\n",
    "import collections\n",
    "import itertools, collections\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import cv2 \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'read_datasets' from '/home/xiaoyuz1/amazon_turk/read_datasets.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.rnn as rnn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo = pd.read_csv('/raid/xiaoyuz1/amazon_turk/df_all.csv')\n",
    "dfo['no_punc_1'] = dfo.no_punc_1.apply(lambda x: [str(y).strip()[1:-1] for y in x[1:-1].split(',')])\n",
    "\n",
    "dfp = pd.read_csv('/raid/xiaoyuz1/amazon_turk/df_all_pair.csv')\n",
    "dfp['no_punc_1'] = dfp.no_punc_1.apply(lambda x: [str(y).strip()[1:-1] for y in x[1:-1].split(',')])\n",
    "dfp['no_punc_2'] = dfp.no_punc_2.apply(lambda x: [str(y).strip()[1:-1] for y in x[1:-1].split(',')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use pretrained word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "device = \"cuda\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False) #Must set jit=False for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_folder = '/raid/xiaoyuz1/clip_model_checkpoint'\n",
    "torch_path_name = os.path.join(root_folder, \"pleasant-tree-10.pt\")\n",
    "checkpoint = torch.load(torch_path_name)\n",
    "print(checkpoint.keys())\n",
    "args = checkpoint['args']\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test out primitive selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"/raid/xiaoyuz1/primitive_selector_training_data/july_18_test.pkl\"\n",
    "data_raw = pickle.load(open(test_file, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'large eyes'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = data_raw[72]\n",
    "\"{} {}\".format(info['processed'], CONST.face_parts_idx_dict_doodler[info['part']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\",\"test\",\"dev\"]:\n",
    "    test_file = f\"/raid/xiaoyuz1/primitive_selector_training_data/july_15_{split}.pkl\"\n",
    "\n",
    "    data_raw = pickle.load(open(test_file, \"rb\"))\n",
    "    data_extended = []\n",
    "    for info in data_raw:\n",
    "        t1 = info[\"raw\"]\n",
    "        info[\"raw\"] = \" \".join(t1.split(\" \")[:-1])\n",
    "        t2 = info[\"processed\"]\n",
    "        info[\"processed\"] = \" \".join(t2.split(\" \")[:-1])\n",
    "        a,b,c,d,e,f,_,_,_ = info[\"M\"]\n",
    "        info.update({\n",
    "            \"p0\" : a,\n",
    "            \"p1\" : b,\n",
    "            \"p2\" : c,\n",
    "            \"p3\" : d,\n",
    "            \"p4\" : e,\n",
    "            \"p5\" : f,\n",
    "        })\n",
    "        data_extended.append(info)\n",
    "    with open(f\"/raid/xiaoyuz1/primitive_selector_training_data/july_18_{split}.pkl\", 'wb+') as f:\n",
    "        pickle.dump(data_extended, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'primitive_selector' from '/home/xiaoyuz1/amazon_turk/primitive_selector.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import primitive_selector as ps\n",
    "reload(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['']\n",
    "args = ps.get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = ps.HParams()\n",
    "args_dict = vars(args)\n",
    "for k,v in args_dict.items():\n",
    "    if k == \"parameter_names\":\n",
    "        for j,name in enumerate(v):\n",
    "            hp.parameter_names[j] = name\n",
    "        continue\n",
    "    if hasattr(hp, k):\n",
    "        setattr(hp, k,v)\n",
    "vocab = ps.preprocess_dataset_language(args.train_file)\n",
    "hp.vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_DICT = {\n",
    "    'arc' : lambda n : rd.generate_arc(n1=n, radius=1, x0=0, y0=0, template_size=1),\n",
    "    'circle' : lambda n : rd.generate_circle(n1=n, radius=1, x0=0, y0=0, template_size=1),\n",
    "    'square' : lambda n : rd.generate_square(n1=n, template_size=2),\n",
    "    'semicircle' : lambda n : rd.generate_semicircle(n1=n, radius=1, x0=0, y0=0, template_size=1),\n",
    "    'zigzag1' : lambda n : rd.generate_zigzag(n1=n, num_fold=1, side_length=1,template_size=1),\n",
    "}\n",
    "templates = {}\n",
    "for i,(k,v) in enumerate(TEMPLATE_DICT.items()):\n",
    "    arr = v(args.num_sampled_points)\n",
    "    templates[i] = (k,arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1296"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"eyes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = pickle.load(open(args.train_seq_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = pickle.load(open(args.test_seq_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_extended = []\n",
    "for data, template, result in test_sequences:\n",
    "    gt_img = np.asarray(rd.render_img([result], line_diameter=3))\n",
    "    sequences_extended.append((data, template, result, gt_img))\n",
    "with open(args.test_seq_file, 'wb+') as f:\n",
    "    pickle.dump(sequences_extended, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_sequences), len(test_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ps.Trainer(hp, args, vocab, templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'category': 'face',\n",
       "  'image_idx': 172,\n",
       "  'part': 0,\n",
       "  'raw': 'pupilless',\n",
       "  'processed': 'pupilless',\n",
       "  'primitive_type': 1,\n",
       "  'M': array([-30.6854, -33.7167, 122.937 ,  17.6897, -12.1063,  91.9407,\n",
       "           0.    ,   0.    ,   1.    ]),\n",
       "  'error': 51.11977070676713,\n",
       "  'split': 'test',\n",
       "  'theta': 2.6186427544429574,\n",
       "  'sx': 35.41919839344829,\n",
       "  'sy': 27.327690743533438,\n",
       "  'hx': 0.6539971055000693,\n",
       "  'tx': 122.93695230331444,\n",
       "  'ty': 91.9407071367543,\n",
       "  'p0': -30.68541047399987,\n",
       "  'p1': -33.71665550947047,\n",
       "  'p2': 122.93695230331444,\n",
       "  'p3': 17.68969188190086,\n",
       "  'p4': -12.10632828731588,\n",
       "  'p5': 91.9407071367543},\n",
       " (tensor([1296]),\n",
       "  tensor(1),\n",
       "  tensor([  2.6186,  35.4192,  27.3277,   0.6540, 122.9370,  91.9407]),\n",
       "  tensor(116)))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test_dataset.data_raw[116], trainer.test_dataset[116]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"eyes\" in vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7976931348623157e+308"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.float_info.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hair'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test_dataset.vocab_reverse[142]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('/raid/xiaoyuz1/doodler_model_checkpoint/dulcet-rain-8/55000.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainer.test_dataset_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (description_ts, primitive_types, affine_paramss, indices) in \\\n",
    "    enumerate(trainer.test_dataset_loader):\n",
    "    if batch_idx == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([117, 122,  91,  94, 107, 118,  64,  65,  66,  67,  69,  71,  76,  77,\n",
       "         82,  83,  84,  85,  88,  90,  92,  93,  95,  96,  97,  98,  99, 100,\n",
       "        101, 103, 105, 108, 110, 111, 112, 113, 114, 119, 120, 121, 124, 125,\n",
       "        126,  68,  70,  72,  73,  74,  75,  78,  79,  80,  81,  86,  87,  89,\n",
       "        104, 106, 109, 115, 123, 127, 102, 116])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([187, 196, 255,   5, 410]),\n",
       " tensor([ 36, 403, 360, 361, 410]),\n",
       " tensor([  5, 175,  88, 410]),\n",
       " tensor([   5,   45,   46, 1296]),\n",
       " tensor([  19,  595,  169, 1296]),\n",
       " tensor([280, 229,   4, 196]),\n",
       " tensor([563,  13, 142]),\n",
       " tensor([  8,  33, 196]),\n",
       " tensor([ 89,  88, 410]),\n",
       " tensor([  3,  81, 196]),\n",
       " tensor([ 45,  10, 139]),\n",
       " tensor([  7, 290, 196]),\n",
       " tensor([150, 158, 142]),\n",
       " tensor([243,  10, 410]),\n",
       " tensor([   7,  242, 1296]),\n",
       " tensor([268,  74, 142]),\n",
       " tensor([ 30,  85, 142]),\n",
       " tensor([  41,   20, 1296]),\n",
       " tensor([252, 229, 410]),\n",
       " tensor([297,   9, 410]),\n",
       " tensor([ 40,  54, 142]),\n",
       " tensor([256,  79, 196]),\n",
       " tensor([  37,  427, 1296]),\n",
       " tensor([  5,  32, 139]),\n",
       " tensor([ 55, 329, 139]),\n",
       " tensor([202,  73, 142]),\n",
       " tensor([191,  84, 139]),\n",
       " tensor([912,  46, 139]),\n",
       " tensor([  6,  46, 142]),\n",
       " tensor([ 39,   4, 196]),\n",
       " tensor([ 10,  34, 410]),\n",
       " tensor([ 10,  36, 196]),\n",
       " tensor([ 19,   4, 196]),\n",
       " tensor([297,  34, 139]),\n",
       " tensor([ 167,  150, 1296]),\n",
       " tensor([ 10,  22, 410]),\n",
       " tensor([234, 401, 139]),\n",
       " tensor([   7,   19, 1296]),\n",
       " tensor([551,  42, 410]),\n",
       " tensor([ 31,  79, 139]),\n",
       " tensor([  8, 119, 410]),\n",
       " tensor([1128,  157,  196]),\n",
       " tensor([ 81,   6, 139]),\n",
       " tensor([ 81, 139]),\n",
       " tensor([268, 142]),\n",
       " tensor([   7, 1296]),\n",
       " tensor([  10, 1296]),\n",
       " tensor([  5, 139]),\n",
       " tensor([ 27, 142]),\n",
       " tensor([  19, 1296]),\n",
       " tensor([  3, 139]),\n",
       " tensor([ 25, 139]),\n",
       " tensor([  17, 1296]),\n",
       " tensor([  71, 1296]),\n",
       " tensor([ 27, 142]),\n",
       " tensor([  38, 1296]),\n",
       " tensor([ 147, 1296]),\n",
       " tensor([ 17, 196]),\n",
       " tensor([  41, 1296]),\n",
       " tensor([206, 139]),\n",
       " tensor([ 55, 139]),\n",
       " tensor([ 19, 196]),\n",
       " tensor([142]),\n",
       " tensor([1296])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([ 187,   36,    5,    5,   19,  280,  563,    8,   89,    3,   45,    7,\n",
       "         150,  243,    7,  268,   30,   41,  252,  297,   40,  256,   37,    5,\n",
       "          55,  202,  191,  912,    6,   39,   10,   10,   19,  297,  167,   10,\n",
       "         234,    7,  551,   31,    8, 1128,   81,   81,  268,    7,   10,    5,\n",
       "          27,   19,    3,   25,   17,   71,   27,   38,  147,   17,   41,  206,\n",
       "          55,   19,  142, 1296,  196,  403,  175,   45,  595,  229,   13,   33,\n",
       "          88,   81,   10,  290,  158,   10,  242,   74,   85,   20,  229,    9,\n",
       "          54,   79,  427,   32,  329,   73,   84,   46,   46,    4,   34,   36,\n",
       "           4,   34,  150,   22,  401,   19,   42,   79,  119,  157,    6,  139,\n",
       "         142, 1296, 1296,  139,  142, 1296,  139,  139, 1296, 1296,  142, 1296,\n",
       "        1296,  196, 1296,  139,  139,  196,  255,  360,   88,   46,  169,    4,\n",
       "         142,  196,  410,  196,  139,  196,  142,  410, 1296,  142,  142, 1296,\n",
       "         410,  410,  142,  196, 1296,  139,  139,  142,  139,  139,  142,  196,\n",
       "         410,  196,  196,  139, 1296,  410,  139, 1296,  410,  139,  410,  196,\n",
       "         139,    5,  361,  410, 1296, 1296,  196,  410,  410]), batch_sizes=tensor([64, 62, 43,  6,  2]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.pack_sequence(description_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_ts_packed = rnn.pack_sequence(description_ts)\n",
    "description_ts_packed, primitive_types, affine_paramss = description_ts_packed.to(trainer.device), primitive_types.to(trainer.device), affine_paramss.to(trainer.device)\n",
    "\n",
    "prim_pred, normal_dists, pi_dists = trainer.model(description_ts_packed)  \n",
    "prim_types = torch.argmax(prim_pred, dim=1)\n",
    "param_samples = trainer.sample_parameters(normal_dists, pi_dists) # each: N x 1\n",
    "np.random.seed(123)\n",
    "plot_indices = np.random.choice(len(indices), trainer.args.num_visualize, replace=False)\n",
    "print(plot_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.templates.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.primitive_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.calculate_metric(description_ts, indices, prim_types, param_samples, plot_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test_meter.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test_meter.metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = trainer.sample_parameters(normal_dists, pi_dists) # each: N x 1\n",
    "np.random.seed(123)\n",
    "plot_indices = np.random.choice(len(indices), trainer.args.num_visualize, replace=False)\n",
    "dataset_indices = [indices[j].item() for j in plot_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = plot_indices[0]\n",
    "data_idx = dataset_indices[0]\n",
    "prim_type = prim_types[idx].item()\n",
    "info = trainer.test_dataset.data_raw[data_idx]\n",
    "data, template, result = trainer.test_sequences[data_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img1 = rd.render_img(\n",
    "    [result], \n",
    "    img_path=None, \n",
    "    show=False,\n",
    "    side=256,\n",
    "    line_diameter=3,\n",
    "    padding=0,\n",
    "    bg_color=(0,0,0),\n",
    "    fg_color=(1,1,1),\n",
    "    original_side = 256.,\n",
    "    convert = True,\n",
    ")\n",
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_info = {}\n",
    "pred_params = [sample[idx].item() for sample in samples]\n",
    "pred_info[\"theta\"],pred_info[\"sx\"],pred_info[\"sy\"],pred_info[\"hx\"],pred_info[\"tx\"],pred_info[\"ty\"] = \\\n",
    "    pred_params\n",
    "pred_M = ps.get_affine_transformation(pred_info)\n",
    "\n",
    "pred_template_name, pred_template = trainer.templates[int(prim_type)]\n",
    "gt_template_name,_ = trainer.templates[int(info[\"primitive_type\"])]\n",
    "if trainer.args.use_projective:\n",
    "    pred_template_pred_param = cv2.perspectiveTransform(pred_template, pred_M).reshape(-1,2)\n",
    "else:\n",
    "    pred_template_pred_param = cv2.transform(np.array([pred_template]).astype(np.float32), pred_M)[0][:,:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info, pred_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.degrees(info[\"theta\"]),np.degrees(pred_info[\"theta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod(a, n):\n",
    "    return (a % n + n) % n\n",
    "\n",
    "def diff_angle(targetA, sourceA):\n",
    "    a = targetA - sourceA\n",
    "    a = mod(a+np.pi, 2 * np.pi) - np.pi\n",
    "    return a\n",
    "\n",
    "np.degrees(diff_angle(pred_info[\"theta\"], info[\"theta\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = rd.render_img(\n",
    "    [pred_template_pred_param], \n",
    "    img_path=None, \n",
    "    show=False,\n",
    "    side=256,\n",
    "    line_diameter=3,\n",
    "    padding=0,\n",
    "    bg_color=(0,0,0),\n",
    "    fg_color=(1,1,1),\n",
    "    original_side = 256.,\n",
    "    convert = True,\n",
    ")\n",
    "img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = np.asarray(img1)\n",
    "img2 = np.asarray(img2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(img1, img2, max_value=255):\n",
    "    \"\"\"\"Calculating peak signal-to-noise ratio (PSNR) between two images.\"\"\"\n",
    "    mse = np.mean((np.array(img1, dtype=np.float32) - np.array(img2, dtype=np.float32)) ** 2)\n",
    "    if mse == 0:\n",
    "        return 100\n",
    "    return 20 * np.log10(max_value / (np.sqrt(mse)))\n",
    "\n",
    "calculate_psnr(img1,img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img1.astype(np.float32)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((np.array(img1, dtype=np.float32) - np.array(img2, dtype=np.float32)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "img1[img1 < 1] = 1\n",
    "img1[img1 == 255] = 0\n",
    "\n",
    "img2[img2 < 1] = 1\n",
    "img2[img2 == 255] = 0\n",
    "\n",
    "ssim(img1, img2, multichannel=False, data_range=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prim_types.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fitting primitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = h = 256\n",
    "TEMPLATE_DICT = {\n",
    "    'arc' : lambda n : rd.generate_arc(n1=n, radius=1, x0=0, y0=0, template_size=1),\n",
    "    'circle' : lambda n : rd.generate_circle(n1=n, radius=1, x0=0, y0=0, template_size=1),\n",
    "    'square' : lambda n : rd.generate_square(n1=n, template_size=2),\n",
    "    'semicircle' : lambda n : rd.generate_semicircle(n1=n, radius=1, x0=0, y0=0, template_size=w),\n",
    "    'zigzag1' : lambda n : rd.generate_zigzag(n1=n, num_fold=1, side_length=1,template_size=1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_spline_num_sampled_points = num_sampled_points = n = 200\n",
    "use_projective = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "templates = {}\n",
    "template_dict_processed = {}\n",
    "for i,(k,v) in enumerate(TEMPLATE_DICT.items()):\n",
    "    arr = v(b_spline_num_sampled_points)\n",
    "    template_dict_processed[k] = arr\n",
    "    templates[i] = (k,arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.plot_primitives(template_dict_processed,w=2, h=2,num_pngs_per_row = 1, row_figsize = 10, column_figsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import primitive_selector as ps\n",
    "reload(ps)\n",
    "all_data = ps.prepare_data(dfo, templates, num_sampled_points = 200, use_projective = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/raid/xiaoyuz1/primitive_selector_training_data/july_15_all.pkl\", \"wb+\") as f:\n",
    "    pickle.dump(all_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [\"train\", \"dev\", \"test\"]:\n",
    "    data_split = [x for x in all_data if x[\"split\"] == t]\n",
    "    data_split_dict = dict(zip(range(len(data_split)), data_split))\n",
    "    with open(f\"/raid/xiaoyuz1/primitive_selector_training_data/july_15_{t}.pkl\", \"wb+\") as f:\n",
    "        pickle.dump(data_split, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pickle.load(open(\"/raid/xiaoyuz1/primitive_selector_training_data/july_15_all.pkl\", \"rb\"))\n",
    "df = pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data,template,result = all_sequences[100]\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12,24)) \n",
    "axs[0].scatter(data[:,0], data[:,1], s=1, c='g')\n",
    "axs[0].scatter(result[:,0], result[:,1], s=1, c='r')\n",
    "axs[0].scatter(template[:,0], template[:,1], s=1, c='b')\n",
    "\n",
    "axs[0].axis(xmin=-w,xmax=w)\n",
    "axs[0].axis(ymin=h,ymax=-h)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def plot_prepared_data(info, num_sampled_points = 200, use_projective=False):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12,24)) \n",
    "    if info['category'] == 'face':\n",
    "        drawing_raw = CONST.face_json['train_data'][info['image_idx']]\n",
    "        part_indices = list(CONST.face_parts_idx_dict_doodler.keys())\n",
    "    else:\n",
    "        drawing_raw = CONST.angel_json['train_data'][info['image_idx']]\n",
    "        part_indices = list(CONST.angel_parts_idx_dict_doodler.keys())\n",
    "\n",
    "    parts = rd.process_quickdraw_to_part_convex_hull(\n",
    "        drawing_raw,\n",
    "        part_indices,\n",
    "        b_spline_num_sampled_points=num_sampled_points,\n",
    "    )\n",
    "    data = parts[info['part']]\n",
    "    template = templates[info['primitive_type']][1]\n",
    "    transform_mat = np.asarray(info['M']).astype(float).reshape(3,3)\n",
    "#     axs[0].scatter(template[:,0], template[:,1], s=1, c='b')\n",
    "    \n",
    "    if use_projective:\n",
    "        result = cv2.perspectiveTransform(template, transform_mat).reshape(-1,2)\n",
    "    else:\n",
    "        result = cv2.transform(np.array([template]).astype(\n",
    "            np.float32), transform_mat)[0][:,:-1]\n",
    "    \n",
    "    \n",
    "    axs[0].scatter(result[:,0], result[:,1], s=1, c='r')\n",
    "    \n",
    "#     theta, scale_mat, shear_mat = rd.decompose_affine(transform_mat)\n",
    "#     T_rotate = rd.get_rotation_matrix(theta)\n",
    "#     T_scale = rd.get_scale_matrix(scale_mat[0][0], scale_mat[1][1])\n",
    "#     T_shear = rd.get_shear_matrix(shear_mat[0][1], shear_mat[1][0])\n",
    "#     tx,ty = transform_mat[0][2],transform_mat[1][2]\n",
    "#     T_translate = rd.get_translation_matrix(tx,ty)\n",
    "    \n",
    "#     result1 = cv2.transform(np.array([template]).astype(\n",
    "#             np.float32), T_rotate)[0][:,:-1]\n",
    "#     axs[0].scatter(result1[:,0], result1[:,1], s=1, c='m')\n",
    "    \n",
    "#     result2 = cv2.transform(np.array([template]).astype(\n",
    "#             np.float32), T_rotate @ T_scale @ T_shear)[0][:,:-1]\n",
    "#     axs[0].scatter(result2[:,0], result2[:,1], s=1, alpha=0.5, c='pink')\n",
    "\n",
    "#     result3 = cv2.transform(np.array([template]).astype(\n",
    "#             np.float32), T_translate @ T_rotate @ T_scale @ T_shear)[0][:,:-1]\n",
    "#     axs[0].scatter(result3[:,0], result3[:,1], s=1, c='lime', alpha=0.5)\n",
    "    \n",
    "    axs[0].scatter(data[:,0], data[:,1], alpha=0.5, s=1, c='lime')\n",
    "\n",
    "    axs[0].axis(xmin=-w,xmax=w)\n",
    "    axs[0].axis(ymin=h,ymax=-h)\n",
    "    axs[1].axis(xmin=-w,xmax=w)\n",
    "    axs[1].axis(ymin=h,ymax=-h)\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_prepared_data(df.iloc[123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_all = np.asarray(df.M.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "X = np.asarray(df.sx.to_list()).reshape(-1, 1)\n",
    "# M_all[:,0].reshape(-1, 1)\n",
    "print(X.min(), X.max())\n",
    "\n",
    "N = np.arange(1, 11)\n",
    "models = [None for i in range(len(N))]\n",
    "\n",
    "for i in range(len(N)):\n",
    "    models[i] = GaussianMixture(N[i]).fit(X)\n",
    "\n",
    "# compute the AIC and the BIC\n",
    "AIC = [m.aic(X) for m in models]\n",
    "BIC = [m.bic(X) for m in models]\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "#  We'll use three panels:\n",
    "#   1) data + best-fit mixture\n",
    "#   2) AIC and BIC vs number of components\n",
    "#   3) probability that a point came from each component\n",
    "\n",
    "fig = plt.figure(figsize=(15, 20))\n",
    "fig.subplots_adjust(left=0.12, right=0.97,\n",
    "                    bottom=0.21, top=0.9, wspace=0.5)\n",
    "\n",
    "\n",
    "# plot 1: data + best-fit mixture\n",
    "ax = fig.add_subplot(311)\n",
    "M_best = models[np.argmin(AIC)]\n",
    "\n",
    "x = np.linspace(X.min(), X.max(), 1000)\n",
    "logprob = M_best.score_samples(x.reshape(-1, 1))\n",
    "responsibilities = M_best.predict_proba(x.reshape(-1, 1))\n",
    "pdf = np.exp(logprob)\n",
    "pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "\n",
    "ax.hist(X, 30, density=True, histtype='stepfilled', alpha=0.4)\n",
    "ax.plot(x, pdf, '-k')\n",
    "ax.plot(x, pdf_individual, '--k')\n",
    "ax.text(0.04, 0.96, \"Best-fit Mixture\",\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$p(x)$')\n",
    "\n",
    "\n",
    "# plot 2: AIC and BIC\n",
    "ax = fig.add_subplot(312)\n",
    "ax.plot(N, AIC, '-k', label='AIC')\n",
    "ax.plot(N, BIC, '--k', label='BIC')\n",
    "ax.set_xlabel('n. components')\n",
    "ax.set_ylabel('information criterion')\n",
    "ax.legend(loc=2)\n",
    "\n",
    "\n",
    "# plot 3: posterior probabilities for each component\n",
    "ax = fig.add_subplot(313)\n",
    "\n",
    "p = responsibilities\n",
    "p = p[:, (1, 0, 2)]  # rearrange order so the plot looks better\n",
    "p = p.cumsum(1).T\n",
    "\n",
    "ax.fill_between(x, 0, p[0], color='gray', alpha=0.3)\n",
    "ax.fill_between(x, p[0], p[1], color='gray', alpha=0.5)\n",
    "ax.fill_between(x, p[1], 1, color='gray', alpha=0.7)\n",
    "ax.set_xlim(-6, 6)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel(r'$p({\\rm class}|x)$')\n",
    "\n",
    "ax.text(-5, 0.3, 'class 1', rotation='vertical')\n",
    "ax.text(0, 0.5, 'class 2', rotation='vertical')\n",
    "ax.text(3, 0.3, 'class 3', rotation='vertical')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M_best.weights_)\n",
    "print(M_best.means_)\n",
    "print(M_best.covariances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prepared_data(df.iloc[110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawing_arr = CONST.face_json['train_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(rd)\n",
    "from tqdm import tqdm\n",
    "\n",
    "M_arr = collections.defaultdict(lambda : np.zeros(9))\n",
    "M_name = collections.defaultdict(lambda : \" \")\n",
    "M_mse = collections.defaultdict(float)\n",
    "\n",
    "\n",
    "for idx in tqdm(range(len(drawing_arr))):\n",
    "# for idx in range(len(drawing_arr)):\n",
    "#     print(idx)\n",
    "    \n",
    "    strokes_spline_fitted = rd.process_quickdraw_to_part_convex_hull(\n",
    "        drawing_arr[idx],\n",
    "        list(CONST.face_parts_idx_dict.keys()),\n",
    "        b_spline_num_sampled_points=n,\n",
    "    )\n",
    "\n",
    "    for _, (part_type, sketch_data) in enumerate(strokes_spline_fitted.items()):\n",
    "        \n",
    "        mat, prim_name, prim_mse = rd.get_transform_smallest_mse(\n",
    "            sketch_data, template_dict_processed, use_projective=use_projective,\n",
    "        )\n",
    "        M_arr[(idx, part_type)] = mat\n",
    "        M_name[(idx, part_type)] = prim_name\n",
    "        M_mse[(idx, part_type)] = prim_mse\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/raid/xiaoyuz1/SPG_Face_Part_256_projective_{}_mse.pkl'.format(use_projective), \"wb+\") as f:\n",
    "    pickle.dump((dict(M_arr), dict(M_name), dict(M_mse)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'/raid/xiaoyuz1/SPG_Face_Part_256_projective_{}_mse.pkl'.format(use_projective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_dict = dict(filter(lambda x: x[1] == 'line', M_name.items()))\n",
    "print(len(line_dict), line_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_arr, M_name = None,None\n",
    "M_mse = None\n",
    "fname = '/raid/xiaoyuz1/SPG_Face_Part_256_projective_False_mse.pkl'\n",
    "with open(fname, \"rb\") as f:\n",
    "    M_arr, M_name, M_mse = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_l = list(filter(lambda x: not math.isinf(x), list(M_mse.values())))\n",
    "np.mean(mse_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_list = list(map(lambda x: int(x[0]), M_name.keys()))\n",
    "stroke_list = list(map(lambda x: int(x[1]), M_name.keys()))\n",
    "arr_list = list(M_arr.values())\n",
    "name_list = list(M_name.values())\n",
    "mse_list = list(M_mse.values())\n",
    "\n",
    "data = {'image': image_list, 'stroke' : stroke_list, 'M' : arr_list, 'primitive' : name_list, 'mse' : mse_list}\n",
    "\n",
    "for c_idx,c in enumerate(np.asarray(arr_list).T):\n",
    "    if c_idx > 5:\n",
    "        continue\n",
    "    data['M{}'.format(str(c_idx))] = c\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = df[df.mse.apply(lambda x: not math.isinf(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_all = np.asarray(dfa.M.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "stroke_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rd.show_these_sketches(\n",
    "    CONST.face_json,\n",
    "    [idx], \n",
    "    [str(idx)], \n",
    "    [[]], \n",
    "    num_pngs_per_row = 6,\n",
    "    row_figsize = 4,\n",
    "    column_figsize = 4,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strokes_spline_fitted = rd.process_quickdraw_to_part_convex_hull(\n",
    "    drawing_arr[idx],\n",
    "    list(CONST.face_parts_idx_dict.keys()),\n",
    "    b_spline_num_sampled_points=n,\n",
    ")\n",
    "\n",
    "# strokes_spline_fitted = rd.process_quickdraw_to_stroke_no_normalize(\n",
    "#     drawing_arr[idx], \n",
    "#     b_spline_num_sampled_points=200,\n",
    "# )\n",
    "# print(len(strokes_spline_fitted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "\n",
    "parts_indices = list(CONST.face_parts_idx_dict.keys())\n",
    "\n",
    "drawing_raw = np.asarray(drawing_arr[idx])\n",
    "drawing_raw[:,0] = np.cumsum(drawing_raw[:,0], 0) + 25\n",
    "drawing_raw[:,1] = np.cumsum(drawing_raw[:,1], 0) + 25\n",
    "\n",
    "parts = []\n",
    "part_idx_part = {}\n",
    "\n",
    "for k in parts_indices:\n",
    "    strokes = drawing_raw[drawing_raw[:,-1] == k]\n",
    "    if len(strokes) < 1:\n",
    "        continue\n",
    "    parts.append(strokes)\n",
    "    part_idx_part[k] = strokes[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_arr_idx, M_name_idx, M_mse_idx = {},{},{}\n",
    "for stroke_index, (part_type, data) in enumerate(strokes_spline_fitted.items()):\n",
    "        \n",
    "    mat, prim_name, prim_mse = rd.get_transform_smallest_mse(\n",
    "        data, template_dict_processed, use_projective=use_projective,\n",
    "    )\n",
    "    M_arr_idx[(idx, part_type)] = mat\n",
    "    M_name_idx[(idx, part_type)] = prim_name\n",
    "    M_mse_idx[(idx, part_type)] = prim_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "for k, M in M_arr_idx.items():\n",
    "#     if k[1] != 3:\n",
    "#         continue\n",
    "    data = strokes_spline_fitted[k[1]]\n",
    "    \n",
    "    template = template_dict_processed[M_name_idx[(idx, k[1])]]\n",
    "    Mm = M.reshape(3,3)\n",
    "    \n",
    "    result, mse = rd.get_transformed_template(template, data, Mm,projective=use_projective)\n",
    "    ax1.scatter(result[:,0], result[:,1], label=\"transformed template\", s=1, c='r')\n",
    "    ax2.scatter(data[:,0], data[:,1], label=\"data\", alpha=0.5, s=1, c='g')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.xlim(0,w)\n",
    "# plt.ylim(h,0)\n",
    "\n",
    "\n",
    "ax1.axis(xmin=0,xmax=w)\n",
    "ax1.axis(ymin=h,ymax=0)\n",
    "\n",
    "ax2.axis(xmin=0,xmax=w)\n",
    "ax2.axis(ymin=h,ymax=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_arr_idx, M_name_idx, M_mse_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "for index, entry in dfa[dfa.image == idx].iterrows():\n",
    "    data = strokes_spline_fitted[entry.stroke]\n",
    "    \n",
    "    template_func = TEMPLATE_DICT[entry.primitive]\n",
    "    template = template_func(n)\n",
    "    \n",
    "    print(entry.M.reshape(3,3))\n",
    "    \n",
    "    result, mse = rd.get_transformed_template(template, data, entry.M.reshape(3,3),projective=use_projective)\n",
    "    ax1.scatter(result[:,0], result[:,1], label=\"transformed template\", s=1, c='r')\n",
    "    ax2.scatter(data[:,0], data[:,1], label=\"data\", alpha=0.5, s=1, c='g')\n",
    "    orig_data = part_idx_part[int(entry.stroke)]\n",
    "    ax2.plot(orig_data[:,0], orig_data[:,1], alpha=0.5, c='b')\n",
    "    \n",
    "#     ax1.plot(result[:,0], result[:,1], label=\"transformed template\", c='r')\n",
    "#     ax2.plot(data[:,0], data[:,1], label=\"data\", alpha=0.5,c='g')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.xlim(0,w)\n",
    "# plt.ylim(h,0)\n",
    "\n",
    "ax1.axis(xmin=0,xmax=w)\n",
    "ax1.axis(ymin=h,ymax=0)\n",
    "\n",
    "ax2.axis(xmin=0,xmax=w)\n",
    "ax2.axis(ymin=h,ymax=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "scrolled": true
   },
   "source": [
    "## old icp code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "from numpy.random import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def del_miss(indeces, dist, max_dist, th_rate = 0.8):\n",
    "    th_dist = max_dist * th_rate\n",
    "    return np.array([indeces[0][np.where(dist.T[0] < th_dist)]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def is_converge(Tr, scale):\n",
    "    delta_angle = 0.0001\n",
    "    delta_scale = scale * 0.0001\n",
    "    \n",
    "    min_cos = 1 - delta_angle\n",
    "    max_cos = 1 + delta_angle\n",
    "    min_sin = -delta_angle\n",
    "    max_sin = delta_angle\n",
    "    min_move = -delta_scale\n",
    "    max_move = delta_scale\n",
    "    \n",
    "    return min_cos < Tr[0, 0] and Tr[0, 0] < max_cos and \\\n",
    "           min_cos < Tr[1, 1] and Tr[1, 1] < max_cos and \\\n",
    "           min_sin < -Tr[1, 0] and -Tr[1, 0] < max_sin and \\\n",
    "           min_sin < Tr[0, 1] and Tr[0, 1] < max_sin and \\\n",
    "           min_move < Tr[0, 2] and Tr[0, 2] < max_move and \\\n",
    "           min_move < Tr[1, 2] and Tr[1, 2] < max_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def icp(d1, d2, max_iterate = 100):\n",
    "    src = np.array([d1.T], copy=True).astype(np.float32)\n",
    "    dst = np.array([d2.T], copy=True).astype(np.float32)\n",
    "    \n",
    "    knn = cv2.KNearest()\n",
    "    responses = np.array(range(len(d2[0]))).astype(np.float32)\n",
    "    knn.train(src[0], responses)\n",
    "        \n",
    "    Tr = np.array([[np.cos(0), -np.sin(0), 0],\n",
    "                   [np.sin(0), np.cos(0),  0],\n",
    "                   [0,         0,          1]])\n",
    "\n",
    "    dst = cv2.transform(dst, Tr[0:2])\n",
    "    max_dist = sys.maxint\n",
    "    \n",
    "    scale_x = np.max(d1[0]) - np.min(d1[0])\n",
    "    scale_y = np.max(d1[1]) - np.min(d1[1])\n",
    "    scale = max(scale_x, scale_y)\n",
    "       \n",
    "    for i in range(max_iterate):\n",
    "        ret, results, neighbours, dist = knn.find_nearest(dst[0], 1)\n",
    "        \n",
    "        indeces = results.astype(np.int32).T     \n",
    "        indeces = del_miss(indeces, dist, max_dist)  \n",
    "        \n",
    "        T = cv2.estimateRigidTransform(dst[0, indeces], src[0, indeces], True)\n",
    "\n",
    "        max_dist = np.max(dist)\n",
    "        dst = cv2.transform(dst, T)\n",
    "        Tr = np.dot(np.vstack((T,[0,0,1])), Tr)\n",
    "        \n",
    "        if (is_converge(T, scale)):\n",
    "            break\n",
    "        \n",
    "    return Tr[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    point_count = 100\n",
    "    th = np.pi / 8\n",
    "    move = np.array([[0.30], [0.5]])\n",
    "    rnd_scale = 0.03\n",
    "    x1 = np.linspace(0, 1.1, point_count)\n",
    "    y1 = np.sin(x1 * np.pi)\n",
    "    d1 = np.array([x1, y1])\n",
    "\n",
    "    rot = np.array([[np.cos(th), -np.sin(th)], [np.sin(th), np.cos(th)]])\n",
    "    rand = np.random.rand(2, point_count)*rnd_scale\n",
    "    d2 = np.dot(rot, d1) + move\n",
    "    d2 = np.add(d2, rand)\n",
    "\n",
    "    plt.plot(d1[0], d1[1])\n",
    "    plt.plot(d2[0], d2[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    ret = icp(d1, d2)\n",
    "    \n",
    "    plt.plot(d1[0], d1[1])\n",
    "    dst = np.array([d2.T], copy=True).astype(np.float32)\n",
    "    dst = cv2.transform(dst, ret)\n",
    "    plt.plot(dst[0].T[0], dst[0].T[1])\n",
    "    plt.show()\n",
    "    \n",
    "    print ret[0][0] * ret[0][0] + ret[0][1] * ret[0][1]\n",
    "    print np.arccos(ret[0][0]) / 2 / np.pi * 360\n",
    "    print np.arcsin(ret[0][1]) / 2 / np.pi * 360\n",
    "\n",
    "    print ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def icp(a, b,\n",
    "        max_time = 1\n",
    "    ):\n",
    "    import cv2\n",
    "    import numpy\n",
    "    import copy\n",
    "    import pylab\n",
    "    import time\n",
    "    import sys\n",
    "    import sklearn.neighbors\n",
    "    import scipy.optimize\n",
    "\n",
    "\n",
    "\n",
    "    def res(p,src,dst):\n",
    "        T = numpy.matrix([[numpy.cos(p[2]),-numpy.sin(p[2]),p[0]],\n",
    "        [numpy.sin(p[2]), numpy.cos(p[2]),p[1]],\n",
    "        [0 ,0 ,1 ]])\n",
    "        n = numpy.size(src,0)\n",
    "        xt = numpy.ones([n,3])\n",
    "        xt[:,:-1] = src\n",
    "        xt = (xt*T.T).A\n",
    "        d = numpy.zeros(numpy.shape(src))\n",
    "        d[:,0] = xt[:,0]-dst[:,0]\n",
    "        d[:,1] = xt[:,1]-dst[:,1]\n",
    "        r = numpy.sum(numpy.square(d[:,0])+numpy.square(d[:,1]))\n",
    "        return r\n",
    "\n",
    "    def jac(p,src,dst):\n",
    "        T = numpy.matrix([[numpy.cos(p[2]),-numpy.sin(p[2]),p[0]],\n",
    "        [numpy.sin(p[2]), numpy.cos(p[2]),p[1]],\n",
    "        [0 ,0 ,1 ]])\n",
    "        n = numpy.size(src,0)\n",
    "        xt = numpy.ones([n,3])\n",
    "        xt[:,:-1] = src\n",
    "        xt = (xt*T.T).A\n",
    "        d = numpy.zeros(numpy.shape(src))\n",
    "        d[:,0] = xt[:,0]-dst[:,0]\n",
    "        d[:,1] = xt[:,1]-dst[:,1]\n",
    "        dUdth_R = numpy.matrix([[-numpy.sin(p[2]),-numpy.cos(p[2])],\n",
    "                            [ numpy.cos(p[2]),-numpy.sin(p[2])]])\n",
    "        dUdth = (src*dUdth_R.T).A\n",
    "        g = numpy.array([  numpy.sum(2*d[:,0]),\n",
    "                        numpy.sum(2*d[:,1]),\n",
    "                        numpy.sum(2*(d[:,0]*dUdth[:,0]+d[:,1]*dUdth[:,1])) ])\n",
    "        return g\n",
    "    \n",
    "    def hess(p,src,dst):\n",
    "        n = numpy.size(src,0)\n",
    "        T = numpy.matrix([[numpy.cos(p[2]),-numpy.sin(p[2]),p[0]],\n",
    "        [numpy.sin(p[2]), numpy.cos(p[2]),p[1]],\n",
    "        [0 ,0 ,1 ]])\n",
    "        n = numpy.size(src,0)\n",
    "        xt = numpy.ones([n,3])\n",
    "        xt[:,:-1] = src\n",
    "        xt = (xt*T.T).A\n",
    "        d = numpy.zeros(numpy.shape(src))\n",
    "        d[:,0] = xt[:,0]-dst[:,0]\n",
    "        d[:,1] = xt[:,1]-dst[:,1]\n",
    "        dUdth_R = numpy.matrix([[-numpy.sin(p[2]),-numpy.cos(p[2])],[numpy.cos(p[2]),-numpy.sin(p[2])]])\n",
    "        dUdth = (src*dUdth_R.T).A\n",
    "        H = numpy.zeros([3,3])\n",
    "        H[0,0] = n*2\n",
    "        H[0,2] = numpy.sum(2*dUdth[:,0])\n",
    "        H[1,1] = n*2\n",
    "        H[1,2] = numpy.sum(2*dUdth[:,1])\n",
    "        H[2,0] = H[0,2]\n",
    "        H[2,1] = H[1,2]\n",
    "        d2Ud2th_R = numpy.matrix([[-numpy.cos(p[2]), numpy.sin(p[2])],[-numpy.sin(p[2]),-numpy.cos(p[2])]])\n",
    "        d2Ud2th = (src*d2Ud2th_R.T).A\n",
    "        H[2,2] = numpy.sum(2*(numpy.square(dUdth[:,0])+numpy.square(dUdth[:,1]) + d[:,0]*d2Ud2th[:,0]+d[:,0]*d2Ud2th[:,0]))\n",
    "        return H\n",
    "    \n",
    "    \n",
    "    t0 = time.time()\n",
    "    init_pose = (0,0,0)\n",
    "    src = numpy.array([a.T], copy=True).astype(numpy.float32)\n",
    "    dst = numpy.array([b.T], copy=True).astype(numpy.float32)\n",
    "    Tr = numpy.array([[numpy.cos(init_pose[2]),-numpy.sin(init_pose[2]),init_pose[0]],\n",
    "                   [numpy.sin(init_pose[2]), numpy.cos(init_pose[2]),init_pose[1]],\n",
    "                   [0,                    0,                   1          ]])\n",
    "    print(\"src\",numpy.shape(src))\n",
    "    print(\"Tr[0:2]\",numpy.shape(Tr[0:2]))\n",
    "    src = cv2.transform(src, Tr[0:2])\n",
    "    p_opt = numpy.array(init_pose)\n",
    "    T_opt = numpy.array([])\n",
    "    error_max = sys.maxsize\n",
    "    first = False\n",
    "    while not(first and time.time() - t0 > max_time):\n",
    "        distances, indices = sklearn.neighbors.NearestNeighbors(n_neighbors=1, algorithm='auto',p = 3).fit(dst[0]).kneighbors(src[0])\n",
    "        p = scipy.optimize.minimize(res,[0,0,0],args=(src[0],dst[0, indices.T][0]),method='Newton-CG',jac=jac,hess=hess).x\n",
    "        T  = numpy.array([[numpy.cos(p[2]),-numpy.sin(p[2]),p[0]],[numpy.sin(p[2]), numpy.cos(p[2]),p[1]]])\n",
    "        p_opt[:2]  = (p_opt[:2]*numpy.matrix(T[:2,:2]).T).A       \n",
    "        p_opt[0] += p[0]\n",
    "        p_opt[1] += p[1]\n",
    "        p_opt[2] += p[2]\n",
    "        src = cv2.transform(src, T)\n",
    "        Tr = (numpy.matrix(numpy.vstack((T,[0,0,1])))*numpy.matrix(Tr)).A\n",
    "        error = res([0,0,0],src[0],dst[0, indices.T][0])\n",
    "\n",
    "        if error < error_max:\n",
    "            error_max = error\n",
    "            first = True\n",
    "            T_opt = Tr\n",
    "\n",
    "    p_opt[2] = p_opt[2] % (2*numpy.pi)\n",
    "    return T_opt, error_max\n",
    "\n",
    "\n",
    "def main():\n",
    "    import cv2\n",
    "    import numpy\n",
    "    import random\n",
    "    import matplotlib.pyplot\n",
    "    n1 = 100\n",
    "    n2 = 75\n",
    "    bruit = 1/10\n",
    "    center = [random.random()*(2-1)*3,random.random()*(2-1)*3]\n",
    "    radius = random.random()\n",
    "    deformation = 2\n",
    "\n",
    "    template = numpy.array([\n",
    "        [numpy.cos(i*2*numpy.pi/n1)*radius*deformation for i in range(n1)], \n",
    "        [numpy.sin(i*2*numpy.pi/n1)*radius for i in range(n1)]\n",
    "    ])\n",
    "\n",
    "    data = numpy.array([\n",
    "        [numpy.cos(i*2*numpy.pi/n2)*radius*(1+random.random()*bruit)+center[0] for i in range(n2)], \n",
    "        [numpy.sin(i*2*numpy.pi/n2)*radius*deformation*(1+random.random()*bruit)+center[1] for i in range(n2)]\n",
    "    ])\n",
    "\n",
    "    T,error = icp(data,template)\n",
    "    dx = T[0,2]\n",
    "    dy = T[1,2]\n",
    "    rotation = numpy.arcsin(T[0,1]) * 360 / 2 / numpy.pi\n",
    "\n",
    "    print(\"T\",T)\n",
    "    print(\"error\",error)\n",
    "    print(\"rotation°\",rotation)\n",
    "    print(\"dx\",dx)\n",
    "    print(\"dy\",dy)\n",
    "\n",
    "    result = cv2.transform(numpy.array([data.T], copy=True).astype(numpy.float32), T).T\n",
    "    matplotlib.pyplot.plot(template[0], template[1], label=\"template\")\n",
    "    matplotlib.pyplot.plot(data[0], data[1], label=\"data\")\n",
    "    matplotlib.pyplot.plot(result[0], result[1], label=\"result: \"+str(rotation)+\"° - \"+str([dx,dy]))\n",
    "    matplotlib.pyplot.legend(loc=\"upper left\")\n",
    "    matplotlib.pyplot.axis('square')\n",
    "    matplotlib.pyplot.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipdraw",
   "language": "python",
   "name": "clipdraw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
