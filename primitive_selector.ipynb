{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "primitive_selector.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "n6wpXVmcup_D",
        "aDIIs3mFuhVH"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPX47kddbPRR2e8xOZUAH75",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErinZhang1998/sketch_collection/blob/master/primitive_selector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4tcGKQMfvmx"
      },
      "outputs": [],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install svgwrite \n",
        "! pip install CairoSVG\n",
        "! pip install wandb"
      ],
      "metadata": {
        "id": "snvBM4_siij_",
        "outputId": "2dbb9bda-f239-4f66-af42-bd92ce852a7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting svgwrite\n",
            "  Downloading svgwrite-1.4.2-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: svgwrite\n",
            "Successfully installed svgwrite-1.4.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting CairoSVG\n",
            "  Downloading CairoSVG-2.5.2-py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tinycss2 in /usr/local/lib/python3.7/dist-packages (from CairoSVG) (1.1.1)\n",
            "Collecting cssselect2\n",
            "  Downloading cssselect2-0.6.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from CairoSVG) (0.7.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from CairoSVG) (7.1.2)\n",
            "Collecting cairocffi\n",
            "  Downloading cairocffi-1.3.0.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from cairocffi->CairoSVG) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1.0->cairocffi->CairoSVG) (2.21)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from cssselect2->CairoSVG) (0.5.1)\n",
            "Building wheels for collected packages: cairocffi\n",
            "  Building wheel for cairocffi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cairocffi: filename=cairocffi-1.3.0-py3-none-any.whl size=89668 sha256=76113ad9ae2a615679d7986ae42777e1837d5d07be1bc9704d854622efb9788e\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/e1/5c8a9692a27f639a07c949044bec943f26c81cd53d3805319f\n",
            "Successfully built cairocffi\n",
            "Installing collected packages: cssselect2, cairocffi, CairoSVG\n",
            "Successfully installed CairoSVG-2.5.2 cairocffi-1.3.0 cssselect2-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "id": "MYyiGLo8f9CG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeda4624-6a3b-4f16-a4c5-3dbe8a43861f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gdrive/MyDrive"
      ],
      "metadata": {
        "id": "EyncPV8Lhyod",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6baad7f-2129-41b3-dc36-78274977b8f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/ErinZhang1998/sketch_collection.git"
      ],
      "metadata": {
        "id": "xvhuR-Q8f9Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "0MMvMZMeiLXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c3acacf-da05-4cb9-e2d8-9c542937966d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Colab Notebooks'\t      pleasant-tree-10.pt\n",
            "'Copy of 简历修改建议.gdoc'   primitive_selector_training_data\n",
            " doodler_model_checkpoint     sketch_collection\n",
            "'Getting started.pdf'\t      wandb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys    \n",
        "path_to_module = '/content/gdrive/MyDrive/sketch_collection'\n",
        "sys.path.append(path_to_module)"
      ],
      "metadata": {
        "id": "eOBKDgcwf9Qq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %mkdir doodler_model_checkpoint"
      ],
      "metadata": {
        "id": "2sTL0JKyiw9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import read_datasets as rd\n",
        "import numpy as np \n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import wandb \n",
        "wandb.login()\n",
        "import argparse\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.rnn as rnn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal, OneHotCategorical"
      ],
      "metadata": {
        "id": "rgxSp6VBr3J6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be5e7970-7a25-4a40-daf5-e742f7f28f00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merinz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = pickle.load(open(\"/content/gdrive/MyDrive/primitive_selector_training_data/july_13_all.pkl\", \"rb\"))\n",
        "# df = pd.DataFrame.from_dict(all_data, orient='index')\n",
        "df = pd.DataFrame(all_data)"
      ],
      "metadata": {
        "id": "uZ4Htljeq1Qv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training with LSTM"
      ],
      "metadata": {
        "id": "xyyKxJU9utW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset_language(path):\n",
        "    f = open(path, \"rb\")\n",
        "    data_raw = pickle.load(f)\n",
        "    q2i = defaultdict(lambda: len(q2i))\n",
        "    pad = q2i[\"<pad>\"]\n",
        "    UNK = q2i[\"<unk>\"]\n",
        "    \n",
        "    for info in data_raw:\n",
        "        description = info['processed']\n",
        "        [q2i[x] for x in description.lower().strip().split(\" \")]\n",
        "    return q2i"
      ],
      "metadata": {
        "id": "XyDDK25Q0L-o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2i = preprocess_dataset_language(\n",
        "    \"/content/gdrive/MyDrive/primitive_selector_training_data/july_13_all.pkl\"\n",
        ")"
      ],
      "metadata": {
        "id": "QEPq9S0jvtl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_primitivedataset(seq_list):\n",
        "    description_ts, primitive_types, affine_paramss = zip(*seq_list)\n",
        "    lens = [len(x) for x in description_ts]\n",
        "    seq_order = sorted(range(len(lens)), key=lens.__getitem__, reverse=True)\n",
        "    description_ts = [description_ts[i] for i in seq_order]\n",
        "    \n",
        "    primitive_types = torch.stack([primitive_types[i] for i in seq_order])\n",
        "    affine_paramss = torch.stack([affine_paramss[i] for i in seq_order])\n",
        "    \n",
        "    # (N, 1) (N, num_transformation_params)\n",
        "    return description_ts, primitive_types, affine_paramss\n"
      ],
      "metadata": {
        "id": "iy5YscsFupe9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "SFFUVG2u0c6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PrimitiveDataset(Dataset):\n",
        "    def __init__(self, path, vocab, num_transformation_params, image_size=256.):\n",
        "        super().__init__()\n",
        "        self.path = path\n",
        "        self.image_size = image_size\n",
        "        \n",
        "        f = open(self.path, \"rb\")\n",
        "        self.data_raw = pickle.load(f)\n",
        "        \n",
        "        self.vocab = vocab\n",
        "        self.vocab_keys = vocab.keys()\n",
        "        self.original_image_size = 256.\n",
        "        self.num_transformation_params = num_transformation_params\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_raw)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # Process language input \n",
        "        info = self.data_raw[index]\n",
        "        description = info['processed']\n",
        "        description_t = [self.vocab[x.lower()] for x in description.split(\" \") if x.lower() in self.vocab_keys]\n",
        "        description_t = torch.from_numpy(np.array(description_t)).long()\n",
        "        \n",
        "        # Process M (num_transformation_params,) and type\n",
        "        primitive_type = torch.from_numpy(np.array([info['primitive_type']])).long()\n",
        "        if 'M' in info:\n",
        "            # affine_params = info['M'].reshape(-1,)[:self.num_transformation_params]\n",
        "            affine_params = info['M'].reshape(-1,)[[2,5]]\n",
        "            affine_params = torch.FloatTensor(affine_params)\n",
        "        else:\n",
        "            raise \n",
        "        return description_t, primitive_type, affine_params\n"
      ],
      "metadata": {
        "id": "Y9VWBJ2oux68"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "EEIIhlLy0fi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PrimitiveSelector(nn.Module):\n",
        "    def __init__(self, hp):\n",
        "        super().__init__()\n",
        "        '''\n",
        "        num_embeddings: vocab size \n",
        "        '''\n",
        "        self.hp = hp\n",
        "        self.embed = nn.Embedding(hp.vocab_size, hp.word_embed_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size = hp.word_embed_dim, \n",
        "            hidden_size = hp.lstm_output_dim, \n",
        "            num_layers = hp.lstm_layers, \n",
        "            dropout = hp.lstm_drop_prob,\n",
        "        )\n",
        "\n",
        "        self.primitive_fc = nn.Linear(hp.lstm_output_dim, hp.num_primitives)\n",
        "        self.num_normal_param = 3\n",
        "        self.gmm_network = nn.Linear(hp.lstm_output_dim, hp.num_transformation_params * 2 * 1 * hp.M)\n",
        "        self.pi_network = nn.Linear(hp.lstm_output_dim, hp.num_transformation_params * hp.M)\n",
        "\n",
        "        # self.gmm_network = nn.Sequential(\n",
        "        #     nn.Linear(hp.lstm_output_dim, hp.lstm_output_dim),\n",
        "        #     nn.ELU(),\n",
        "        #     nn.Linear(hp.lstm_output_dim, hp.num_transformation_params * 2 * 1 * hp.M)\n",
        "        # )\n",
        "        # self.pi_network = nn.Sequential(\n",
        "        #     nn.Linear(hp.lstm_output_dim, hp.lstm_output_dim),\n",
        "        #     nn.ELU(),\n",
        "        #     nn.Linear(hp.lstm_output_dim, hp.num_transformation_params * hp.M)\n",
        "        # )\n",
        "    \n",
        "    def forward(self, question):\n",
        "        seq_tensor, seq_lengths = rnn.pad_packed_sequence(question, batch_first=True)               \n",
        "        embedded_seq_tensor = self.embed(seq_tensor)\n",
        "        seq_packed = rnn.pack_padded_sequence(\n",
        "            torch.transpose(embedded_seq_tensor,0,1), \n",
        "            seq_lengths)\n",
        "        _, (hidden,_) = self.lstm(seq_packed, None)\n",
        "        seq_last_layer = hidden[-1] # N x lstm_output_dim\n",
        "        prim_pred = self.primitive_fc(seq_last_layer) \n",
        "\n",
        "        params = self.gmm_network(seq_last_layer)\n",
        "        pis = self.pi_network(seq_last_layer)\n",
        "        # mean, sd = torch.split(params, params.shape[1] // 2, dim=1)\n",
        "        # mean = torch.stack(mean.split(mean.shape[1] // self.hp.M, 1))\n",
        "        # sd = torch.stack(sd.split(sd.shape[1] // self.hp.M, 1))\n",
        "        # normal_dist = Normal(mean.transpose(0, 1), (F.elu(sd)+1+1e-7).transpose(0, 1))\n",
        "        # pi_dist = OneHotCategorical(logits=pis)\n",
        "\n",
        "        params_list = torch.split(params, 2 * 1 * self.hp.M, dim=1)\n",
        "        pis_list = torch.split(pis, self.hp.M, dim=1)\n",
        "        \n",
        "        normal_dists, pi_dists = [],[]\n",
        "        for i in range(self.hp.num_transformation_params):\n",
        "            param = params_list[i]\n",
        "            pi = pis_list[i]\n",
        "            mean, sd = torch.split(param, param.shape[1] // 2, dim=1)\n",
        "            mean = torch.stack(mean.split(mean.shape[1] // self.hp.M, 1))\n",
        "            sd = torch.stack(sd.split(sd.shape[1] // self.hp.M, 1))\n",
        "            \n",
        "            # print(mean,sd)\n",
        "            \n",
        "            normal_dist = Normal(mean.transpose(0, 1), (F.elu(sd)+1+1e-7).transpose(0, 1))\n",
        "            pi_dist = OneHotCategorical(logits=pi)\n",
        "\n",
        "            normal_dists.append(normal_dist)\n",
        "            pi_dists.append(pi_dist)\n",
        "\n",
        "        return prim_pred, normal_dists, pi_dists\n",
        "\n",
        "    # def forward(self, question): # question: PackedSequence \n",
        "    #     seq_tensor, seq_lengths = rnn.pad_packed_sequence(question, batch_first=True)               \n",
        "    #     embedded_seq_tensor = self.embed(seq_tensor)\n",
        "    #     seq_packed = rnn.pack_padded_sequence(\n",
        "    #         torch.transpose(embedded_seq_tensor,0,1), \n",
        "    #         seq_lengths)\n",
        "    #     _, (hidden,_) = self.lstm(seq_packed, None)\n",
        "    #     seq_last_layer = hidden[-1] # N x hidden_embed_dim\n",
        "    #     # print(hidden.shape, seq_last_layer.shape) #torch.Size([32, 512])\n",
        "    #     prim_pred = self.primitive_fc(seq_last_layer) \n",
        "    #     # N x (self.num_normal_param * M * num_transformation_params)\n",
        "    #     prim_param_pred = self.affine_fc(seq_last_layer) \n",
        "    #     # print(prim_pred.shape, prim_param_pred.shape) torch.Size([32, 5]) torch.Size([32, 36])\n",
        "    #     # [N x (num_normal_param * M)]\n",
        "    #     each_prim_param = torch.split(prim_param_pred, self.num_normal_param * self.hp.M, 1) \n",
        "    #     # print([x.shape for x in each_prim_param]) [torch.Size([32, 6])]\n",
        "    #     pi_list = [] # length num_transformation_params\n",
        "    #     mu_list = []\n",
        "    #     sigma_list = []\n",
        "    #     for y in each_prim_param: # N x (num_normal_param * M)\n",
        "    #         params = torch.split(y, self.num_normal_param, 1) # N x self.num_normal_param\n",
        "    #         # print([x.shape for x in params]) # [torch.Size([32, 3])]\n",
        "    #         params_mixture = torch.stack(params) # M x N x self.num_normal_param\n",
        "    #         # print(params_mixture.shape) # torch.Size([2, 32, 3])\n",
        "\n",
        "    #         pi, mu, sigma = torch.split(params_mixture, 1, 2) # M x N x 1\n",
        "    #         pi = F.softmax(pi.transpose(0,1).squeeze(), dim=-1) # N x M\n",
        "    #         mu = mu.transpose(0,1).squeeze().contiguous()\n",
        "    #         sigma = torch.exp(sigma.transpose(0,1).squeeze())\n",
        "            \n",
        "    #         pi_list.append(pi)\n",
        "    #         mu_list.append(mu)\n",
        "    #         sigma_list.append(sigma)\n",
        "        \n",
        "    #     return prim_pred, pi_list, mu_list, sigma_list"
      ],
      "metadata": {
        "id": "kjOPKQtPu_33"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainer"
      ],
      "metadata": {
        "id": "lumsb4ubzddh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer():\n",
        "    def __init__(self, train_dataset, val_dataset, hp, args):\n",
        "        \n",
        "        self.hp = hp\n",
        "        self.args = args \n",
        "        \n",
        "        if args.enable_wandb:\n",
        "            wandb.init(project=args.wandb_project_name, entity=args.wandb_project_entity, config=hp.__dict__)\n",
        "        \n",
        "        self.enable_wandb = args.enable_wandb and not wandb.run is None\n",
        "        if self.enable_wandb:\n",
        "            self.run_name = wandb.run.name \n",
        "        else:\n",
        "            import datetime\n",
        "            import time \n",
        "            ts = time.time()                                                                                            \n",
        "            self.run_name = datetime.datetime.fromtimestamp(ts).strftime('%Y_%m_%d_%H_%M_%S') \n",
        "        \n",
        "        self.save_folder = os.path.join(args.save_root_folder, self.run_name)\n",
        "        if not os.path.exists(self.save_folder):\n",
        "            os.mkdir(self.save_folder)\n",
        "        \n",
        "        self.train_dataset_loader = DataLoader(\n",
        "            train_dataset, \n",
        "            batch_size=hp.batch_size, \n",
        "            shuffle=True, \n",
        "            num_workers=args.num_workers, \n",
        "            collate_fn=collate_primitivedataset)\n",
        "        self.val_dataset_loader = DataLoader(\n",
        "                val_dataset, \n",
        "                batch_size=hp.batch_size, \n",
        "                shuffle=False, \n",
        "                num_workers=args.num_workers, \n",
        "                collate_fn=collate_primitivedataset)\n",
        "\n",
        "        self.device = \"cuda\" # if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = PrimitiveSelector(hp).cuda()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=hp.lr, weight_decay=hp.weight_decay)\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "    \n",
        "    def make_target(self, affine_paramss):\n",
        "        \"\"\"Create ground truth for training transformation parameters by stacking M copies of each parameter\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        affine_paramss : torch.Tensor\n",
        "            (N, num_transformation_params)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        list of torch.Tensor\n",
        "            GT for calculating log likelihood loss, each has shape (N, M)\n",
        "            list of size num_transformation_params\n",
        "        \"\"\"\n",
        "        return [\n",
        "            torch.stack([affine_paramss[:,i]] * self.hp.M, 1) for i in range(affine_paramss.shape[1])\n",
        "        ]\n",
        "    \n",
        "    def normal_pdf(self, x, mu, sigma):\n",
        "        \"\"\"Calculate univariate normal pdf for GMM\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            (N, M)\n",
        "        mu : torch.Tensor\n",
        "            (N, M)\n",
        "            predicted GMM means\n",
        "        sigma : torch.Tensor\n",
        "            (N, M)\n",
        "            predicted GMM standard deviation\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pdf : torch.Tensor\n",
        "            (N, M)\n",
        "            predicted probability\n",
        "        \"\"\"\n",
        "        z = ( (x - mu) / sigma ) ** 2\n",
        "        exp = torch.exp(-z / 2.0)\n",
        "        norm = np.sqrt(2.0 * np.pi) * sigma\n",
        "        pdf = exp / norm\n",
        "        return pdf\n",
        "    \n",
        "    def log_losses(self, params_gt_list, pi_list, mu_list, sigma_list, epoch):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            params_gt_list : list of torch.Tensor (N, M)\n",
        "                a list of GT transformation parameters\n",
        "            pi_list : list of torch.Tensor (N, M)\n",
        "                weights for combining the normal pdf in GMM\n",
        "            mu_list : list of torch.Tensor (N, M)\n",
        "                mean of GMM\n",
        "            sigma_list : list of torch.Tensor (N, M)\n",
        "                standard deviation of GMM\n",
        "        Returns:\n",
        "            losses : list of scalars\n",
        "                each scalar is the log loss across the entire batch for one transformation parameter\n",
        "        \"\"\"\n",
        "        losses = []\n",
        "        for param_idx,(param,pi,mu,sigma) in enumerate(zip(params_gt_list, pi_list, mu_list, sigma_list)):\n",
        "            pdf = self.normal_pdf(param, mu, sigma)\n",
        "            gmm_pdf = torch.sum(pi * pdf, 1)\n",
        "            log_prob = torch.log(1e-5 + gmm_pdf) # (N,)\n",
        "            loss = -torch.sum(log_prob)\n",
        "            losses.append(loss)\n",
        "\n",
        "            if epoch == 1:\n",
        "                #print(f\"{param_idx} pi: \",pi)\n",
        "                print(f\"{param_idx} param: \",param)\n",
        "                print(f\"{param_idx} mu: \",mu)\n",
        "                print(f\"{param_idx} sigma: \",sigma)\n",
        "                print(f\"{param_idx} pdf: \", pdf)\n",
        "                print(f\"{param_idx} gmm: \", gmm_pdf)\n",
        "            \n",
        "        return losses\n",
        "    \n",
        "    def loss(self, y_list, normal_dists, pi_dists, epoch):\n",
        "        losses = []\n",
        "        for param_idx,(y,normal_dist,pi_dist) in enumerate(zip(y_list, normal_dists, pi_dists)):\n",
        "            ys = y.unsqueeze(1).expand_as(normal_dist.loc)\n",
        "            loglik = normal_dist.log_prob(ys)\n",
        "            loglik = torch.sum(loglik, dim=2)\n",
        "            loss = -torch.logsumexp(pi_dist.logits + loglik, dim=1)\n",
        "            losses.append(loss.mean())\n",
        "        return losses\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train()\n",
        "        step = 0\n",
        "        for epoch in range(self.args.start_epoch, self.args.start_epoch + self.args.num_epochs):\n",
        "        \n",
        "            for batch_idx, (description_ts, primitive_types, affine_paramss) in enumerate(self.train_dataset_loader):\n",
        "                description_ts = rnn.pack_sequence(description_ts)\n",
        "                primitive_types = primitive_types.squeeze()\n",
        "                description_ts, primitive_types, affine_paramss = description_ts.to(self.device), primitive_types.to(self.device), affine_paramss.to(self.device)\n",
        "                # params_gt_list = self.make_target(affine_paramss)\n",
        "                params_gt_list = [\n",
        "                    affine_paramss[:,i].view(-1,1) for i in range(affine_paramss.shape[1])\n",
        "                ]\n",
        "\n",
        "                # prim_pred, pi_list, mu_list, sigma_list = self.model(description_ts)\n",
        "                prim_pred, normal_dists, pi_dists = self.model(description_ts)\n",
        "                \n",
        "                self.optimizer.zero_grad()\n",
        "                \n",
        "                cel = self.ce_loss(prim_pred, primitive_types)\n",
        "                # lls = self.log_losses(params_gt_list, pi_list, mu_list, sigma_list, epoch)\n",
        "                lls = self.loss(params_gt_list, normal_dists, pi_dists, epoch)\n",
        "                # print(lls)\n",
        "                total_ll = torch.stack(lls).sum()\n",
        "                total_lls = total_ll + cel\n",
        "                \n",
        "                wandb_dict = {'prim_type_loss' : cel.item(), 'total_param_loss' : total_ll.item()}\n",
        "                for idx, ll in enumerate(lls):\n",
        "                    wandb_dict[f'param_{idx}_loss'] = ll.item()\n",
        "                wandb_dict['total_loss'] = total_lls.item()\n",
        "                \n",
        "                total_lls.backward()\n",
        "                self.optimizer.step()\n",
        "                \n",
        "                if self.enable_wandb:\n",
        "                    wandb.log(wandb_dict, step=step)\n",
        "                else:\n",
        "                    if step % self.hp.print_every == 0:\n",
        "                        print_s = [f\"Epoch {epoch} Iter {step}: \"]\n",
        "                        for k,v in wandb_dict.items():\n",
        "                            print_s.append(f\"{k} : {v}\")\n",
        "                        print(\" | \".join(print_s))\n",
        "                \n",
        "                if step % self.hp.save_every == 0:\n",
        "                    self.save_model(step)\n",
        "                \n",
        "                step += 1\n",
        "            #     if epoch == 1:\n",
        "            #         break\n",
        "            # if epoch == 1:\n",
        "            #     break\n",
        "\n",
        "    # def evaluate(self):\n",
        "    #     model.eval()\n",
        "    #     with torch.no_grad(): \n",
        "    #         for batch_idx, (description_ts, primitive_types, affine_paramss) in enumerate(self.val_dataset_loader): \n",
        "    #             description_ts, primitive_types, affine_paramss = description_ts.to(self.device), \\\n",
        "    #                     primitive_types.to(self.device), affine_paramss.to(self.device)                \n",
        "    #             prim_pred, pi_list, mu_list, sigma_list = self.model(description_ts)         \n",
        "                \n",
        "    def save_model(self, step):\n",
        "        \n",
        "        torch_path_name = os.path.join(self.save_folder, f\"{step}.pt\")\n",
        "\n",
        "        torch.save({\n",
        "            'iteration' : step,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "        }, torch_path_name)\n"
      ],
      "metadata": {
        "id": "D-Kg-xJNuyEZ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### main"
      ],
      "metadata": {
        "id": "LGUNZHQazZQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HParams():\n",
        "    def __init__(self):\n",
        "        self.word_embed_dim = 128\n",
        "        self.lstm_output_dim = 512\n",
        "        self.lstm_layers = 2 \n",
        "        self.lstm_drop_prob = 0.4\n",
        "        self.num_primitives = 5\n",
        "        self.num_transformation_params = 2\n",
        "        self.vocab_size = None\n",
        "        self.M = 2\n",
        "        self.weight_decay = 0.0\n",
        "        self.batch_size = 32\n",
        "        self.lr = 0.001\n",
        "        \n",
        "        self.save_every = 500\n",
        "        self.print_every = 100\n",
        "\n",
        "class CommandParams():\n",
        "    def __init__(self):\n",
        "        self.enable_wandb = False \n",
        "        self.start_epoch = 0\n",
        "        self.num_epochs = 50\n",
        "        self.num_workers = 0\n",
        "        self.wandb_project_name = \"doodler-draw\"\n",
        "        self.wandb_project_entity = \"erinz\"\n",
        "        self.save_root_folder = \"/content/gdrive/MyDrive/doodler_model_checkpoint\"\n",
        "        self.train_file = \"/content/gdrive/MyDrive/primitive_selector_training_data/july_13_train.pkl\"\n",
        "        self.val_file = \"/content/gdrive/MyDrive/primitive_selector_training_data/july_13_val.pkl\"\n",
        "        self.test_file = \"/content/gdrive/MyDrive/primitive_selector_training_data/july_13_test.pkl\"\n",
        "        # self.word_file = \"/content/gdrive/MyDrive/primitive_selector_training_data\"\n",
        "\n",
        "def get_args():\n",
        "    args = CommandParams()\n",
        "    return args"
      ],
      "metadata": {
        "id": "M4ZKKBcj1asm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = get_args()\n",
        "hp = HParams()"
      ],
      "metadata": {
        "id": "bL_m9LnDuyKL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = preprocess_dataset_language(args.train_file)"
      ],
      "metadata": {
        "id": "-ewibXPeuyNY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hp.vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "lBNg0j60vwz9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = PrimitiveDataset(args.train_file, vocab, hp.num_transformation_params)\n",
        "val_dataset = PrimitiveDataset(args.test_file, vocab, hp.num_transformation_params)"
      ],
      "metadata": {
        "id": "xLLA1wAZyPfB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(train_dataset, val_dataset, hp, args)"
      ],
      "metadata": {
        "id": "9BepBlPzyPkF"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "2TuiJ877yPm6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a190be9-6313-4c69-9986-5352bfd5115a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Iter 0:  | prim_type_loss : 1.6124851703643799 | total_param_loss : 12546.390625 | param_0_loss : 6198.1083984375 | param_1_loss : 6348.28173828125 | total_loss : 12548.0029296875\n",
            "Epoch 0 Iter 100:  | prim_type_loss : 1.4718098640441895 | total_param_loss : 47.326602935791016 | param_0_loss : 22.864288330078125 | param_1_loss : 24.46231460571289 | total_loss : 48.79841232299805\n",
            "Epoch 0 Iter 200:  | prim_type_loss : 1.3185399770736694 | total_param_loss : 28.102401733398438 | param_0_loss : 13.026363372802734 | param_1_loss : 15.076038360595703 | total_loss : 29.420942306518555\n",
            "Epoch 1 Iter 300:  | prim_type_loss : 1.4346470832824707 | total_param_loss : 17.4107608795166 | param_0_loss : 9.459742546081543 | param_1_loss : 7.951018810272217 | total_loss : 18.845407485961914\n",
            "Epoch 1 Iter 400:  | prim_type_loss : 1.1037076711654663 | total_param_loss : 13.532354354858398 | param_0_loss : 6.784598350524902 | param_1_loss : 6.747755527496338 | total_loss : 14.636061668395996\n",
            "Epoch 1 Iter 500:  | prim_type_loss : 1.2469696998596191 | total_param_loss : 11.352546691894531 | param_0_loss : 5.658577919006348 | param_1_loss : 5.693968296051025 | total_loss : 12.599515914916992\n",
            "Epoch 2 Iter 600:  | prim_type_loss : 1.3131062984466553 | total_param_loss : 11.135910034179688 | param_0_loss : 5.440601348876953 | param_1_loss : 5.695308208465576 | total_loss : 12.449016571044922\n",
            "Epoch 2 Iter 700:  | prim_type_loss : 1.140717625617981 | total_param_loss : 10.505844116210938 | param_0_loss : 4.849597454071045 | param_1_loss : 5.656247138977051 | total_loss : 11.646561622619629\n",
            "Epoch 2 Iter 800:  | prim_type_loss : 1.0012832880020142 | total_param_loss : 10.055255889892578 | param_0_loss : 4.868237018585205 | param_1_loss : 5.187018394470215 | total_loss : 11.056539535522461\n",
            "Epoch 3 Iter 900:  | prim_type_loss : 1.1087708473205566 | total_param_loss : 10.477314949035645 | param_0_loss : 4.636326789855957 | param_1_loss : 5.8409881591796875 | total_loss : 11.58608627319336\n",
            "Epoch 3 Iter 1000:  | prim_type_loss : 1.0294408798217773 | total_param_loss : 10.024232864379883 | param_0_loss : 4.229452133178711 | param_1_loss : 5.79478120803833 | total_loss : 11.05367374420166\n",
            "Epoch 3 Iter 1100:  | prim_type_loss : 0.8618810772895813 | total_param_loss : 10.84959602355957 | param_0_loss : 4.465709686279297 | param_1_loss : 6.383886337280273 | total_loss : 11.711477279663086\n",
            "Epoch 4 Iter 1200:  | prim_type_loss : 1.3982231616973877 | total_param_loss : 10.061397552490234 | param_0_loss : 4.385623455047607 | param_1_loss : 5.675773620605469 | total_loss : 11.459620475769043\n",
            "Epoch 4 Iter 1300:  | prim_type_loss : 1.071599006652832 | total_param_loss : 9.986915588378906 | param_0_loss : 4.539431571960449 | param_1_loss : 5.447483539581299 | total_loss : 11.058514595031738\n",
            "Epoch 5 Iter 1400:  | prim_type_loss : 0.9401845932006836 | total_param_loss : 9.926342010498047 | param_0_loss : 4.397296905517578 | param_1_loss : 5.529045104980469 | total_loss : 10.86652660369873\n",
            "Epoch 5 Iter 1500:  | prim_type_loss : 0.9644370675086975 | total_param_loss : 9.453413009643555 | param_0_loss : 4.404489994049072 | param_1_loss : 5.048922538757324 | total_loss : 10.417850494384766\n",
            "Epoch 5 Iter 1600:  | prim_type_loss : 1.033756971359253 | total_param_loss : 9.908109664916992 | param_0_loss : 4.587647914886475 | param_1_loss : 5.320461273193359 | total_loss : 10.941866874694824\n",
            "Epoch 6 Iter 1700:  | prim_type_loss : 0.9439254999160767 | total_param_loss : 9.756306648254395 | param_0_loss : 4.366643905639648 | param_1_loss : 5.389662742614746 | total_loss : 10.70023250579834\n",
            "Epoch 6 Iter 1800:  | prim_type_loss : 0.8328115344047546 | total_param_loss : 9.457540512084961 | param_0_loss : 4.332859039306641 | param_1_loss : 5.12468147277832 | total_loss : 10.290351867675781\n",
            "Epoch 6 Iter 1900:  | prim_type_loss : 0.9953881502151489 | total_param_loss : 9.958915710449219 | param_0_loss : 4.544503688812256 | param_1_loss : 5.414412498474121 | total_loss : 10.954303741455078\n",
            "Epoch 7 Iter 2000:  | prim_type_loss : 0.742708146572113 | total_param_loss : 9.775199890136719 | param_0_loss : 4.425593852996826 | param_1_loss : 5.349606513977051 | total_loss : 10.517908096313477\n",
            "Epoch 7 Iter 2100:  | prim_type_loss : 0.8989887833595276 | total_param_loss : 9.947345733642578 | param_0_loss : 4.503022193908691 | param_1_loss : 5.444323539733887 | total_loss : 10.846334457397461\n",
            "Epoch 7 Iter 2200:  | prim_type_loss : 1.4323312044143677 | total_param_loss : 10.141887664794922 | param_0_loss : 4.441930770874023 | param_1_loss : 5.699957370758057 | total_loss : 11.57421875\n",
            "Epoch 8 Iter 2300:  | prim_type_loss : 0.9306492209434509 | total_param_loss : 9.757184982299805 | param_0_loss : 4.3262810707092285 | param_1_loss : 5.430903434753418 | total_loss : 10.687833786010742\n",
            "Epoch 8 Iter 2400:  | prim_type_loss : 1.0545499324798584 | total_param_loss : 9.522216796875 | param_0_loss : 4.406630992889404 | param_1_loss : 5.1155853271484375 | total_loss : 10.576766967773438\n",
            "Epoch 8 Iter 2500:  | prim_type_loss : 0.8934842348098755 | total_param_loss : 9.528203964233398 | param_0_loss : 4.250109672546387 | param_1_loss : 5.27809476852417 | total_loss : 10.421688079833984\n",
            "Epoch 9 Iter 2600:  | prim_type_loss : 0.9064341187477112 | total_param_loss : 9.692188262939453 | param_0_loss : 4.186452865600586 | param_1_loss : 5.505735874176025 | total_loss : 10.59862232208252\n",
            "Epoch 9 Iter 2700:  | prim_type_loss : 0.7952801585197449 | total_param_loss : 10.061590194702148 | param_0_loss : 4.4195756912231445 | param_1_loss : 5.642014980316162 | total_loss : 10.856870651245117\n",
            "Epoch 10 Iter 2800:  | prim_type_loss : 0.8389448523521423 | total_param_loss : 9.833236694335938 | param_0_loss : 4.40987491607666 | param_1_loss : 5.423361778259277 | total_loss : 10.672181129455566\n",
            "Epoch 10 Iter 2900:  | prim_type_loss : 0.9314424991607666 | total_param_loss : 9.376626968383789 | param_0_loss : 4.220449447631836 | param_1_loss : 5.156177043914795 | total_loss : 10.308069229125977\n",
            "Epoch 10 Iter 3000:  | prim_type_loss : 1.017612338066101 | total_param_loss : 9.703448295593262 | param_0_loss : 4.276741027832031 | param_1_loss : 5.4267072677612305 | total_loss : 10.721060752868652\n",
            "Epoch 11 Iter 3100:  | prim_type_loss : 0.9394662380218506 | total_param_loss : 9.704986572265625 | param_0_loss : 4.279987335205078 | param_1_loss : 5.424998760223389 | total_loss : 10.644453048706055\n",
            "Epoch 11 Iter 3200:  | prim_type_loss : 0.9367787837982178 | total_param_loss : 9.728988647460938 | param_0_loss : 4.331546306610107 | param_1_loss : 5.397441864013672 | total_loss : 10.665767669677734\n",
            "Epoch 11 Iter 3300:  | prim_type_loss : 0.8506249189376831 | total_param_loss : 9.675520896911621 | param_0_loss : 4.296046257019043 | param_1_loss : 5.379474639892578 | total_loss : 10.526145935058594\n",
            "Epoch 12 Iter 3400:  | prim_type_loss : 0.848908543586731 | total_param_loss : 9.428285598754883 | param_0_loss : 4.348916053771973 | param_1_loss : 5.07936954498291 | total_loss : 10.277194023132324\n",
            "Epoch 12 Iter 3500:  | prim_type_loss : 0.9884750247001648 | total_param_loss : 9.32823657989502 | param_0_loss : 4.298494338989258 | param_1_loss : 5.029742240905762 | total_loss : 10.31671142578125\n",
            "Epoch 12 Iter 3600:  | prim_type_loss : 1.2589017152786255 | total_param_loss : 9.7138032913208 | param_0_loss : 4.567233562469482 | param_1_loss : 5.146569728851318 | total_loss : 10.972704887390137\n",
            "Epoch 13 Iter 3700:  | prim_type_loss : 0.7873833775520325 | total_param_loss : 9.558456420898438 | param_0_loss : 4.288007736206055 | param_1_loss : 5.270448684692383 | total_loss : 10.345839500427246\n",
            "Epoch 13 Iter 3800:  | prim_type_loss : 0.8908330202102661 | total_param_loss : 9.285382270812988 | param_0_loss : 4.261285781860352 | param_1_loss : 5.024096488952637 | total_loss : 10.176215171813965\n",
            "Epoch 13 Iter 3900:  | prim_type_loss : 0.9414607286453247 | total_param_loss : 9.672433853149414 | param_0_loss : 4.343266487121582 | param_1_loss : 5.329167366027832 | total_loss : 10.61389446258545\n",
            "Epoch 14 Iter 4000:  | prim_type_loss : 0.8204744458198547 | total_param_loss : 9.593147277832031 | param_0_loss : 4.3298821449279785 | param_1_loss : 5.263265132904053 | total_loss : 10.41362190246582\n",
            "Epoch 14 Iter 4100:  | prim_type_loss : 0.9896320104598999 | total_param_loss : 9.126455307006836 | param_0_loss : 4.071470260620117 | param_1_loss : 5.054985046386719 | total_loss : 10.116086959838867\n",
            "Epoch 15 Iter 4200:  | prim_type_loss : 0.9257500767707825 | total_param_loss : 9.542996406555176 | param_0_loss : 4.5433149337768555 | param_1_loss : 4.99968147277832 | total_loss : 10.468746185302734\n",
            "Epoch 15 Iter 4300:  | prim_type_loss : 0.791703999042511 | total_param_loss : 9.339363098144531 | param_0_loss : 4.311423301696777 | param_1_loss : 5.027939796447754 | total_loss : 10.131067276000977\n",
            "Epoch 15 Iter 4400:  | prim_type_loss : 0.8570039868354797 | total_param_loss : 9.234844207763672 | param_0_loss : 4.2679924964904785 | param_1_loss : 4.966851234436035 | total_loss : 10.091848373413086\n",
            "Epoch 16 Iter 4500:  | prim_type_loss : 0.7292700409889221 | total_param_loss : 9.3662109375 | param_0_loss : 4.1772871017456055 | param_1_loss : 5.1889238357543945 | total_loss : 10.095480918884277\n",
            "Epoch 16 Iter 4600:  | prim_type_loss : 0.896680474281311 | total_param_loss : 9.570433616638184 | param_0_loss : 4.203496932983398 | param_1_loss : 5.366936683654785 | total_loss : 10.467114448547363\n",
            "Epoch 16 Iter 4700:  | prim_type_loss : 1.0117676258087158 | total_param_loss : 9.359820365905762 | param_0_loss : 4.139992713928223 | param_1_loss : 5.219827651977539 | total_loss : 10.371587753295898\n",
            "Epoch 17 Iter 4800:  | prim_type_loss : 0.970205545425415 | total_param_loss : 9.265016555786133 | param_0_loss : 4.204771041870117 | param_1_loss : 5.060245513916016 | total_loss : 10.235221862792969\n",
            "Epoch 17 Iter 4900:  | prim_type_loss : 0.8701294660568237 | total_param_loss : 9.489840507507324 | param_0_loss : 4.377978324890137 | param_1_loss : 5.1118621826171875 | total_loss : 10.359970092773438\n",
            "Epoch 17 Iter 5000:  | prim_type_loss : 0.8416798710823059 | total_param_loss : 9.374724388122559 | param_0_loss : 4.235170364379883 | param_1_loss : 5.139554023742676 | total_loss : 10.21640396118164\n",
            "Epoch 18 Iter 5100:  | prim_type_loss : 0.8857267498970032 | total_param_loss : 9.13823127746582 | param_0_loss : 4.087533473968506 | param_1_loss : 5.050698280334473 | total_loss : 10.023958206176758\n",
            "Epoch 18 Iter 5200:  | prim_type_loss : 0.8807711005210876 | total_param_loss : 9.48281478881836 | param_0_loss : 4.4132232666015625 | param_1_loss : 5.069591045379639 | total_loss : 10.363585472106934\n",
            "Epoch 18 Iter 5300:  | prim_type_loss : 1.0413974523544312 | total_param_loss : 9.440984725952148 | param_0_loss : 4.280697822570801 | param_1_loss : 5.160287380218506 | total_loss : 10.482381820678711\n",
            "Epoch 19 Iter 5400:  | prim_type_loss : 1.0750679969787598 | total_param_loss : 9.46861743927002 | param_0_loss : 4.399538993835449 | param_1_loss : 5.06907844543457 | total_loss : 10.543685913085938\n",
            "Epoch 19 Iter 5500:  | prim_type_loss : 1.1230581998825073 | total_param_loss : 9.196571350097656 | param_0_loss : 4.150169372558594 | param_1_loss : 5.0464019775390625 | total_loss : 10.319629669189453\n",
            "Epoch 20 Iter 5600:  | prim_type_loss : 1.0180039405822754 | total_param_loss : 9.358293533325195 | param_0_loss : 4.182559013366699 | param_1_loss : 5.175734519958496 | total_loss : 10.376296997070312\n",
            "Epoch 20 Iter 5700:  | prim_type_loss : 1.0780335664749146 | total_param_loss : 9.34799575805664 | param_0_loss : 4.29270076751709 | param_1_loss : 5.055294990539551 | total_loss : 10.426029205322266\n",
            "Epoch 20 Iter 5800:  | prim_type_loss : 0.6664333343505859 | total_param_loss : 9.158714294433594 | param_0_loss : 4.2997918128967285 | param_1_loss : 4.858922004699707 | total_loss : 9.82514762878418\n",
            "Epoch 21 Iter 5900:  | prim_type_loss : 1.052431583404541 | total_param_loss : 9.147212028503418 | param_0_loss : 4.078351974487305 | param_1_loss : 5.068860054016113 | total_loss : 10.199644088745117\n",
            "Epoch 21 Iter 6000:  | prim_type_loss : 1.0147980451583862 | total_param_loss : 9.621221542358398 | param_0_loss : 4.480884552001953 | param_1_loss : 5.140336513519287 | total_loss : 10.636019706726074\n",
            "Epoch 21 Iter 6100:  | prim_type_loss : 1.452713966369629 | total_param_loss : 9.280423164367676 | param_0_loss : 4.244682312011719 | param_1_loss : 5.035740852355957 | total_loss : 10.733137130737305\n",
            "Epoch 22 Iter 6200:  | prim_type_loss : 0.8342418670654297 | total_param_loss : 9.231952667236328 | param_0_loss : 4.249425888061523 | param_1_loss : 4.982527256011963 | total_loss : 10.066194534301758\n",
            "Epoch 22 Iter 6300:  | prim_type_loss : 0.889573872089386 | total_param_loss : 9.412252426147461 | param_0_loss : 4.506255149841309 | param_1_loss : 4.905997276306152 | total_loss : 10.301826477050781\n",
            "Epoch 22 Iter 6400:  | prim_type_loss : 0.9786208271980286 | total_param_loss : 9.734006881713867 | param_0_loss : 4.619970321655273 | param_1_loss : 5.114036560058594 | total_loss : 10.712627410888672\n",
            "Epoch 23 Iter 6500:  | prim_type_loss : 1.2233750820159912 | total_param_loss : 9.497451782226562 | param_0_loss : 4.420902729034424 | param_1_loss : 5.0765485763549805 | total_loss : 10.720827102661133\n",
            "Epoch 23 Iter 6600:  | prim_type_loss : 0.8675753474235535 | total_param_loss : 9.569931983947754 | param_0_loss : 4.479994773864746 | param_1_loss : 5.089937210083008 | total_loss : 10.437507629394531\n",
            "Epoch 24 Iter 6700:  | prim_type_loss : 0.996635913848877 | total_param_loss : 9.311159133911133 | param_0_loss : 4.363994121551514 | param_1_loss : 4.947165489196777 | total_loss : 10.307794570922852\n",
            "Epoch 24 Iter 6800:  | prim_type_loss : 0.6377764940261841 | total_param_loss : 9.239119529724121 | param_0_loss : 4.229357719421387 | param_1_loss : 5.009761810302734 | total_loss : 9.876895904541016\n",
            "Epoch 24 Iter 6900:  | prim_type_loss : 0.7042360305786133 | total_param_loss : 9.204809188842773 | param_0_loss : 4.298490047454834 | param_1_loss : 4.906318664550781 | total_loss : 9.909045219421387\n",
            "Epoch 25 Iter 7000:  | prim_type_loss : 1.317412257194519 | total_param_loss : 9.349664688110352 | param_0_loss : 4.407478332519531 | param_1_loss : 4.94218635559082 | total_loss : 10.66707706451416\n",
            "Epoch 25 Iter 7100:  | prim_type_loss : 0.8384802341461182 | total_param_loss : 8.997954368591309 | param_0_loss : 4.206521987915039 | param_1_loss : 4.7914323806762695 | total_loss : 9.836434364318848\n",
            "Epoch 25 Iter 7200:  | prim_type_loss : 0.8188358545303345 | total_param_loss : 9.318711280822754 | param_0_loss : 4.467386245727539 | param_1_loss : 4.851325035095215 | total_loss : 10.137547492980957\n",
            "Epoch 26 Iter 7300:  | prim_type_loss : 1.0847856998443604 | total_param_loss : 9.435174942016602 | param_0_loss : 4.340572834014893 | param_1_loss : 5.094601631164551 | total_loss : 10.519960403442383\n",
            "Epoch 26 Iter 7400:  | prim_type_loss : 0.6577671766281128 | total_param_loss : 9.358034133911133 | param_0_loss : 4.334506988525391 | param_1_loss : 5.023527145385742 | total_loss : 10.015801429748535\n",
            "Epoch 26 Iter 7500:  | prim_type_loss : 1.158087134361267 | total_param_loss : 9.080491065979004 | param_0_loss : 4.030722618103027 | param_1_loss : 5.049768447875977 | total_loss : 10.238577842712402\n",
            "Epoch 27 Iter 7600:  | prim_type_loss : 0.8738844394683838 | total_param_loss : 9.18978214263916 | param_0_loss : 4.1651105880737305 | param_1_loss : 5.02467155456543 | total_loss : 10.063666343688965\n",
            "Epoch 27 Iter 7700:  | prim_type_loss : 0.7350403666496277 | total_param_loss : 9.069952964782715 | param_0_loss : 4.05153751373291 | param_1_loss : 5.018415451049805 | total_loss : 9.804993629455566\n",
            "Epoch 27 Iter 7800:  | prim_type_loss : 0.8561371564865112 | total_param_loss : 9.131086349487305 | param_0_loss : 4.104400634765625 | param_1_loss : 5.0266852378845215 | total_loss : 9.987223625183105\n",
            "Epoch 28 Iter 7900:  | prim_type_loss : 0.6230919361114502 | total_param_loss : 9.271944046020508 | param_0_loss : 4.419436454772949 | param_1_loss : 4.852507591247559 | total_loss : 9.895035743713379\n",
            "Epoch 28 Iter 8000:  | prim_type_loss : 0.841392457485199 | total_param_loss : 9.094724655151367 | param_0_loss : 4.2637128829956055 | param_1_loss : 4.831011772155762 | total_loss : 9.936117172241211\n",
            "Epoch 29 Iter 8100:  | prim_type_loss : 1.192829966545105 | total_param_loss : 9.128498077392578 | param_0_loss : 4.213502883911133 | param_1_loss : 4.914994716644287 | total_loss : 10.321328163146973\n",
            "Epoch 29 Iter 8200:  | prim_type_loss : 0.7482537627220154 | total_param_loss : 9.078998565673828 | param_0_loss : 4.254889011383057 | param_1_loss : 4.8241095542907715 | total_loss : 9.827252388000488\n",
            "Epoch 29 Iter 8300:  | prim_type_loss : 0.7157866358757019 | total_param_loss : 9.261969566345215 | param_0_loss : 4.229059219360352 | param_1_loss : 5.032910346984863 | total_loss : 9.97775650024414\n",
            "Epoch 30 Iter 8400:  | prim_type_loss : 0.730309247970581 | total_param_loss : 8.989609718322754 | param_0_loss : 4.0548200607299805 | param_1_loss : 4.934789657592773 | total_loss : 9.719919204711914\n",
            "Epoch 30 Iter 8500:  | prim_type_loss : 1.0299782752990723 | total_param_loss : 9.136580467224121 | param_0_loss : 4.298914909362793 | param_1_loss : 4.837665557861328 | total_loss : 10.166559219360352\n",
            "Epoch 30 Iter 8600:  | prim_type_loss : 0.7913596034049988 | total_param_loss : 8.78246784210205 | param_0_loss : 4.003444194793701 | param_1_loss : 4.77902364730835 | total_loss : 9.573827743530273\n",
            "Epoch 31 Iter 8700:  | prim_type_loss : 1.1295113563537598 | total_param_loss : 8.873167037963867 | param_0_loss : 4.201044082641602 | param_1_loss : 4.672122955322266 | total_loss : 10.002677917480469\n",
            "Epoch 31 Iter 8800:  | prim_type_loss : 0.9552148580551147 | total_param_loss : 8.67270278930664 | param_0_loss : 4.052945137023926 | param_1_loss : 4.619757652282715 | total_loss : 9.627917289733887\n",
            "Epoch 31 Iter 8900:  | prim_type_loss : 0.6144654154777527 | total_param_loss : 9.094295501708984 | param_0_loss : 4.377956390380859 | param_1_loss : 4.716338634490967 | total_loss : 9.708761215209961\n",
            "Epoch 32 Iter 9000:  | prim_type_loss : 0.6251667737960815 | total_param_loss : 8.79372787475586 | param_0_loss : 4.250974178314209 | param_1_loss : 4.542754173278809 | total_loss : 9.41889476776123\n",
            "Epoch 32 Iter 9100:  | prim_type_loss : 0.7339650392532349 | total_param_loss : 8.50271224975586 | param_0_loss : 3.9495139122009277 | param_1_loss : 4.553198337554932 | total_loss : 9.236677169799805\n",
            "Epoch 32 Iter 9200:  | prim_type_loss : 1.0695334672927856 | total_param_loss : 8.79458999633789 | param_0_loss : 4.188085556030273 | param_1_loss : 4.606504440307617 | total_loss : 9.864123344421387\n",
            "Epoch 33 Iter 9300:  | prim_type_loss : 0.8802234530448914 | total_param_loss : 8.903085708618164 | param_0_loss : 4.400341033935547 | param_1_loss : 4.502744674682617 | total_loss : 9.783308982849121\n",
            "Epoch 33 Iter 9400:  | prim_type_loss : 0.9293025135993958 | total_param_loss : 8.881662368774414 | param_0_loss : 4.458255767822266 | param_1_loss : 4.423407077789307 | total_loss : 9.810964584350586\n",
            "Epoch 34 Iter 9500:  | prim_type_loss : 0.6821345090866089 | total_param_loss : 8.738327026367188 | param_0_loss : 4.191859245300293 | param_1_loss : 4.5464677810668945 | total_loss : 9.420461654663086\n",
            "Epoch 34 Iter 9600:  | prim_type_loss : 0.600348949432373 | total_param_loss : 8.936351776123047 | param_0_loss : 4.356972694396973 | param_1_loss : 4.579379081726074 | total_loss : 9.536701202392578\n",
            "Epoch 34 Iter 9700:  | prim_type_loss : 0.6905773282051086 | total_param_loss : 8.599588394165039 | param_0_loss : 4.137975215911865 | param_1_loss : 4.461612701416016 | total_loss : 9.290165901184082\n",
            "Epoch 35 Iter 9800:  | prim_type_loss : 1.048571228981018 | total_param_loss : 8.971421241760254 | param_0_loss : 4.226361274719238 | param_1_loss : 4.745059967041016 | total_loss : 10.01999282836914\n",
            "Epoch 35 Iter 9900:  | prim_type_loss : 0.6709503531455994 | total_param_loss : 8.563959121704102 | param_0_loss : 4.162079334259033 | param_1_loss : 4.401879787445068 | total_loss : 9.234909057617188\n",
            "Epoch 35 Iter 10000:  | prim_type_loss : 1.0297198295593262 | total_param_loss : 8.910887718200684 | param_0_loss : 4.243040084838867 | param_1_loss : 4.667847633361816 | total_loss : 9.940607070922852\n",
            "Epoch 36 Iter 10100:  | prim_type_loss : 0.6598559617996216 | total_param_loss : 8.500112533569336 | param_0_loss : 4.044030666351318 | param_1_loss : 4.456081390380859 | total_loss : 9.159968376159668\n",
            "Epoch 36 Iter 10200:  | prim_type_loss : 0.6114405393600464 | total_param_loss : 8.468939781188965 | param_0_loss : 4.202725887298584 | param_1_loss : 4.266213893890381 | total_loss : 9.0803804397583\n",
            "Epoch 36 Iter 10300:  | prim_type_loss : 0.7923111915588379 | total_param_loss : 8.308281898498535 | param_0_loss : 4.014467239379883 | param_1_loss : 4.293814659118652 | total_loss : 9.100593566894531\n",
            "Epoch 37 Iter 10400:  | prim_type_loss : 0.7468670010566711 | total_param_loss : 8.82339859008789 | param_0_loss : 4.24582576751709 | param_1_loss : 4.577572822570801 | total_loss : 9.570265769958496\n",
            "Epoch 37 Iter 10500:  | prim_type_loss : 0.36874231696128845 | total_param_loss : 9.042232513427734 | param_0_loss : 4.41541862487793 | param_1_loss : 4.626813888549805 | total_loss : 9.410974502563477\n",
            "Epoch 37 Iter 10600:  | prim_type_loss : 0.4163946509361267 | total_param_loss : 8.770872116088867 | param_0_loss : 4.105754852294922 | param_1_loss : 4.665116786956787 | total_loss : 9.18726634979248\n",
            "Epoch 38 Iter 10700:  | prim_type_loss : 0.6294739842414856 | total_param_loss : 8.291951179504395 | param_0_loss : 4.096334457397461 | param_1_loss : 4.195616722106934 | total_loss : 8.921424865722656\n",
            "Epoch 38 Iter 10800:  | prim_type_loss : 0.6421705484390259 | total_param_loss : 8.47779655456543 | param_0_loss : 4.1066718101501465 | param_1_loss : 4.371125221252441 | total_loss : 9.119967460632324\n",
            "Epoch 39 Iter 10900:  | prim_type_loss : 0.8025141358375549 | total_param_loss : 8.816719055175781 | param_0_loss : 4.327831745147705 | param_1_loss : 4.488886833190918 | total_loss : 9.619233131408691\n",
            "Epoch 39 Iter 11000:  | prim_type_loss : 0.8075854182243347 | total_param_loss : 8.38045597076416 | param_0_loss : 3.983593463897705 | param_1_loss : 4.396862506866455 | total_loss : 9.188041687011719\n",
            "Epoch 39 Iter 11100:  | prim_type_loss : 0.8340038061141968 | total_param_loss : 8.725077629089355 | param_0_loss : 4.249520778656006 | param_1_loss : 4.47555685043335 | total_loss : 9.559081077575684\n",
            "Epoch 40 Iter 11200:  | prim_type_loss : 0.48714473843574524 | total_param_loss : 8.711088180541992 | param_0_loss : 4.625508785247803 | param_1_loss : 4.0855793952941895 | total_loss : 9.198232650756836\n",
            "Epoch 40 Iter 11300:  | prim_type_loss : 0.6240605115890503 | total_param_loss : 8.377653121948242 | param_0_loss : 4.206071853637695 | param_1_loss : 4.171581745147705 | total_loss : 9.001713752746582\n",
            "Epoch 40 Iter 11400:  | prim_type_loss : 0.4785335063934326 | total_param_loss : 8.517404556274414 | param_0_loss : 4.106743335723877 | param_1_loss : 4.410661220550537 | total_loss : 8.995938301086426\n",
            "Epoch 41 Iter 11500:  | prim_type_loss : 0.6503791809082031 | total_param_loss : 8.374603271484375 | param_0_loss : 4.042605400085449 | param_1_loss : 4.331997871398926 | total_loss : 9.024982452392578\n",
            "Epoch 41 Iter 11600:  | prim_type_loss : 0.6362404823303223 | total_param_loss : 8.500325202941895 | param_0_loss : 4.194608688354492 | param_1_loss : 4.305716514587402 | total_loss : 9.136566162109375\n",
            "Epoch 41 Iter 11700:  | prim_type_loss : 0.5058494806289673 | total_param_loss : 8.618417739868164 | param_0_loss : 4.1635942459106445 | param_1_loss : 4.454823970794678 | total_loss : 9.124267578125\n",
            "Epoch 42 Iter 11800:  | prim_type_loss : 0.6793636083602905 | total_param_loss : 9.141494750976562 | param_0_loss : 4.485142230987549 | param_1_loss : 4.656352519989014 | total_loss : 9.820858001708984\n",
            "Epoch 42 Iter 11900:  | prim_type_loss : 0.49946334958076477 | total_param_loss : 8.324241638183594 | param_0_loss : 4.144474029541016 | param_1_loss : 4.179767608642578 | total_loss : 8.823704719543457\n",
            "Epoch 43 Iter 12000:  | prim_type_loss : 0.5206193327903748 | total_param_loss : 8.372628211975098 | param_0_loss : 4.314939022064209 | param_1_loss : 4.057689189910889 | total_loss : 8.893247604370117\n",
            "Epoch 43 Iter 12100:  | prim_type_loss : 0.531730592250824 | total_param_loss : 8.65235424041748 | param_0_loss : 4.3134260177612305 | param_1_loss : 4.33892822265625 | total_loss : 9.18408489227295\n",
            "Epoch 43 Iter 12200:  | prim_type_loss : 0.8357988595962524 | total_param_loss : 8.454720497131348 | param_0_loss : 4.220147132873535 | param_1_loss : 4.2345733642578125 | total_loss : 9.290519714355469\n",
            "Epoch 44 Iter 12300:  | prim_type_loss : 0.48412778973579407 | total_param_loss : 8.158580780029297 | param_0_loss : 3.9201550483703613 | param_1_loss : 4.238426208496094 | total_loss : 8.642708778381348\n",
            "Epoch 44 Iter 12400:  | prim_type_loss : 0.6616078019142151 | total_param_loss : 8.508914947509766 | param_0_loss : 4.095300197601318 | param_1_loss : 4.413614273071289 | total_loss : 9.170522689819336\n",
            "Epoch 44 Iter 12500:  | prim_type_loss : 0.567895770072937 | total_param_loss : 8.431238174438477 | param_0_loss : 4.192281246185303 | param_1_loss : 4.238956928253174 | total_loss : 8.999134063720703\n",
            "Epoch 45 Iter 12600:  | prim_type_loss : 0.5697565674781799 | total_param_loss : 8.285198211669922 | param_0_loss : 4.077626705169678 | param_1_loss : 4.207571029663086 | total_loss : 8.854954719543457\n",
            "Epoch 45 Iter 12700:  | prim_type_loss : 0.3723878562450409 | total_param_loss : 8.189050674438477 | param_0_loss : 4.207289695739746 | param_1_loss : 3.9817607402801514 | total_loss : 8.56143856048584\n",
            "Epoch 45 Iter 12800:  | prim_type_loss : 0.5528112053871155 | total_param_loss : 8.531657218933105 | param_0_loss : 4.445819854736328 | param_1_loss : 4.085837364196777 | total_loss : 9.084468841552734\n",
            "Epoch 46 Iter 12900:  | prim_type_loss : 0.4569002389907837 | total_param_loss : 8.361577987670898 | param_0_loss : 4.213301181793213 | param_1_loss : 4.148277282714844 | total_loss : 8.81847858428955\n",
            "Epoch 46 Iter 13000:  | prim_type_loss : 0.6230372190475464 | total_param_loss : 8.557524681091309 | param_0_loss : 4.210015296936035 | param_1_loss : 4.347509384155273 | total_loss : 9.180562019348145\n",
            "Epoch 46 Iter 13100:  | prim_type_loss : 0.6810942888259888 | total_param_loss : 8.254878044128418 | param_0_loss : 4.114053249359131 | param_1_loss : 4.140824794769287 | total_loss : 8.935972213745117\n",
            "Epoch 47 Iter 13200:  | prim_type_loss : 0.47890666127204895 | total_param_loss : 8.386602401733398 | param_0_loss : 4.176822185516357 | param_1_loss : 4.209780216217041 | total_loss : 8.865509033203125\n",
            "Epoch 47 Iter 13300:  | prim_type_loss : 0.7087559700012207 | total_param_loss : 8.113958358764648 | param_0_loss : 4.058069705963135 | param_1_loss : 4.055889129638672 | total_loss : 8.822713851928711\n",
            "Epoch 48 Iter 13400:  | prim_type_loss : 0.371604859828949 | total_param_loss : 8.068345069885254 | param_0_loss : 4.056422710418701 | param_1_loss : 4.011922359466553 | total_loss : 8.439949989318848\n",
            "Epoch 48 Iter 13500:  | prim_type_loss : 0.5609232187271118 | total_param_loss : 8.323600769042969 | param_0_loss : 4.0961408615112305 | param_1_loss : 4.227459907531738 | total_loss : 8.88452434539795\n",
            "Epoch 48 Iter 13600:  | prim_type_loss : 0.36269840598106384 | total_param_loss : 8.265408515930176 | param_0_loss : 4.047501564025879 | param_1_loss : 4.217906951904297 | total_loss : 8.628107070922852\n",
            "Epoch 49 Iter 13700:  | prim_type_loss : 0.30733564496040344 | total_param_loss : 8.22309398651123 | param_0_loss : 3.993349075317383 | param_1_loss : 4.229744911193848 | total_loss : 8.53042984008789\n",
            "Epoch 49 Iter 13800:  | prim_type_loss : 0.5251486897468567 | total_param_loss : 8.894784927368164 | param_0_loss : 4.064339637756348 | param_1_loss : 4.830444812774658 | total_loss : 9.419933319091797\n",
            "Epoch 49 Iter 13900:  | prim_type_loss : 0.5787598490715027 | total_param_loss : 8.215937614440918 | param_0_loss : 3.9458799362182617 | param_1_loss : 4.270057678222656 | total_loss : 8.794697761535645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-CHcPnYtyPp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP"
      ],
      "metadata": {
        "id": "n6wpXVmcup_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import os\n",
        "\n",
        "device = \"cuda\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False) #Must set jit=False for training\n",
        "\n",
        "root_folder = '/content/gdrive/MyDrive'\n",
        "torch_path_name = os.path.join(root_folder, \"pleasant-tree-10.pt\")\n",
        "checkpoint = torch.load(torch_path_name)\n",
        "print(checkpoint.keys())\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "id": "DSg4JrAk0I_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.text = df['raw'].to_list()\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.text[index]\n",
        "text_dataset = TextDataset(df)\n",
        "text_loader = DataLoader(text_dataset, batch_size = 16, shuffle=False) "
      ],
      "metadata": {
        "id": "ibxuoBFnr-z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "word_features = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for list_txt in text_loader:\n",
        "        texts = clip.tokenize(list_txt).to(device)\n",
        "        feat = model.encode_text(texts)\n",
        "        word_features.append(feat)"
      ],
      "metadata": {
        "id": "jRZ33yxnkfzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "h6kydinwt2Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KiZNi_UdmdbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrained text embeddings"
      ],
      "metadata": {
        "id": "aDIIs3mFuhVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PrimitiveDataset(Dataset):\n",
        "    def __init__(self, path, word_path, vocab, num_transformation_params, image_size=256.):\n",
        "        super().__init__()\n",
        "        self.path = path\n",
        "        \n",
        "        f = open(self.path, \"rb\")\n",
        "        self.data_raw = pickle.load(f)\n",
        "\n",
        "        self.word_data = pickle.load(open(word_path), \"rb\")\n",
        "        \n",
        "        # self.vocab = vocab\n",
        "        # self.vocab_keys = vocab.keys()\n",
        "        # self.original_image_size = 256.\n",
        "        self.num_transformation_params = num_transformation_params\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_raw)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # Process language input \n",
        "        info = self.data_raw[index]\n",
        "        word_arr = self.word_data[index] # (len, word_dim)\n",
        "        description_t = torch.FloatTensor(word_arr)\n",
        "        # description = info['processed']\n",
        "        # description_t = [self.vocab[x.lower()] for x in description.split(\" \") if x.lower() in self.vocab_keys]\n",
        "        # description_t = torch.from_numpy(np.array(description_t)).long()\n",
        "        \n",
        "        # Process M (num_transformation_params,) and type\n",
        "        primitive_type = torch.from_numpy(np.array([info['primitive_type']])).long()\n",
        "        if 'M' in info:\n",
        "            affine_params = info['M'].reshape(-1,)[:self.num_transformation_params]\n",
        "            affine_params = torch.FloatTensor(affine_params)\n",
        "        else:\n",
        "            raise \n",
        "        return description_t, primitive_type, affine_params"
      ],
      "metadata": {
        "id": "RzdtxsyRjS34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_primitivedataset(seq_list):\n",
        "    description_ts, primitive_types, affine_paramss = zip(*seq_list)\n",
        "    lens = [len(x) for x in description_ts]\n",
        "    seq_order = sorted(range(len(lens)), key=lens.__getitem__, reverse=True)\n",
        "    description_ts = [description_ts[i] for i in seq_order]\n",
        "    \n",
        "    primitive_types = torch.stack([primitive_types[i] for i in seq_order])\n",
        "    affine_paramss = torch.stack([affine_paramss[i] for i in seq_order])\n",
        "    \n",
        "    # (N, 1) (N, num_transformation_params)\n",
        "    return description_ts, primitive_types, affine_paramss\n"
      ],
      "metadata": {
        "id": "aueUzhffnlJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PrimitiveSelector(nn.Module):\n",
        "    def __init__(self, hp):\n",
        "        super().__init__()\n",
        "        '''\n",
        "        num_embeddings: vocab size \n",
        "        '''\n",
        "        self.hp = hp\n",
        "        # self.embed = nn.Embedding(hp.vocab_size, hp.word_embed_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size = hp.word_embed_dim, \n",
        "            hidden_size = hp.lstm_output_dim, \n",
        "            num_layers = hp.lstm_layers, \n",
        "            dropout = hp.lstm_drop_prob,\n",
        "        )\n",
        "\n",
        "        self.primitive_fc = nn.Linear(hp.lstm_output_dim, hp.num_primitives)\n",
        "        self.num_normal_param = 3\n",
        "        self.affine_fc = nn.Linear(hp.lstm_output_dim, self.num_normal_param * hp.M * hp.num_transformation_params)\n",
        "        \n",
        "    def forward(self, question): # question: PackedSequence \n",
        "        # seq_tensor, seq_lengths = rnn.pad_packed_sequence(question, batch_first=True)               \n",
        "        # embedded_seq_tensor = self.embed(seq_tensor)\n",
        "        # seq_packed = rnn.pack_padded_sequence(np.transpose(embedded_seq_tensor,0,1), seq_lengths)\n",
        "        _, hidden = self.lstm(question, None)\n",
        "        seq_last_layer = hidden[-1] # N x hidden_embed_dim\n",
        "        \n",
        "        prim_pred = self.primitive_fc(seq_last_layer) \n",
        "        prim_param_pred = self.affine_fc(seq_last_layer) # N x (self.num_normal_param * M * num_transformation_params)\n",
        "        each_prim_param = torch.split(prim_param_pred, self.num_normal_param * self.hp.M, 1) # [N x (num_normal_param * M)]\n",
        "        pi_list = [] # length num_transformation_params\n",
        "        mu_list = []\n",
        "        sigma_list = []\n",
        "        for y in each_prim_param: # N x (num_normal_param * M)\n",
        "            params = torch.split(y, self.num_normal_param, 1) # N x self.num_normal_param\n",
        "            params_mixture = torch.stack(params) # M x N x self.num_normal_param\n",
        "            pi, mu, sigma = torch.split(params_mixture, 1, 2) # M x N x 1\n",
        "            pi = F.softmax(pi.transpose(0,1).squeeze(), dim=-1) # N x M\n",
        "            mu = mu.transpose(0,1).squeeze().contiguous()\n",
        "            sigma = torch.exp(sigma.transpose(0,1).squeeze())\n",
        "            \n",
        "            pi_list.append(pi)\n",
        "            mu_list.append(mu)\n",
        "            sigma_list.append(sigma)\n",
        "        \n",
        "        return prim_pred, pi_list, mu_list, sigma_list\n"
      ],
      "metadata": {
        "id": "_M4BwBWfjjPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "owz9ej2DjjSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nMV7qkawjjUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4ohdsw3gjjW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gf7YxXRTjjZn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}